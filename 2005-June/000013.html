<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plfs-svn] doc 00-plfs.bib,1.4,1.5 00-plfs.tex,1.3,1.4 02.tex,1.10,1.11 03.tex,1.10,1.11 04.tex,1.6,1.7 ToDo.txt,1.2,1.3
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plfs-svn/2005-June/index.html" >
   <LINK REL="made" HREF="mailto:plfs-svn%40lists.berlios.de?Subject=Re%3A%20%5BPlfs-svn%5D%20doc%2000-plfs.bib%2C1.4%2C1.5%2000-plfs.tex%2C1.3%2C1.4%2002.tex%2C1.10%2C1.11%2003.tex%2C1.10%2C1.11%2004.tex%2C1.6%2C1.7%20ToDo.txt%2C1.2%2C1.3&In-Reply-To=%3C200506171857.j5HIvOI11873%40bat.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000012.html">
   <LINK REL="Next"  HREF="000014.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plfs-svn] doc 00-plfs.bib,1.4,1.5 00-plfs.tex,1.3,1.4 02.tex,1.10,1.11 03.tex,1.10,1.11 04.tex,1.6,1.7 ToDo.txt,1.2,1.3</H1>
    <B>xscript</B> 
    <A HREF="mailto:plfs-svn%40lists.berlios.de?Subject=Re%3A%20%5BPlfs-svn%5D%20doc%2000-plfs.bib%2C1.4%2C1.5%2000-plfs.tex%2C1.3%2C1.4%2002.tex%2C1.10%2C1.11%2003.tex%2C1.10%2C1.11%2004.tex%2C1.6%2C1.7%20ToDo.txt%2C1.2%2C1.3&In-Reply-To=%3C200506171857.j5HIvOI11873%40bat.berlios.de%3E"
       TITLE="[Plfs-svn] doc 00-plfs.bib,1.4,1.5 00-plfs.tex,1.3,1.4 02.tex,1.10,1.11 03.tex,1.10,1.11 04.tex,1.6,1.7 ToDo.txt,1.2,1.3">nobody at sheep.berlios.de
       </A><BR>
    <I>Fri Jun 17 20:57:24 CEST 2005</I>
    <P><UL>
        <LI>Previous message: <A HREF="000012.html">[Plfs-svn] doc 03.tex,1.9,1.10
</A></li>
        <LI>Next message: <A HREF="000014.html">[Plfs-svn] doc 00-plfs.bib,1.5,1.6
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#13">[ date ]</a>
              <a href="thread.html#13">[ thread ]</a>
              <a href="subject.html#13">[ subject ]</a>
              <a href="author.html#13">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Update of /cvsroot/plfs/doc
In directory sheep:/tmp/cvs-serv968

Modified Files:
	00-plfs.bib 00-plfs.tex 02.tex 03.tex 04.tex ToDo.txt 
Log Message:
Finalizacion de la documentacion

Index: 00-plfs.bib
===================================================================
RCS file: /cvsroot/plfs/doc/00-plfs.bib,v
retrieving revision 1.4
retrieving revision 1.5
diff -C2 -d -r1.4 -r1.5
*** 00-plfs.bib	12 Jun 2005 21:52:27 -0000	1.4
--- 00-plfs.bib	17 Jun 2005 18:57:21 -0000	1.5
***************
*** 15,32 ****
  
  @manual{Araneola,
! 	title = {Araneola},
! 	note = {\url{<A HREF="http://">http://</A>}}
! }
! 
! @misc{rfc-track,
! 	title = {Tree-Based ACK (TRACK) Building Block for Reliable Multicast Transport},
! 	author = {Brian Whetten et al.},
! 	howpublished = {RMT Working Group, IETF},
! 	month = {December},
! 	year = {2003}
  }
  
- #--- no utilizados ---
- 
  @manual{ESM,
  	title = {End System Multicast (ESM)},
--- 15,25 ----
  
  @manual{Araneola,
! 	title = {Araneola: A Scalable Reliable Multicast System for Dynamic
! 		Environments},
! 	note =
! {\url{<A HREF="http://dsl.cs.technion.ac.il/completed_projects/Araneola/html/araneola.htm">http://dsl.cs.technion.ac.il/completed_projects/Araneola/html/araneola.htm</A>
! }}
  }
  
  @manual{ESM,
  	title = {End System Multicast (ESM)},
***************
*** 34,37 ****
--- 27,38 ----
  }
  
+ @manual{LMDD,
+ 	title = {Logistical Multicast for Data Distribution},
+ 	note =
+ {\\\url{
+ <A HREF="http://loci.cs.utk.edu/modules.php?name=Publications&amp;d_op=ViewPublication&amp;lid=24">http://loci.cs.utk.edu/modules.php?name=Publications&amp;d_op=ViewPublication&amp;lid=24</A>
+ 3}}
+ }
+ 
  @manual{LSAM,
  	title = {Large-Scale Active Multicast},
***************
*** 39,45 ****
  }
  
! @manual{LMDD,
! 	title = {Logistical Multicast for Data Distribution},
! 	note = {\url{<A HREF="http://loci.cs.utk.edu/modules.php?name=Publications&amp;d_op=ViewPublication&amp;lid=243">http://loci.cs.utk.edu/modules.php?name=Publications&amp;d_op=ViewPublication&amp;lid=243</A>}}
  }
  
--- 40,49 ----
  }
  
! @misc{rfc-track,
! 	title = {Tree-Based ACK (TRACK) Building Block for Reliable Multicast Transport},
! 	author = {Brian Whetten et al.},
! 	howpublished = {RMT Working Group, IETF},
! 	month = {December},
! 	year = {2003}
  }
  
***************
*** 80,83 ****
--- 84,97 ----
  }
  
+ @manual{FAM,
+ 	title = {File Alteration Monitor},
+ 	note = {\url{<A HREF="http://oss.sgi.com/projects/fam/">http://oss.sgi.com/projects/fam/</A>}}
+ }
+ 
+ @manual{Gamin,
+ 	title = {Gamin the File Alteration Monitor},
+ 	note = {\url{<A HREF="http://www.gnome.org/~veillard/gamin/">http://www.gnome.org/~veillard/gamin/</A>}}
+ }
+ 
  @manual{PLC,
  	title = {PlanetLab Central API Documentation},
***************
*** 111,113 ****
  	note = {\url{<A HREF="http://www.cs.arizona.edu/stork/">http://www.cs.arizona.edu/stork/</A>}}
  }
- 
--- 125,126 ----

Index: 00-plfs.tex
===================================================================
RCS file: /cvsroot/plfs/doc/00-plfs.tex,v
retrieving revision 1.3
retrieving revision 1.4
diff -C2 -d -r1.3 -r1.4
*** 00-plfs.tex	12 Jun 2005 21:46:11 -0000	1.3
--- 00-plfs.tex	17 Jun 2005 18:57:21 -0000	1.4
***************
*** 79,83 ****
  \input{03}
  \input{04}
! \input{05}
  
  \pagebreak
--- 79,83 ----
  \input{03}
  \input{04}
! %\input{05}
  
  \pagebreak

Index: 02.tex
===================================================================
RCS file: /cvsroot/plfs/doc/02.tex,v
retrieving revision 1.10
retrieving revision 1.11
diff -C2 -d -r1.10 -r1.11
*** 02.tex	14 Jun 2005 00:55:26 -0000	1.10
--- 02.tex	17 Jun 2005 18:57:21 -0000	1.11
***************
*** 8,11 ****
--- 8,12 ----
  
  \section{Componentes}
+ \label{sect:components}
  
  En este apartado mostraremos un esquema inicial de los principales
***************
*** 66,69 ****
--- 67,71 ----
  
  \section{Esquema de \plfs (PlanetLab File System)}
+ \label{sect:plfsschema}
  
  Antes de comentar el esquema del &#225;rbol de ficheros y directorios de \plfs, es
***************
*** 90,93 ****
--- 92,97 ----
  ficheros como se puede apreciar en la figura:
  
+ \newpage
+ 
  \begin{code}
  	/plfs
***************
*** 99,105 ****
--- 103,111 ----
  	|   |-key
  	|   |-passwd
+ 	|   |-status
  	|   |-script
  	|   |-nodes
  	|   | `-&lt;node&gt;
+ 	|   |   |-version
  	|   |   `-unshared
  	|   `-shared
***************
*** 137,142 ****
  		del slice.
  
  	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/script}] :\\
! 		&#201;ste fichero es un shell script que se ejecutar&#225; en cada m&#225;quina
  		una vez hecho el despliegue.
  		\\
--- 143,152 ----
  		del slice.
  
+ 	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/status}] :\\
+ 		Este fichero permite conocer el estado de despliegue de los
+ 		nodos del slice.
+ 
  	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/script}] :\\
! 		Este fichero es un shell script que se ejecutar&#225; en cada m&#225;quina
  		una vez hecho el despliegue.
  		\\
***************
*** 154,160 ****
--- 164,180 ----
  			\item [deploy:] lleva a cabo las operaciones necesarias
  				para preparar el arranque del servicio
+ 				(instalaci&#243;n de la aplicaci&#243;n y logueo de los
+ 				ficheros generados durante &#233;sta)
+ 
+ 			\item [clean:] elimina los ficheros que ha desplegado
+ 				el \texttt{script$\rightarrow$deploy}
  		\end{description}
  
  	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/nodes/\textless{}
+ 		node\textgreater/version}] :\\
+ 		Este fichero contiene el n&#250;mero de version del &#250;ltimo
+ 		despliegue (ver apartado \ref{sect:versioning}).
+ 		
+ 	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/nodes/\textless{}
  		node\textgreater/unshared}] :\\
  		&#201;ste directorio es propio de cada nodo en el contexto del slice
***************
*** 180,183 ****
--- 200,204 ----
  
  \section{Operaciones de \plfs}
+ \label{sect:plfsoperations}
  
  Las posibles operaciones que \plfs permite, y por ende, que \kplfs implemente
***************
*** 302,305 ****
--- 323,327 ----
  
  \section{Descubrimiento de entidades del sistema}
+ \label{sect:discovery}
  
  El sistema, adem&#225;s de las funcionalidades propias de despliegue de
***************
*** 358,372 ****
  \label{sect:deployment}
  
- % TODO: HAY QUE AVISAR DE CUALES SE HA PODIDO DESPLEGAR Y CUALES NO
- %
- % que hacer con los que se han quedado a medias/han fallado?
- %
- % estaria bien un fichero de estado? Lista de nodos registrados en PLC en el
- % slice, con:
- % - ultima operacion realizada
- % - ultima operacion fallida
- % - ultima operacion en proceso
- % - nodo no accesible (no responde)
- 
  Despu&#233;s de pensar diferentes posibilidades sobre como el usuario podr&#237;a llevar
  a cabo realmente el despliegue mediante el sistema de ficheros, hemos llegado
--- 380,383 ----
***************
*** 378,390 ****
  		despliegue que a continuaci&#243;n comentamos.
  
! 	\item A continuaci&#243;n el usuario realiza \texttt{mv/cp} del script de
! 		arranque de despliegue, que tambi&#233;n es desplegado mediante el
  		mismo proceso hacia los nodos destino.
  \end{enumerate}
  
  Adem&#225;s, y como es obvio dada la naturaleza del propio sistema de ficheros, se
! deber&#225; cumplir la siguiente precondici&#243;n antes de poder realizar el despliegue:
  
! \textit{Precondici&#243;n}: El slice destino est&#225; ya creado (ej: \texttt{mkdir}).
  
  Una vez aclarado lo anterior, pasamos a describir como se realizar&#237;a de forma
--- 389,402 ----
  		despliegue que a continuaci&#243;n comentamos.
  
! 	\item A continuaci&#243;n el usuario realiza \texttt{mv/cp/touch} del script
! 		de arranque de despliegue, que tambi&#233;n es desplegado mediante el
  		mismo proceso hacia los nodos destino.
  \end{enumerate}
  
  Adem&#225;s, y como es obvio dada la naturaleza del propio sistema de ficheros, se
! deber&#225; cumplir la siguiente precondici&#243;n antes de poder realizar el
! despliegue:\\
  
! \textit{Precondici&#243;n}: El slice destino est&#225; ya creado (ej: \texttt{mkdir}).\\
  
  Una vez aclarado lo anterior, pasamos a describir como se realizar&#237;a de forma
***************
*** 406,410 ****
  	\item \dpld $\rightarrow$ \umcc \\
  		Se comunica el slice destino, se dan los ficheros a desplegar y
! 		el shell script con los comandos a ejecutar una vez desplegados.
  
  	\item \umcc $\rightarrow$ red \umcr \\
--- 418,423 ----
  	\item \dpld $\rightarrow$ \umcc \\
  		Se comunica el slice destino, se dan los ficheros a desplegar y
! 		el shell script con los comandos a ejecutar una vez
! 		desplegados, indicando la versi&#243;n que se est&#225; desplegando.
  
  	\item \umcc $\rightarrow$ red \umcr \\
***************
*** 433,451 ****
  	\item \dplc $\rightarrow$ slice destino (ssh) \\
  		Se despliega la aplicaci&#243;n (se copian los ficheros), y una vez
! 		hecho esto, se ejecuta el shell script adjuntado, en caso de
! 		estar presente. Para ello, primero ejecuta el \textbf{deploy} y
! 		luego el \textbf{start}.
! 		% TODO: diferenciar primeros de ultimo fichero
! 		% ahora el ultimo debe ser el script y se hara el deploy+start
  \end{enumerate}
  
! % TODO: NOTA:
! % En caso de haber un fallo en cualquiera de los dos pasos anteriores,
! % se devuelve un error al dpld origen (a modo de NACK) ??????
! % Si todo es correcto anunciar al dpld??? Escalabilidad? ACK's
  
  
  
  \section{Comunicaci&#243;n Multicast}
  
  Como hemos comentado, el despliegue de los ficheros se hace sobre una red
--- 446,474 ----
  	\item \dplc $\rightarrow$ slice destino (ssh) \\
  		Se despliega la aplicaci&#243;n (se copian los ficheros), y una vez
! 		hecho esto, se ejecuta el shell script (que es el &#250;ltimo
! 		fichero en llegar). Para ello, primero se desinstala la
! 		anterior aplicaci&#243;n (\texttt{script$\rightarrow$clean}), se
! 		copian los nuevos ficheros, y luego se despliega la aplicaci&#243;n
! 		(\texttt{script$\rightarrow$deploy}) y se pone en marcha
! 		(\texttt{script$\rightarrow$start}).
  \end{enumerate}
  
! Como se ve en el apartado \ref{sect:redeployment}, no es necesario controlar
! los casos en que algun despliegue no se lleve a cabo en alg&#250;n nodo, ya que
! entre ellos mismos se encargar&#225;n de mantener la coherencia del despliegue.
! 
! El fichero \texttt{/plfs/slices/\textless{}slice\textgreater/status} contiene,
! para cada nodo del slice, si se ha podido contactar con &#233;l y, en caso
! afirmativo, en qu&#233; estado de despligue se encuentra el nodo.
! 
! Esta informaci&#243;n se deriva a trav&#233;s del contenido del acceso a los ficheros
! \texttt{/plfs/slices/\textless{}slice\textgreater/nodes/\textless{}
! node\textgreater/version}, dejando comprobar en qu&#233; estado se encuentra el
! despliegue al comparar la versi&#243;n de los nodos con la actual.
  
  
  
  \section{Comunicaci&#243;n Multicast}
+ \label{sect:multicast}
  
  Como hemos comentado, el despliegue de los ficheros se hace sobre una red
***************
*** 500,507 ****
  \end{description}
  
! Creemos que TRACK es m&#225;s completo y nos permitir&#237;a realizar m&#225;s operaciones a
! nivel de despliegue. Por ejemplo, en caso de recibir un TRACK donde se informa
! de que han fallado algunos paquetes, el cliente de multicast \umcc informar&#237;a a
! \dpld el cual se encargar&#237;a de reenviar los datos.
  
  
--- 523,555 ----
  \end{description}
  
! Creemos que TRACK es m&#225;s completo y permitir&#237;a realizar extensiones al sistema,
! como por ejemplo a modo de monitorizaci&#243;n o posibilidad de extender el modelo
! de despliegue a, por ejemplo, exigir un m&#237;nimo de N nodos.
! 
! 
! 
! \section{Versionamiento de despliegues}
! \label{sect:versioning}
! 
! Como los nodos de los slices pueden recibir varios despliegues en el tiempo y de
! manera secuencial, y queremos tener los nodos de un mismo slice sincronizados
! al &#250;ltimo despliegue realizado, necesitamos una manera de controlar las
! versiones que nos permita detectar las situaciones que comentamos en el
! apartado \ref{sect:redeployment}.
! 
! Como presuponemos que los despliegues son relativamente poco frecuentes, y no
! soportamos la posibilidad de realizar despliegues simult&#225;neos (en parte por que
! no tienen sentido por la naturaleza del problema), consideramos que la fecha y
! la hora en un formato com&#250;n entre todos los nodos (ej:UTC o GMT), son
! identificadores suficientes de la versi&#243;n de un despliegue, presuponiendo que
! los nodos de PlanetLab est&#225;n sincronizados a esta hora con unos l&#237;mites de
! error aceptables.
! 
! Este n&#250;mero de versi&#243;n es accesible a trav&#233;s del fichero
! \texttt{/plfs/slices/\textless{}slice\textgreater/nodes/\textless{}
! node\textgreater/version}.
! 
! Cabe destacar un tipo de versi&#243;n especial que llamaremos ``versi&#243;n corrupta''
! que ser&#225; considerada como la m&#225;s vieja de todas las versiones.
  
  
***************
*** 510,551 ****
  \label{sect:redeployment}
  
! El componente \famon se encarga de la monitorizaci&#243;n de los ficheros etiquetados
! como \texttt{shared}. De este modo, cuando se produce una modificaci&#243;n
! incontrolada de estos ficheros, este componente avisa al \dplc para que a su
! vez lo notifique a los nodos \dpld responsables de ese fichero, es decir,
! aquellos que est&#233;n suscritos como oyentes de ese grupo multicast, o slice, del
! cual son los responsables de los despliegues que se realicen en &#233;l, y por
! lo tanto deben encargarse de invalidar los ficheros afectados del sistema de
! ficheros \plfs.
  
! Entenderemos por modificaci&#243;n incontrolada, aquella que no provenga de uno de
! estos nodos responsables (nodos con \dpld).
  
! Cuando \famon avisa al \dplc de ese nodo, &#233;ste proceder&#225; a redesplegar el
! fichero siguiendo los pasos a continuaci&#243;n:
  
  \begin{enumerate}
! 	\item \dplc realiza un script-&gt;stop deteniendo moment&#225;neamente la
! 		aplicaci&#243;n en ese nodo corrupto.
  
! 	\item se despliega el fichero desde un nodo del mismo slice (un nodo
! 		hermano hablando desde el punto de vista de &#225;rboles de
! 		directorios) que no sea corrupto.
  
! 	\item \dplc realiza un script-&gt;start para reiniciar la aplicaci&#243;n.
  \end{enumerate}
  
  Obviamente surge un posible problema como consecuencia de este dise&#241;o, y es que
! podr&#237;a suceder que por circunstancias no existiera ning&#250;n otro nodo no corrupto
  (o bien porque todos los hermanos lo est&#225;n, o bien porque es el &#250;nico nodo
  activo del slice).
  
  De modo que la soluci&#243;n que proponemos es la de mantener en un repositorio
! central los ficheros originales desplegados como \texttt{shared}.
  La URI de este repositorio es la que se indica en el fichero \texttt{backup}
  que hemos presentado en el esquema de \plfs.
  
! Finalmente, si la recuperaci&#243;n mediante este repositorio no pudiera llevarse a
! cabo, se invalidar&#237;a el nodo del slice, y quedar&#237;a como corrupto.
  
  
--- 558,656 ----
  \label{sect:redeployment}
  
! Existen dos casos en los que es necesario un redespliegue:
  
! \begin{description}
! 	\item[Ca&#237;da de nodos] : \\
! 		Los nodos en PlanetLab &quot;caen&quot; de forma aleatoria, o bien un nodo
! 		concreto (cuelgue de la m&#225;quina, reinicio, p&#233;rdida
! 		de conectividad, etc.) o por grupos institucionales
! 		(fallo el&#233;ctrico de una zona, ca&#237;da del acceso a la red, etc.).
! 		Estas situaciones se detectan en caso de rearrancar \dplc (por
! 		una ca&#237;da de la m&#225;quina) o bien cuando el nodo queda reconectado
! 		a la red multicast (despu&#233;s de haber sido expulsado por
! 		falta de respuesta). \\
! 		Ser&#225; necesario realizar un redespliegue, si al volver al
! 		grupo, el nodo detecta que hay una versi&#243;n m&#225;s nueva de
! 		despliegue.
! 	
! 	\item[Modificaci&#243;n de ficheros] : \\
! 		Cuando se produce una modificaci&#243;n incontrolada (que no viene 
! 		de uno de los \dpld registrados) de los ficheros de un nodo al
! 		que se le ha realizado un despliegue, y estos son de tipo
! 		\textit{shared}, \famon se encarga de detectarlo y avisar a
! 		\dplc, que se encargar&#225; de avisar a los \dpld
! 		registrados para que hagan una invalidaci&#243;n de
! 		dichos ficheros.
! 		\\
! 		En este caso es necesario adem&#225;s marcar la versi&#243;n como
! 		``versi&#243;n corrupta'' (nunca ser&#225; rearrancada una
! 		aplicaci&#243;n marcada de esta manera).
! \end{description}
  
! Cuando se detectan uno de estos dos casos, antes de nada, se detiene la
! aplicaci&#243;n mediante el script de despliegue (script$\rightarrow$stop) y
! posteriormente se realiza el proceso de redespliegue.
! 
! Para realizar el redespliegue es necesario que los \dplc mantengan una lista de
! \textit{peers} para cada slice al que pertenece. Cada lista contiene N
! nodos con el &#250;ltimo n&#250;mero de versi&#243;n conocido para cada uno de ellos (a modo de
! \textit{vector clock}).
! 
! Para obtener esta lista, cuando \dplc arranca obtiene las direcciones de N nodos
! cualesquiera mediante \pldb y a continuaci&#243;n les pregunta a cada uno de ellos
! su n&#250;mero de versi&#243;n dando a cambio el suyo propio.
! 
! En caso de que un nodo no responda se sustituye por otro que no
! est&#233; en la lista de forma aleatoria. An&#225;logamente, en el caso de que est&#233; en la
! lista pero posteriormente se pierda la conexi&#243;n con &#233;l (ej: detecci&#243;n por pings
! peri&#243;dicamente con per&#237;odos largos para no saturar la red), tambi&#233;n ser&#225;
! sustituido.
! 
! En el caso de que sea el propio nodo el que haya perdido la conexi&#243;n, \dplc
! quedar&#225; suspendido hasta reactivar la conexi&#243;n, momento en el que intentar&#225;
! revalidar las entradas de su tabla de \textit{peers}.
! 
! El proceso de redespliegue (cuando sea necesario tal como hemos comentado
! antes) se realizar&#225; mediante los siguientes pasos:
  
  \begin{enumerate}
! 	\item En caso de no tener los datos de ssh, \dplc se los pedir&#225; a alg&#250;n
! 		nodo de la lista para poder abrir las conexiones.
  
! 	\item Una vez abiertas las conexiones, se pide un despliegue unicast a
! 		uno de los \textit{peers} de la lista.
  
! 	\item Se procede al despliegue de los ficheros actualizando as&#237; la
! 		versi&#243;n.
  \end{enumerate}
  
  Obviamente surge un posible problema como consecuencia de este dise&#241;o, y es que
! podr&#237;a suceder que por circunstancias no existiera ning&#250;n nodo con una versi&#243;n
  (o bien porque todos los hermanos lo est&#225;n, o bien porque es el &#250;nico nodo
  activo del slice).
  
  De modo que la soluci&#243;n que proponemos es la de mantener en un repositorio
! central los ficheros originales desplegados como \texttt{shared}, de modo que
! si \dplc no puede contactar con ning&#250;n nodo no corrupto pueda descargar estos
! ficheros.
! 
  La URI de este repositorio es la que se indica en el fichero \texttt{backup}
  que hemos presentado en el esquema de \plfs.
  
! Para evitar un colapso de la red los cambios de versi&#243;n no se informan de
! immediato al producirse dicho cambio (excepto del cambio de versi&#243;n a ``versi&#243;n
! corrupta'' que produce un aviso instant&#225;neo), sino que se utilza un mecanismo
! de coherencia relajada a trav&#233;s de los mensajes que se env&#237;an los
! \textit{peers} para actualizar sus listas de versiones, aprovechando la
! entrop&#237;a que propicia el hecho de que cada uno env&#237;a dichos mensajes en tiempos
! muy probablemente diferentes (porque han empezado en un instante distinto).
! 
! Como mejora, ser&#237;a interesante detectar qu&#233; nodos est&#225;n en una misma subred, es
! decir, que forman parte de una misma instituci&#243;n y comparten una red mucho m&#225;s
! r&#225;pida que Internet (ya sea por configuraci&#243;n manual o por detecci&#243;n a trav&#233;s
! de las interf&#237;cies configuradas), para as&#237; delegar un posible proceso de
! \textit{redeployment} de todo el grupo a un solo nodo, que luego haria el mismo
! \textit{redeployment} al resto de nodos del grupo (hasta pudiendo as&#237;
! aprovecharse de las capacidades de multicast del nivel de enlace).
  
  
***************
*** 577,586 ****
  
  \begin{enumerate}
! 	\item Se consulta con PlanetLab Central y se escoge un nodo cualquiera
  		del slice con el que nos queremos comunicar.
  		\label{step:auth_get_node}
  
  	\item Se pide la clave p&#250;blica del nodo seleccionado, $K_{pub_{c}}$, y
! 		se le da la p&#250;blica del cliente, $K_{pub_{d}}$.\\
  		Si el nodo no responde, se vuleve al paso
  		\ref{step:auth_get_node}).
--- 682,691 ----
  
  \begin{enumerate}
! 	\item Se consulta con \pldb y se escoge un nodo cualquiera 
  		del slice con el que nos queremos comunicar.
  		\label{step:auth_get_node}
  
  	\item Se pide la clave p&#250;blica del nodo seleccionado, $K_{pub_{c}}$, y
! 		se le da la p&#250;blica del \dpld, $K_{pub_{d}}$.\\
  		Si el nodo no responde, se vuleve al paso
  		\ref{step:auth_get_node}).
***************
*** 599,603 ****
  \end{enumerate}
  
- %TODO: como avisa un nodo a un cliente que este no esta autentificado?
  Hay varios casos en los que una operaci&#243;n realizada por un \dpld no podr&#237;a ser
  llevada a cabo en un \dplc:
--- 704,707 ----
***************
*** 616,633 ****
  
  La soluci&#243;n que hemos pensado para estos tres casos anteriores, es la de
! realizar nuevos registros. En el primer caso, \dplc avisar&#237;a mediante un
! \textit{warning} (ver apartado \ref{sect:warnings}), y \dpld realizar&#237;a un
  registro unicast en ese nodo. En los otros casos \dpld realizar&#237;a de
  nuevo el registro multicast cada cierto tiempo (pol&#237;tica de revalidaci&#243;n).
  Cabe notar que las conexiones que \dpld mantiene con los slices tienen un
! \textit{timeout} asociado, de modo que una vez expirado se perder&#225;n los datos
! de dichas conexiones y si posteriormente se quisiera realizar una operaci&#243;n en
! alguno de esos slices se deber&#225; reiniciar de cero el proceso de registro.
  
  En el caso en que se hayan cambiado la clave y/o el password ssh y que la
! conexi&#243;n ssh est&#233; cerrada, el \dpld recibir&#237;a algunos \textit{warnings}
! procedentes de los \dplc destinatarios con la conexi&#243;n ssh cerrada (que ser&#225;n
! una minor&#237;a debido a la pol&#237;tica de revalidaci&#243;n de registros comentada
! anteriormente).
  
  Como los datos desplegados por \dpld van firmados para evitar problemas de
--- 720,744 ----
  
  La soluci&#243;n que hemos pensado para estos tres casos anteriores, es la de
! realizar nuevos registros. En el primer caso, \dplc avisar&#237;a mediante una
! operaci&#243;n (ver apartado \ref{sect:dplc}), y \dpld realizar&#237;a un
  registro unicast en ese nodo. En los otros casos \dpld realizar&#237;a de
  nuevo el registro multicast cada cierto tiempo (pol&#237;tica de revalidaci&#243;n).
+ 
  Cabe notar que las conexiones que \dpld mantiene con los slices tienen un
! \textit{timeout} asociado, de modo que una vez expirado se perder&#225;n dichas
! conexiones y si posteriormente se quisiera realizar una operaci&#243;n en
! alguno de esos slices se deber&#225; reiniciar de cero el proceso de registro
! (momento en el que se intentar&#225; revalidar los fiheros que tubiera cacheados el
! \dpld).
! 
! Ahora bien, \dplc no pierde los datos de conexi&#243;n (clave y password ssh),
! puesto que como se explica en el apartado \ref{sect:redeployment}, &#233;stos son
! necesarios para mantener la coherencia del slice.
  
  En el caso en que se hayan cambiado la clave y/o el password ssh y que la
! conexi&#243;n ssh est&#233; cerrada, el \dpld recibir&#237;a algunas operaciones de aviso (ver
! apartado \ref{sect:dpld}) procedentes de los \dplc destinatarios con la conexi&#243;n
! ssh cerrada (que ser&#225;n una minor&#237;a debido a la pol&#237;tica de revalidaci&#243;n de
! registros comentada anteriormente).
  
  Como los datos desplegados por \dpld van firmados para evitar problemas de
***************
*** 635,639 ****
  deber&#225; propagar la clave p&#250;blica mediante un registro multicast a los slices
  con los que tuviera una relaci&#243;n abierta.
- 
- % TODO(?): 'ls' =&gt; renew =&gt; cheksums??
- % =&gt; invalidar els fitxers cada X temps
--- 746,747 ----

Index: 03.tex
===================================================================
RCS file: /cvsroot/plfs/doc/03.tex,v
retrieving revision 1.10
retrieving revision 1.11
diff -C2 -d -r1.10 -r1.11
*** 03.tex	15 Jun 2005 18:55:35 -0000	1.10
--- 03.tex	17 Jun 2005 18:57:21 -0000	1.11
***************
*** 9,30 ****
  
  \section{\kplfs}
  Este m&#243;dulo implementa la interfaz del VFS, y las operaciones que permite las
! hemos descrito en el cap&#237;tulo anterior en el apartado \ref{sect:deployment}.
  
! % TODO(?): invalidate
  
  
  
  \section{\uplfs}
  
! Las operaciones que \uplfs ofrece, adem&#225;s de las especificadas en el m&#243;dulo
! \pldb para las que hace de intermediario, son las siguientes:
  
- %TODO: es solo esto?
  \begin{description}
! 	\item[\texttt{execute(component, operation, \ldots)}] :\\
  		Llama a la funci&#243;n \texttt{operation} del componente
  		\texttt{component} con los par&#225;metros extra que se indiquen (en caso
  		de indicar alguno).
  \end{description}
  
--- 9,49 ----
  
  \section{\kplfs}
+ 
  Este m&#243;dulo implementa la interfaz del VFS, y las operaciones que permite las
! hemos descrito en el cap&#237;tulo anterior en el apartado \ref{sect:plfsoperations}.
  
! Las operaciones que ofrece, adem&#225;s de las del VFS, son:
! 
! \begin{description}
! 	\item[\texttt{invalidate (slice\_name, type, path)}] :\\
! 		Invalida el fichero determinado por \texttt{path} del slice
! 		\texttt{slice\_name} del tipo \texttt{type} (\textit{shared} o
! 		\textit{unshared}).
! \end{description}
  
  
  
  \section{\uplfs}
+ \label{sect:uplfs}
  
! Cuando el usuario quiere mirar el contenido del directorio de slices o de
! nodos de un slice, y \kplfs remite dicha operaci&#243;n a \uplfs, este realiza la
! consulta de dicha informaci&#243;n llamando a las operaciones que ofrece \pldb, por
! ejemplo mediante llamadas a procedimiento remoto (como podr&#237;a ser Java RMI,
! soportado por PlanetLab Central).
! 
! Una vez obtenida la informaci&#243;n, \uplfs proceder&#237;a a pas&#225;rsela a \kplfs el cual
! actualizar&#237;a los inodos de \plfs y mostrar&#237;a el resultado al usuario.
! 
! Por ello, las operaciones que \uplfs ofrece, adem&#225;s de las del VFS para las que
! hace de intermediario hacia \kplfs, son las siguientes:
  
  \begin{description}
! 	\item[\texttt{execute (component, operation, \ldots)}] :\\
  		Llama a la funci&#243;n \texttt{operation} del componente
  		\texttt{component} con los par&#225;metros extra que se indiquen (en caso
  		de indicar alguno).
+ 		\\
+ 		Los posibles componentes actualmente son \dpld o \pldb.
  \end{description}
  
***************
*** 35,98 ****
  Las operaciones que \dpld ofrece son las siguientes:
  
- % TODO: faltan las credenciales de autentificaci&#243;n, o mejor un paso previo de
- % autentificaci&#243;n?!?!?!?!
- % TODO(~): faltan los parametros de que desplegar, etc
- % TODO(~): explicar fichero de comandos etc otra vez? 		NOO!!
- % TODO(~): poner los resultados de las ops
  \begin{description}
! 	\item[\texttt{void deployAppToSlice (slice\_name, files[], script, key,
! 		passwd,...)}] :\\
! 		Realiza la comunicaci&#243;n con el componente \umcc para desplegar
! 		una aplicaci&#243;n (files) a trav&#233;s de la red \umcr en el grupo
! 		indicado por el slice\_name. Para ello tambi&#233;n requerir&#225; el
! 		script de inicializaci&#243;n, y la clave y password de ssh del
  		slice.
  		\\
! 		% TODO: pero ahora hay TRACK!!!
! 		% primer intento + reintentos a solo a quien falla? (pero por multicast si cierto numero)
! 		NOTA: La operaci&#243;n no puede devolver resultado de &#233;xito,
! 		puesto que la implementaci&#243;n de ACKS en Multicast tiene
! 		problemas de escalabilidad, y por lo que hace a los NACKS
! 		no creemos que sea muy bueno mantener el \dpld esperando a ver
! 		si recibe alguno.
  
! 	\item[\texttt{ void deployAppToNode (slice\_name,
! 		node\_name,files,script, key, passwd) }] : \\
! 		Lo mismo que deployAppToSlice pero hacia un nodo concreto.
! 		&#218;til, si se a&#241;ade un nodo a un slice al que se ha realizado ya
! 		el despliegue.
  
! 	\item[\texttt{void addNodeToSlice (slice\_name, node\_name)}] :\\
! 		Crea el directorio que representa al nodo en el sistema de
! 		ficheros y posteriormente ejecuta \texttt{deployAppToNode}.
  
! 	\item[\texttt{registerToDplc (node\_name, slice\_name)}] :\\
! 		Se registra el nodo de \dpld como oyente de los cambios que se
! 		producen en un slice de un nodo concreto.
! 		% TODO(?): se puede utilizar un grupo de oyentes a cambios en
! 		% grupos de dplc, as&#237; no hace falta el registro...
  
! 	\item[\texttt{invalidateObj (slice\_name, name)}] :\\
! 		% TODO(?): utilizar identificador de obj diferente del path?
! 		% hace falta nombre del slice? (path completo)
! 		Invalida un objeto del sistema de ficheros (un fichero o un
! 		directorio).
! 		%NOTA: de que puede servir invalidar un objeto del FS?
  
! 	% TODO(?): validaciones? temporalidad?     ES &#218;TIL PLANTE&#193;RSELO?
  
! 	% TODO: quien lo utiliza?
! 	\item[\texttt{getInfoFilesShared (slice\_name)}] :\\
! 		Env&#237;a una petici&#243;n a multicast de informaci&#243;n de los ficheros
! 		de tipo \textit{shared} del slice \textit{slice\_name}.
  
! 	% TODO: quien lo utiliza?
! 	\item[\texttt{getInfoFilesUnshared (node\_name,slice\_name)}] :\\
! 		Env&#237;a una petici&#243;n unicast hacia el \dplc del nodo
! 		\textit{node\_name} de informaci&#243;n acerca de los ficheros de
! 		tipo \textit{unshared} del slice \textit{slice\_name} para dicho
! 		nodo.
  \end{description}
  
  
  
--- 54,144 ----
  Las operaciones que \dpld ofrece son las siguientes:
  
  \begin{description}
! 	\item[\texttt{deployToSlice (slice\_name, path\_from, path\_to)}]
! 		:\\
! 		Realiza la comunicaci&#243;n con los componentes \dplc para desplegar
! 		un fichero \textit{shared} (\texttt{path\_from}) al slice
! 		\texttt{slice\_name} en \texttt{path\_to}.
! 		\\
! 		Para ello, lee el fichero origen y env&#237;a los datos al grupo
! 		multicast que representa el slice a trav&#233;s de \umcc.
! 		\\
! 		&#201;sta operaci&#243;n provoca un aumento del n&#250;mero de versi&#243;n del
  		slice.
+ 
+ 	\item[\texttt{deployToNode (slice\_name, node\_name, path\_from,
+ 		path\_to) }] :\\
+ 		Lo mismo que \texttt{deployToSlice} pero hacia un nodo concreto
+ 		para desplegar un fichero \textit{unshared}.
+ 
+ 	\item[\texttt{getInfoShared (slice\_name, path)}] :\\
+ 		Hace una petici&#243;n de informaci&#243;n de un objeto \textit{shared}
+ 		del sistema de ficheros del slice \texttt{slice\_name} a un nodo
+ 		cualquiera.
  		\\
! 		Si la petici&#243;n falla, lo intentar&#225; con otro nodo cualquiera,
! 		hasta llegar a un n&#250;mero m&#225;ximo de reintentos.
  
! 	\item[\texttt{getInfoUnshared (slice\_name, node\_name, path)}] :\\
! 		Hace una petici&#243;n de informaci&#243;n de un objeto \textit{unshared}
! 		del sistema de ficheros del slice \texttt{slice\_name} al nodo
! 		\texttt{node\_name}.
! 		\\
! 		Si la operaci&#243;n falla, se reintentar&#225; un n&#250;mero finito de veces.
  
! 	\item[\texttt{getFileShared (slice\_name, path)}] :\\
! 		Hace una petici&#243;n de un objeto \textit{shared} del
! 		sistema de ficheros del slice \texttt{slice\_name} a un nodo
! 		cualquiera.
! 		\\
! 		Si la petici&#243;n falla, lo intentar&#225; con otro nodo cualquiera,
! 		hasta llegar a un n&#250;mero m&#225;ximo de reintentos.
  
! 	\item[\texttt{getFileUnshared (slice\_name, node\_name, path)}] :\\
! 		Hace una petici&#243;n de un objeto \textit{unshared} del sistema de
! 		ficheros del slice \texttt{slice\_name} al nodo
! 		\texttt{node\_name}.
! 		\\
! 		Si la operaci&#243;n falla, se reintentar&#225; un n&#250;mero finito de veces.
  
! 	\item[\texttt{deleteFileShared (slice\_name, path)}] :\\
! 		Hace la petici&#243;n de eliminaci&#243;n de un objeto del sistema de
! 		ficheros (\texttt{path}) de todo el slice \texttt{slice\_name}.
! 		\\
! 		&#201;sta operaci&#243;n provoca un aumento del n&#250;mero de versi&#243;n del
! 		slice.
  
! 	\item[\texttt{deleteFileUnshared (slice\_name, node\_name, path)}] :\\
! 		Hace la petici&#243;n de eliminaci&#243;n de un objeto del sistema de
! 		ficheros (\texttt{path}) de un nodo \texttt{node\_name} del
! 		slice \texttt{slice\_name}.
! 		\\
! 		Si la operaci&#243;n falla, se reintentar&#225; un n&#250;mero finito de veces.
  
! 	\item[\texttt{getSliceNodeVersion (slice\_name, node\_name)}] :\\
! 		Permite obtener la versi&#243;n del slice \texttt{slice\_name} desde
! 		el nodo \texttt{node\_name}.
  
! 	\item[\texttt{registerNeeded (slice\_name, node\_name)}] :\\
! 		Petici&#243;n de necesidad de registro.
! 	
! 	\item[\texttt{addNodeToSlice (slice\_name, node\_name)}] :\\
! 		Informa de la adici&#243;n de un nodo a un slice.
! 
! 	\item[\texttt{removeNodeToSlice (slice\_name, node\_name)}] :\\
! 		Informa de la eliminaci&#243;n de un nodo a un slice.
! 
! 	\item[\texttt{invalidate (slice\_name, node\_name, type, path)}] :\\
! 		Invalida un objeto del sistema de ficheros (un fichero o un
! 		directorio).
  \end{description}
  
+ En todas las anteriores operaciones, excepto las cuatro &#250;ltimas, se puede dar el
+ caso de que el/los \dplc destinatarios de las operaciones no hayan pasado por
+ un proceso de registro desde el \dpld or&#237;gen, por lo que se puede recibir un
+ aviso de necesidad de registro, al cual reaccionar&#225; con un registro
+ (\textit{unicast} o \textit{multicast} seg&#250;n el n&#250;mero de destinatarios de la
+ operaci&#243;n).
+ 
  
  
***************
*** 102,151 ****
  nodos de PlanetLab.
  
! En concreto, las operaciones que ofrece (API), son:
  
  \begin{description}
! 	\item[\texttt{getSlices}] :\\
  		Permite obtener la lista de slices que se han dado de alta en
  		PlanetLab
  
! 	\item[\texttt{getNodes}] :\\
  		Permite obtener la lista de nodos que se han dado de alta en
  		PlanetLab
  
! 	\item[\texttt{getSlice (slice\_name)}] :\\
! 		Permite obtener informaci&#243;n de un slice
  
! 	\item[\texttt{getNode (node\_name)}] :\\
! 		Permite obtener informaci&#243;n de un nodo
  
! 	\item[\texttt{getSliceNodes (slice\_name)}] :\\
! 		Permite obtener los nodos que hay en un slice
  
  	\item[\texttt{getNodeSlices (node\_name)}] :\\
! 		Permite obtener los slices de un nodo
  
  	\item[\texttt{getSliceNode (slice\_name, node\_name)}] :\\
  		Permite obtener informaci&#243;n de un nodo de un slice
  
! 	\item[\texttt{getNodeSlice (node\_name, slice\_name)}] :\\
! 		Permite obtener informaci&#243;n de un slice de un nodo
! 
! 	\item[\texttt{addSlice (slice\_name)}] :\\
! 		Permite a&#241;adir un slice
! 
! 	\item[\texttt{addNode (node\_name)}] :\\
! 		Permite a&#241;adir un nodo
! 
! 	\item[\texttt{addSliceNode (slice\_name, node\_name)}] :\\
! 		Permite a&#241;adir un nodo en un slice
! 
! 	\item[\texttt{removeSlice (slice\_name)}] :\\
! 		Permite eliminar un slice
! 
! 	\item[\texttt{removeNode (node\_name)}] :\\
! 		Permite eliminar un nodo
  
! 	\item[\texttt{removeSliceNode (slice\_name, node\_name)}] :\\
! 		Permite eliminar un nodo de un slice
  \end{description}
  
--- 148,185 ----
  nodos de PlanetLab.
  
! Las operaciones que ofrece son:
  
  \begin{description}
! 	\item[\texttt{getSlices ()}] :\\
  		Permite obtener la lista de slices que se han dado de alta en
  		PlanetLab
  
! 	\item[\texttt{getNodes ()}] :\\
  		Permite obtener la lista de nodos que se han dado de alta en
  		PlanetLab
  
! 	\item[\texttt{getSliceNodes (slice\_name)}] :\\
! 		Permite obtener la lista de los nodos que hay en un slice
  
! 	\item[\texttt{getSliceNodesAny (slice\_name, num)}] :\\
! 		Permite obtener una lista de \texttt{num} nodos cualesquiera de
! 		un slice
  
! 	\item[\texttt{getSliceNodesNumber (slice\_name)}] :\\
! 		Permite obtener el numero de nodos que hay en un slice
  
  	\item[\texttt{getNodeSlices (node\_name)}] :\\
! 		Permite obtener la lista de los slices de un nodo
  
  	\item[\texttt{getSliceNode (slice\_name, node\_name)}] :\\
  		Permite obtener informaci&#243;n de un nodo de un slice
  
! 	\item[\texttt{getSliceNodeAny (slice\_name)}] :\\
! 		Permite obtener un nodo cualquiera de un slice
  
! 	\item[\texttt{getSliceNodeNearest (slice\_name)}] :\\
! 		Permite obtener el nodo mas cercado de un slice
! 		(aprovech&#225;ndose, si se da el caso, de un sistema DNS con
! 		soporte para ``localidad'').
  \end{description}
  
***************
*** 158,184 ****
  \label{sect:umcc}
  
! &#201;ste es el m&#243;dulo cliente de la red multicast, b&#225;sicamente tiene dos
! funciones:
  
  \begin{description}
! 	\item Enviar los datos a desplegar al nodo \umcr m&#225;s cercano.
  
! 	\item Recibir los datos a desplegar y hacerlos llegar al m&#243;dulo dplc.
  \end{description}
  
  El primer caso, se da cuando \dpld ordena el despliegue de una
  aplicaci&#243;n/servicio hacia un slice destino. En este caso deber&#225; comunicarse
! con el m&#243;dulo umcr m&#225;s cercano para comunicarle los datos necesarios.
! El segundo caso, se da cuando un nodo \umcr le comunica que se debe desplegar
! una aplicaci&#243;n/servicio en el nodo d&#243;nde \umcc reside, en este caso deber&#225;
! comunicare con el m&#243;dulo \dplc para hacerle llegar los datos de despliegue.
  
  De modo que las operaciones que ofrece, son:
  
- % TODO(~): seguridad a la hora de hacer cambios? umcc va en la m&#225;quina de
- % administraci&#243;n (se puede modificar lo que se quiera)      .... I????
- %TODO(?): ``dns''?  ------- PQ EL GRUPID NO POT SER SLICE_NAME?
- %			    TB SERVIRIA SI VOLEM ENVIAR A SLICE
- %			    DPLD WARNINGS
  \begin{description}
  	\item[\texttt{group\_id getGroup (group\_name)}] :\\
--- 192,216 ----
  \label{sect:umcc}
  
! &#201;ste es el m&#243;dulo cliente de la red multicast, que b&#225;sicamente tiene las
! dos siguientes funcionalidades:
  
  \begin{description}
! 	\item Enviar los datos al nodo \umcr m&#225;s cercano para que los haga
! 		llegar al grupo de destino.
  
! 	\item Recibir los datos procedentes de \umcr y hacerlos llegar al
! 		componente al que van destinados.
  \end{description}
  
  El primer caso, se da cuando \dpld ordena el despliegue de una
  aplicaci&#243;n/servicio hacia un slice destino. En este caso deber&#225; comunicarse
! con el m&#243;dulo \umcr m&#225;s cercano para comunicarle los datos necesarios.
! 
! El segundo caso, se da cuando un nodo \umcc comunica a \dplc que se debe
! desplegar una aplicaci&#243;n/servicio en el nodo d&#243;nde &#233;ste reside. En este caso
! deber&#225; comunicase con &#233;l para hacerle llegar los datos.
  
  De modo que las operaciones que ofrece, son:
  
  \begin{description}
  	\item[\texttt{group\_id getGroup (group\_name)}] :\\
***************
*** 187,211 ****
  
  	\item[\texttt{setOptions (group\_id, options[])}] :\\
! 		%TODO(?): QUE FA AIXO??
! 		%TODO(?): necesario? depende del soft multicast de debajo
  
  	\item[\texttt{sendData (group\_id, data)}] :\\
! 		Permite enviar datos a un grupo
  
  	\item[\texttt{recieveData (group\_id, data)}] :\\
! 		Recibe datos procedentes de un emisor del grupo, y se los pasa
! 		al m&#243;dulo \dplc (quien se encargar&#225; del despliegue.
  
! 	%TODO(~): addGroup / deleteGroup ?
  
! 	\item[\texttt{join (group\_name, node\_name)}] :\\
! 		%TODO(?): hace falta el nombre o ya va bien por remitente?
! 		%TODO(?): c&#243;mo hacer el join de otro?
! 		Permite a&#241;adir un nodo a un grupo multicast
  
! 	\item[\texttt{delete (group\_id, node\_name)}] :\\
! 		%TODO(?): hace falta el nombre o ya va bien por remitente?
! 		%TODO(?): c&#243;mo hacer el delete de otro?
! 		Permite eliminar un nodo de un grupo multicast
  \end{description}
  
--- 219,246 ----
  
  	\item[\texttt{setOptions (group\_id, options[])}] :\\
! 		Permite cambiar las posibles opciones que permita la
! 		implementaci&#243;n concreta del software multicast que usemos.
  
+ 	\item[\texttt{setOptions (group\_id, options[])}] :\\
+ 		Permite obtener las posibles opciones que permita la
+ 		implementaci&#243;n concreta del software multicast que usemos.
+ 	
  	\item[\texttt{sendData (group\_id, data)}] :\\
! 		Permite enviar datos a un grupo.
  
  	\item[\texttt{recieveData (group\_id, data)}] :\\
! 		Permite recibir datos procedentes de un emisor del grupo.
  
! 	\item[\texttt{joinSender (group\_id, node\_name)}] :\\
! 		Permite a&#241;adir un nodo emisor a un grupo multicast.
  
! 	\item[\texttt{deleteSender (group\_id, node\_name)}] :\\
! 		Permite eliminar un nodo emisor de un grupo multicast.
  
! 	\item[\texttt{joinReceiver (group\_id, node\_name)}] :\\
! 		Permite a&#241;adir un nodo receptor a un grupo multicast.
! 
! 	\item[\texttt{deleteReceiver (group\_id, node\_name)}] :\\
! 		Permite eliminar un nodo receptor de un grupo multicast.
  \end{description}
  
***************
*** 214,224 ****
  permite que se puedan utilizar otros proyectos ya realizados.
  
! % TODO(~): estudiarlo!!!
! % El punto que a&#250;n no hemos aclarado es si el propio identificador de grupo
! % multicast ser&#225; el identificador de slice (VOTO POR ELLO!!!!!!!!!!!) o alg&#250;n
! % otro de m&#225;s gen&#233;rico (que requerir&#237;a una nueva capa de administraci&#243;n de
! % grupos multicast).
! %
! % Depende del software de multicast que se utilice! (araneola)
  
  
--- 249,255 ----
  permite que se puedan utilizar otros proyectos ya realizados.
  
! Un punto que queda sin controlar es el acceso restringido a la red multicast,
! es decir faltar&#237;a una autentificaci&#243;n de emisores, por lo que se podr&#237;an
! realizar ataques DoS desde cualquier m&#225;quina a la red.
  
  
***************
*** 228,240 ****
  Estos nodos se encargan de hacer la transmisi&#243;n multicast, con tal de hacer
  llegar los datos a los \umcc del grupo de destino.
  Asumiremos que estos nodos pertenecen a un slice administrativo en el cual no se
  realizan modificaciones o configuraciones maliciosas.
  
! C&#243;mo todos los accesos a la red multicast se hacen a trav&#233;s de \umcc, se puede
  f&#225;cilmente utilizar una implementaci&#243;n ya existente de una red overlay
! multicast en modo usuario, como es Araneola \cite{Araneola}.
  
! %TODO(!!!): EXPLICAR ARANEAOLA SI ES LA UNICA SOLUCIO
! %TODO: Operaciones?
  
  
--- 259,275 ----
  Estos nodos se encargan de hacer la transmisi&#243;n multicast, con tal de hacer
  llegar los datos a los \umcc del grupo de destino.
+ 
  Asumiremos que estos nodos pertenecen a un slice administrativo en el cual no se
  realizan modificaciones o configuraciones maliciosas.
  
! Como todos los accesos a la red multicast se hacen a trav&#233;s de \umcc, se puede
  f&#225;cilmente utilizar una implementaci&#243;n ya existente de una red overlay
! multicast en modo usuario
  
! En cuanto a las implementaciones, hemos encontrado varias \cite{Araneola, ESM,
! LMDD, LSAM}, pero consideramos que Araneola \cite{Araneola} se ajusta m&#225;s a
! nuestras necesidades que las otras implementaciones por el hecho de haber sido
! dise&#241;ada para entornos din&#225;micos y con la fiabilidad en mente, adem&#225;s de ser un
! sistema de comunicaci&#243;n de M a N.
  
  
***************
*** 242,438 ****
  \section{\dplc}
  
- % TODO: pertenece a un slice administrado correctamente (no habr&#225;
- % modificaciones ni configuraciones maliciosas)
- 
  &#201;ste m&#243;dulo, situado en cada una de las m&#225;quinas clientes (o receptoras de las
  aplicaciones de las que queremos hacer el despliegue), se sit&#250;a en un
! slice propio (donde estar&#225;n todos los nodos con &#233;ste m&#243;dulo), y al recibir los
! datos de \umcc acceder&#225; a s&#237; mismo por ssh con tal de poner los ficheros al
! slice destino y seguidamente ejecutar el shell script asociado, en caso de
! estar presente.
  
! Para realizar esta conexi&#243;n por ssh, el nodo del slice dplc necesita los datos
  de clave y password ssh para que \dplc pueda hacer una conexi&#243;n a la propia
  m&#225;quina y acceder a la m&#225;quina virtual asociada al slice de destino.
  
! Para ello, se utiliza el proceso de autentificaci&#243;n descrito en el apartado
! \ref{sect:security}.
  
  De modo que las operaciones que debe implementar este m&#243;dulo son las
  siguientes:
  
- % TODO(?): seguridad?
  \begin{description}
! 	% TODO: que es _la informaci&#243;n_?
! 	\item[\texttt{getInfoNodeSlice(slice\_name)}] :\\
! 		Obtiene la informaci&#243;n de los ficheros tipo \textit{unshared}
! 		de la aplicaci&#243;n desplegada en el slice \textit{slice\_name} en
! 		el mismo nodo que el propio \dplc.
  
! 	% TODO: ELIMINAR: llama a la operaci&#243;n, no la ofrece
! 	\item[\texttt{warnDpldNewNodeInSlice (slice\_name, node\_name)}] :\\
! 		Avisa a los \dpld encargados del slice \textit{slice\_name} de
! 		que se ha agregado un nuevo nodo \textit{node\_name} a dicho a
! 		slice y que por lo tanto se le deber&#237;a realizar el despliegue de
! 		la aplicaci&#243;n.
  
! 	\item[\texttt{deployAppToSlice (slice\_name, ...)}] :\\
! 		Realiza la conexi&#243;n por ssh al slice destino, despliega la
! 		aplicaci&#243;n y ejecuta el script de inicializaci&#243;n.
  
! 	\item[\texttt{registerToDplc (node\_name, slice\_name)}] :\\
! 		Un nodo se registra en \dplc como integrante del slice
! 		slice\_name al que se va realizar despliegue de
! 		aplicaciones.
  
! 	% TODO: para que? se para sola... hace falta ofrecer la opci&#243;n de
! 	% pararla?
! 	\item[\texttt{stopAppInSlice (slice\_name)}] :
! 		El \dplc del nodo ejecutar&#225; script-&gt;stop en el slice
! 		\textit{slice\_name} de dicho nodo. Es decir, detendr&#225; la
! 		aplicaci&#243;n desplegada en &#233;l.
  
! 	\item[\texttt{invalidateObj (slice\_name, name)}] :\\
! 		% TODO(~): utilizar identificador de obj diferente del path?
! 		% hace falta nombre del slice? (path completo) &lt;&lt;-- PATH COMPLET
! 		Invalida un objeto del sistema de ficheros (un fichero o un
! 		directorio)
  
! 	% TODO(?): validaciones? temporalidad?
  \end{description}
  
! % TODO(~): se&#241;alar los triggers de join / delete de los grupos
  
! % TODO:
! %Cuando se a&#241;ade un nodo a un slice en el que se realiza despliegue, se
! %tiene que informar al m&#243;dulo \dpld, puesto que es posible que ya se hubiera
! %realizado un desplagamiento previo a dicho slice y \dpld tenga que realizar un
! %deployment al nodo en concreto.
! %
! %Para ello hemos pensado en un protocolo de &quot;warnings&quot; en el que \dplc realizara
! %comunicaciones multicast hacia los nodos \dpld, comunic&#225;ndoles:
! %	- nodo
! %	- slice
! %
! % ===&gt;&gt;&gt; LA QUAL COSA ACABARIA AMB L'OPERACIO QUE HE AFEGIT addNodeToSlice de
! %	dpld
! %
! % Como diferenciar al entrar un nodo en un grupo de si ya esta al dia o no?
! % - no se hace nada (solo hay cambios si esta presente en un deploy expreso)
! % - tiene la ultima version de los ficheros (numeraci&#243;n + indicador de ultima
! %   version (*))
! % - los tiene igual que la mayoria (checksums + problema bizantino)
! % - los tiene igual que un repositorio (checksums + repositorio - bamboo? -)
! %
! % (*) Puede ser:
! % o el nunmero mas grande que den los miembros del grupo
! %   - deben responder todos
! %   - alguien puede enga&#241;arnos y dar un numero equivocado?
! %   -&gt; dplc nos firma la version -&gt; garantiza la validez (como el algodon,
! %     dplc nuca enga&#241;a)
! % o el numero que den la mayoria
! %   - deben responder todos
! %   - podemos tirar a una version anterior (pq la mayoria de ahora no estaban
! %     presentes en un deployment anterior y decidieron al connectar que la
! %     version era esta)
! % o lo que digan los dpld (mayoria o maximo)
! %   - debe haber dpld en marcha
! %   + #dpld &lt; #dplc
! % o lo que diga un repositorio central
! %   - poca escalabilidad
! %   - punto unico de fallada
! %   + resultados fiables
! %
! % Al detectar un desfase de las versiones (nodo corrupto?), que hacer:
! % - protocolo de las transparencias
! % - parar y esperar al siguiente deployment
! %
! % ---------------------------------------------------------------
! % =&gt; Posible estrategia de sincronizacion por numero de version:
! % ---------------------------------------------------------------
! %
! % =&gt; Precondicion: queremos que todos los nodos tengan el mismo shared
! %     siempre!
! %
! % =&gt; Formato de version:
! % - numeracion consecutiva
! % - numeracion consecutiva con separadores (.), siendo el override del usuario
! %   el que permite avanzar a una version superior no consecutiva 1.9 -&gt; 2.0
! % - fecha en formato universal (estan los nodos de PL en hora?); como los
! %   deploys no son muy frecuentes, es bastante fiable aun con desfases de
! %   reloj
! %
! % =&gt; Como obtener nodos cualesquiera
! % hacer una peticion de la lista a PLC y seleccionar uno aleatoriamente
! %
! % =&gt; Pasos
! % - cada nodo en su slice guarda un fichero con el numero de version que se le
! %   ha desplegado (si no existe, se supono que no hay despliegue previo)
! %   (posibilidad de ver un fichero en unshared?)
! %   (posibilidad de ver un fichero en shared con el valor maximo actual?)
! %
! % - dpld obtiene el numero de version que actualmente hay en la red
! %   (no necesario si es version por fecha)
! %   * cuando:
! %     - al hacer el deploy, pregunta
! %     - en alguna operacion anterior ya lo va haciendo en background por si a
! %       caso
! %     - al hacer el registro unicast, el resultado es la version actual
! %   * como:
! %     - repositorio central (a modo transaccional, solo se actualiza el valor
! %       si se hace el deploy de como minimo N nodos - 1? quizas mas si
! %       utilizamos tecnicas de fallo bizantino? -)
! %     - se guarda el valor en la maquina dpld (demasiado rigido, no podemos
! %       hacer deploy desde diferentes sitios)
! %     - se le hace poner un valor en un fichero al usuario (esta bien como
! %       posibilidad para forzar el valor, pero no como obligacion)
! %     - el valor que nos den de una sola consulta (podria estar desfasado)
! %     - el valor maximo de N preguntas
! %       * si el valor se guarda en el slice, podria estar falseado? a quien le
! %         interesa si es su propio slice?
! %       * si el valor se guarda en dplc, seguro que siempre es el ultimo que
! %         dplc ha visto, no?
! %     - el valor mas ``votado'', por si hubiera alguno falseado (lo habria?)
! %
! % - hace el deploy indicando el nuevo numero de version
! %
! % - existe la posibilidad de que los dplc hagan piggybacking indicando su
! %   version actual, para que dpld:
! %   * se ponga al dia (hay algun problema en coger la mayor aunque solo la
! %     tenga uno? -&gt; pero que no sea de un corrupto!)
! %   * dpld le indique que necesita actualizacion y
! %     - le haga un deploy unicast
! %     - le indique un vecino cercano con la version ``buena''
! %
! % - al arrancar o recuperar la conexion un dplc, obtiene el numero ``bueno''
! %   de version tal como lo hace dpld
! %
! % - si esta desactualizado, obtiene la nueva version de un peer no corrupto
! %
! % - cada dplc guarda los ultimos N nodos (incluyendo dpld) que le han
! %   preguntado su version, si se le cambia la version, lo notifica a estos
! %
! % - cuando llega una notificacion de cambio de version, se hace un redeploy
! %   desde ese peer originador del mensaje
! %   [ deploy virico para los que se han quedado rezagados ]
! %
! % - existencia de un fichero en /plfs/slices/&lt;slice&gt; que al hacerle un cat
! %   intenta contactar todos los nodos y mostrar su estado
! %
! % =&gt; interesante la deteccion de que nodos estan en la misma subred (fichero
! %    de conf de dplc con las redes locales?)
! %    al entrar en funcionamiento, lo pueden hacer en grupo (corte de luz o
! %    reconexion a inet - gracias al RFC TRACK multicast, se da aviso de
! %    eliminacion del grupo multicast, verdad? -)
! %
! % =&gt; las caidas son aleatorias, por lo que afectan a nodo separados (bloqueos
! %    de la maquina) o a una institucion o parte de ella (red local)
! %    =&gt; no hace falta un redeploy multicast (no habra tantos a la vez como
! %       para hacerlo preferible y viable)
! %    =&gt; designar un cabeza de grupo que lo pide a un peer exterior y luego
! %       hace un deploy unicast a
! %       - la direccion multicast del grupo institucional
! %       - cada peer local (si no se puede por multicast... se dara el
! %         caso?)
  
  
--- 277,387 ----
  \section{\dplc}
  
  &#201;ste m&#243;dulo, situado en cada una de las m&#225;quinas clientes (o receptoras de las
  aplicaciones de las que queremos hacer el despliegue), se sit&#250;a en un
! slice propio (donde estar&#225;n todos los nodos con &#233;ste m&#243;dulo), que
! consideraremos administrado correctamente (por lo que no habr&#225; configuraciones
! ni modificaciones maliciosas) y al recibir los datos de \umcc acceder&#225; a s&#237;
! mismo por ssh con tal de poner los ficheros al slice destino y seguidamente
! ejecutar el shell script asociado, en caso de estar presente.
  
! Para realizar esta conexi&#243;n por ssh, el nodo del slice \dplc necesita los datos
  de clave y password ssh para que \dplc pueda hacer una conexi&#243;n a la propia
  m&#225;quina y acceder a la m&#225;quina virtual asociada al slice de destino.
  
! Para ello, se utiliza el proceso de registro descrito en el apartado
! \ref{sect:security}. Pero los datos de conexi&#243;n tambi&#233;n puede obtenerse de otro
! nodo \textit{peer} \dplc, como se detalla en el apartado
! \ref{sect:redeployment}.
  
  De modo que las operaciones que debe implementar este m&#243;dulo son las
  siguientes:
  
  \begin{description}
! 	\item[\texttt{deployToSlice (slice\_name, path\_to, file, type,
! 		new\_version)}] :\\
! 		Despliega el fichero \texttt{file} al path indicado a trav&#233;s de
! 		la conexi&#243;n SSH.
! 		\\
! 		El fichero puede ser \textit{shared} o \textit{unshared} seg&#250;n
! 		indique \texttt{type}, guardando siempre una relaci&#243;n de qu&#233;
! 		tipo es cada fichero.
! 		\\
! 		&#201;sta operaci&#243;n provoca un aumento del n&#250;mero de versi&#243;n del
! 		slice a \texttt{new\_version}.
  
! 	\item[\texttt{getInfo (slice\_name, path)}] :\\
! 		Obtiene la informaci&#243;n de un objeto del sistema de ficheros
! 		(necesario tanto para listar los contenidos de directorios, como
! 		para que \dpld los revalide al re-registrarse).
  
! 	\item[\texttt{getFile (slice\_name, path)}] :\\
! 		Hace una petici&#243;n de un objeto del sistema de ficheros
! 		del slice \texttt{slice\_name}.
! 	
! 	\item[\texttt{deleteFile (slice\_name, path, new\_version)}] :\\
! 		Hace la petici&#243;n de eliminaci&#243;n de un objeto del sistema de
! 		ficheros (\texttt{path}) de todo el slice \texttt{slice\_name}.
! 		\\
! 		&#201;sta operaci&#243;n provoca un aumento del n&#250;mero de versi&#243;n del
! 		slice a \texttt{new\_version}.
  
! 	\item[\texttt{getVersion (slice\_name)}] :\\
! 		Devuelve la versi&#243;n que el nodo tiene del slice
! 		\texttt{slice\_name}.
  
! 	\item[\texttt{getKey (slice\_name, $K_{pub_{d}}$)}] :\\
! 		Informa de la clave p&#250;blica remota y devuelve la propia clave
! 		p&#250;blica.
  
! 	\item[\texttt{getSSH (slice\_name)}] :\\
! 		Devuelve la clave y password SSH del slice \texttt{slice\_name}.
  
! 	\item[\texttt{ping (slice\_name, node\_name, version)}] :\\
! 		Informa de la versi&#243;n de despliegue que
! 		\texttt{node\_name} tiene del slice \texttt{slice\_name}
! 		desplegada.
! 		\\
! 		Como consecuencia, el nodo receptor realiza un \texttt{ping} al
! 		nodo emisor.
! 
! 	\item[\texttt{needDeploy (slice\_name, node\_name, path)}] :\\
! 		Informa de la necesidad de \texttt{node\_name} de recibir un
! 		deploy de \texttt{path} del slice \texttt{slice\_name}.
! 
! 	\item[\texttt{registerWithResponse (slice\_name, node\_name, $K_{SSH}$,
! 		$P_{SSH}$, $K_{pub}$)}] :\\
! 		Permite a \texttt{node\_name} registrarse en \dplc como
! 		integrante del slice y devuelve un resultado para informar si
! 		se ha podido realizar la conexi&#243;n SSH.
! 
! 	\item[\texttt{register (slice\_name, node\_name, $K_{SSH}$, $P_{SSH}$,
! 		$K_{pub}$)}] :\\
! 		Permite a \texttt{node\_name} registrarse en \dplc como
! 		integrante del slice.
! 
! 	\item[\texttt{joined (slice\_name)}] :\\
! 		Informa de la adici&#243;n del nodo al grupo multicast del slice
! 		\texttt{slice\_name}.
! 
! 	\item[\texttt{deleted (slice\_name)}] :\\
! 		Informa de la eliminaci&#243;n del nodo al grupo multicast del slice
! 		\texttt{slice\_name}.
! 
! 	\item[\texttt{invalidate (slice\_name, path)}] :\\
! 		Invalida un objeto del sistema de ficheros (un fichero o un
! 		directorio).
! 		\\
! 		Si el fichero estaba en la lista, que \dplc mantiene,
! 		como \textit{shared}, ejecuta un \texttt{script-&gt;stop}.
  \end{description}
  
! Como consecuencia de las operaciones de registro, \dplc intenta abrir la
! conexi&#243;n SSH con los datos obtenidos (si no estaba abierta), o los verifica (si
! ya estaba abierta), iniciando o reseteando el contador de \textit{timeout} del
! registro y arrancando una instancia de \famon para &#233;se slice si no estaba ya en
! marcha.
  
! Cuando ``salta'' el \textit{timeout}, si no quedan m&#225;s registros para &#233;se
! slice, se apaga \famon y se cierra la conexi&#243;n SSH.
  
  
***************
*** 444,467 ****
  
  Para ello debe haber una instancia corriendo en cada m&#225;quina virtual que
! corresponda a un slice que funciona a traves de \plfs.
  
! % TODO: quien/cuando se arranca?
! % TODO: que se vilgila?
! % - ficheros desplegados expresamente
! % - ficheros resultado de (genereados despues de) desplegar y/o arrancar la aplicacion
! %   (como se detecta que crea la aplicacion y que lo hace el usuario u otro proceso que no es
! %   la aplicacion desplegada?)
! %   (si se detecta, como diferenciar shared/unshared?)
! %
! % Yo solo vigilaria lo desplegado expresamente.
  
! % TODO (no va aqui)
! % script-&gt;deploy : depliega la aplicacion y guarda una lista de ficheros extra generados en la posible descompresion
! %    (mejor script-&gt;install)
! % script-&gt;remove : elimina los ficheros instalados, para poder instalar otra vez (posible nueva version)
! % script-&gt;update?
  
! % Las operaciones que debe ofrecer \famon son las siguientes:
! % TODO: operaciones de viliar / dejar paths concretos?
  
! % TODO: buscar un buen programa que lo haga =&gt; famd, gamin?
--- 393,414 ----
  
  Para ello debe haber una instancia corriendo en cada m&#225;quina virtual que
! corresponda a un slice que funciona a traves de \plfs, siendo arrancado por el
! correspondiente \dplc en el momento en que hay alg&#250;n \dpld registrado.
  
! Las operaciones que nos interesan son las siguientes:
  
! \begin{description}
! 	\item[\texttt{watch (path)}] :\\
! 		Activa la monitorizaci&#243;n sobre \texttt{path}
  
! 	\item[\texttt{unwatch (path)}] :\\
! 		Desactiva la monitorizaci&#243;n sobre \texttt{path}
  
! 	\item[\texttt{stop ()}] :\\
! 		Detiene \famon
! \end{description}
! 
! Hemos encontrado diversas implementaciones \cite{FAM, Gamin}, y de ellas hemos
! preferido \textit{Gamin} \cite{Gamin}, por implementar un subconjunto m&#225;s
! sencillo de las operaciones que define el modelo de \textit{FAM} definido por
! \textit{SGI} y siendo un programa que consume as&#237; menos recursos.

Index: 04.tex
===================================================================
RCS file: /cvsroot/plfs/doc/04.tex,v
retrieving revision 1.6
retrieving revision 1.7
diff -C2 -d -r1.6 -r1.7
*** 04.tex	13 Jun 2005 19:57:46 -0000	1.6
--- 04.tex	17 Jun 2005 18:57:21 -0000	1.7
***************
*** 8,14 ****
  implementar en cada una, para asegurar autenticidad y confidencialidad.}}
  
  
  
! \section{\kplfs $\rightarrow$ \uplfs}
  
  Para que la aplicaci&#243;n en modo usuario pueda enterarse de las operaciones
--- 8,32 ----
  implementar en cada una, para asegurar autenticidad y confidencialidad.}}
  
+ Las comunicaciones entre componentes de un mismo nodo funcionan a modo de
+ llamadas ``normales'' entre partes de un mismo programa (aunque se carguen en
+ forma de \textit{plugins}).
  
+ La comunicaci&#243;n entre componentes de nodos separados se realiza siempre mediante
+ el paso de mensajes que siguen &#233;ste formato:
  
! \begin{enumerate}
! 	\item IDoperaci&#243;n
! 
! 	\item Remitente
! 
! 	\item Datos de la operaci&#243;n (par&#225;metros o resultado)
! \end{enumerate}
! 
! Seguidamente pasamos a concretar los detalles de comunicaci&#243;n de los diferentes
! componentes.
! 
! 
! 
! \section{\kplfs $\longleftrightarrow$ \uplfs}
  
  Para que la aplicaci&#243;n en modo usuario pueda enterarse de las operaciones
***************
*** 17,21 ****
  Para realizar esta comunicaci&#243;n de eventos, hay varias posibilidades:
  
- % TODO(~): hay mas posibilidades?
  \begin{description}
  	\item [Dispositivo:]
--- 35,38 ----
***************
*** 34,180 ****
  utilizaremos un \textbf{dispositivo} para comunicar ambos componentes.
  
- 
- \subsection{Tipos de mensajes}
- 
- % TODO: hace falta esta explicaci&#243;n? ya esta hecha antes...
  Cuando se realice una operaci&#243;n (\texttt{ls},\texttt{mv}, etc) en el sistema
  de ficheros \plfs, el m&#243;dulo de kernel \kplfs que implementa estas operaciones,
  deber&#225; informar dichos eventos al m&#243;dulo a nivel usuario \uplfs (el cual deber&#225;
! realizar las operaciones pertinentes).
! Para ello, hemos pensado que el m&#243;dulo \kplfs realizar&#237;a la comunicaci&#243;n de
! estos eventos a \uplfs mediante paso de mensajes, que deber&#225;n tener el
! siguiente formato:
! 
! % TODO: aclararlo con las operaciones de 02.tex/03.tex
! 	Campo1: Operaci&#243;n (&quot;ls&quot;, &quot;mv&quot;, ...)
! 	Campo2: Parametros (fichero, directorio...)
! 
! 
! 
! \section{\uplfs $\rightarrow$ \pldb}
! %TODO: REVISAR :PPPP
! Cuando el usuario quiere mirar el contenido del directorio de slices o de
! nodos de un slice, y \kplfs remite dicha operaci&#243;n a \uplfs, este realiza la
! consulta de dicha informaci&#243;n llamando a las operaciones que ofrece \pldb, por
! ejemplo mediante llamadas a procedimiento remoto (como podr&#237;a ser Java RMI).
  
! Una vez obtenida la informaci&#243;n, \uplfs proceder&#237;a a pas&#225;rsela a \kplfs el cual
! actualizar&#237;a los inodos de \plfs y mostrar&#237;a el resultado al usuario.
  
  
  
! \section{\uplfs $\rightarrow$ \dpld}
  
! % TODO: forma de comunicaci&#243;n
! % TODO: formato de los mensajes
  
  
  
! \section{\dpld $\rightarrow$ \pldb}
  
! %TODO(~) An&#225;logamente a \uplfs -&gt; \pldb
  
  
  
  \section{\dpld $\rightarrow$ \dplc}
- % TODO: problema de comunicaci&#243;n directa: puertos de destino?
- % no es lo mismo dpld-&gt;dplc que dpld-&gt;ummc-&gt;dplc!
- 
- En primer lugar antes de realizar ninguna operaci&#243;n de despliegue o consulta a
- slice destinos, debemos realizar el proceso de registro y
- autentificaci&#243;n siguiente:
- 
- \begin{enumerate}
- 	\item \dpld obtiene la clave p&#250;blica del slice \dplc.
- 
- 	\item \dpld selecciona un nodo cualquiera del slice \dplc.
- 
- 	\item \dpld se registra en dicho nodo cifrando con la clave p&#250;blica de
- 		\dplc la clave y password de ssh, y adem&#225;s manda su propia clave
- 		p&#250;blica.
- 
- 	\item El nodo \dplc descifra la clave y password ssh e intenta la conexi&#243;n
- 		ssh al slice destino. En caso negativo se manda un mensaje de retorno
- 		y se aborta el proceso de registro.
- 
- 		\begin{center}
- 			\texttt{ssh \textless{}slice\_destino\textgreater{}@localhost -i
- 			\textless{}clave\_privada\textgreater}
- 		\end{center}
- 
- 	\item El nodo \dplc manda un mensaje a \dpld aceptando el registro. En el
- 		caso que dicho mensaje de aceptaci&#243;n tarde un cierto tiempo
- 		\textit{timeout} se repiten los pasos anteriores con otro nodo \dplc.
- 
- 	\item \dpld env&#237;a registro multicast al slice destino con el password y
- 		clave ssh cifrados con la clave p&#250;blica de \dplc y adem&#225;s env&#237;a la
- 		clave p&#250;blica de \dpld.
- 
- 	\item Todos los nodos \dplc a los que llega el mensaje, realizan la
- 		conexi&#243;n ssh y almacenan la asociaci&#243;n \textless slice destino - \dpld
- 		origen - clave p&#250;blica del \dpld \textgreater para futuras
- 		comunicaciones.
- \end{enumerate}
  
! Una vez realizado el proceso de registro, cualquier operaci&#243;n (despliegue,
! consulta, etc) se realizar&#225; siguiendo los pasos siguientes:
! 
! \begin{enumerate}
! 	\item \dpld cifra los datos a enviar con la clave p&#250;blica de \dplc.
! 
! 	\item \dpld crea una firma de los datos con su clave privada.
! 
! 	\item \dplc comprueba la firma y descifra los datos.
! 
! 	\item \dplc realiza la operaci&#243;n deseada.
! \end{enumerate}
! 
! Si en alg&#250;n momento se contacta con alg&#250;n nodo \dplc reiniciado o nuevo con el
! que no se est&#225; registrado, se procede a realizar un registro unicast nuevo.
! 
! 
! \subsection{Tipos de mensajes}
! 
! \begin{figure}[h]
! 	\centering
! 	\includegraphics[scale=0.5]{paq_-_uplfs-pldb.eps}
! 	\caption{Paquete gen&#233;rico \uplfs $\rightarrow$ \dplc}
! 	\label{fig:paq_-_uplfs-pldb}
! \end{figure}
! 
! 
! \subsubsection{Datos}
! % TODO: En caso de que el paquete contenga el campo \texttt{Cmd}.....
! 
! \begin{figure}[h]
! 	\centering
! 	\includegraphics[scale=0.5]{paq_-_uplfs-pldb_-_data.eps}
! 	\caption{Paquete de datos \uplfs $\rightarrow$ \dplc}
! 	\label{fig:paq_-_uplfs-pldb_-_data}
! \end{figure}
! 
! % TODO: Datos =&gt; lista ficheros + ficheros?
! %       (kernel permite agrupaci&#243;n de ficheros en un env&#237;o?)
! % TODO: si ya se pre-acredita, no hace falta ahora enviar passwd+clave
! % TODO: otros tipos de paquete?
! 
! \begin{description}
! 	\item[Slice destino:] nombre del slice destino al que va destinado el
! 		paquete
! 
! 	\item[Clave/Password privados ssh:] clave y password privados de ssh
! para
! 		que	\dplc se comunique con el slice destino
! 
! 	\item[Datos:] datos a desplegar en el slice destino
! 
! 	\item[Shell script:] shell script a ejecutar una vez desplegados los
! 		ficheros (optativo)
! \end{description}
! 
! 
! \subsubsection{Registro de oyente}
  
! % TODO Formate de paquete de registro
  
  
--- 51,87 ----
  utilizaremos un \textbf{dispositivo} para comunicar ambos componentes.
  
  Cuando se realice una operaci&#243;n (\texttt{ls},\texttt{mv}, etc) en el sistema
  de ficheros \plfs, el m&#243;dulo de kernel \kplfs que implementa estas operaciones,
  deber&#225; informar dichos eventos al m&#243;dulo a nivel usuario \uplfs (el cual deber&#225;
! realizar las operaciones pertinentes, comunic&#225;ndose con los diferentes
! componentes disponibles).
  
! En &#233;ste caso, aunque los componentes est&#233;n en el mismo nodo, deben enviarse
! mensajes a trav&#233;s del dispositivo de control, mensajes que tienen el mismo
! formato que se ha comentado al inicio del cap&#237;tulo, pero sin el par&#225;metro de
! \texttt{Remitente}.
  
  
  
! \section{\uplfs/\dpld $\rightarrow$ \pldb}
  
! Llamada interna de funciones.
  
  
  
! \section{\uplfs $\longleftrightarrow$ \dpld}
  
! Llamada interna de funciones.
  
  
  
  \section{\dpld $\rightarrow$ \dplc}
  
! Todas las comunicaciones van cifradas con $K_{pub_{c}}$ (para asegurar que s&#243;lo
! los \dplc puedan leer los contenidos) y firmadas con $K_{priv_{d}}$ (para poder
! as&#237; comprovar su autenticidad).
  
! Luego, en el destino, se comprueba la firma, se descifran los datos y se
! procede a realizar la operaci&#243;n.
  
  
***************
*** 183,210 ****
  \label{sect:warnings}
  
! Los nodos \dplc deben informar de los cambios en los grupos slice destino
! a los \dpld que est&#225;n registrados en ellos para esos slice.
! 
! Los cambios pueden ser:
! \begin{itemize}
! 	\item Se a&#241;ade un nuevo nodo al slice.
! 
! 	\item Se quita un nodo del slice.
! 
! 	\item Modificaci&#243;n de ficheros (invalidaci&#243;n).
  
! 	\item Se requiere autentificaci&#243;n.
! \end{itemize}
  
- Para anunciar estos cambios hemos dise&#241;ado un paquete de warning mediante el
- cual los nodos \dplc los anuncian a los \dpld registrados.
  
- %TODO figura paquete warning
- Campo 1: tipo de cambio.
- Campo 2: string (nodo o fichero).
- Campo 3: shared o unshared
- ...
  
! Para autentificar los paquetes de warning se firman con la clave privada
! de \dplc.
  
--- 90,108 ----
  \label{sect:warnings}
  
! Todas las comunicaciones van cifradas con $K_{pub_{d}}$ (para asegurar que s&#243;lo
! los \dplc puedan leer los contenidos) y firmadas con $K_{priv_{c}}$ (para poder
! as&#237; comprovar su autenticidad).
  
! Luego, en el destino, se comprueba la firma, se descifran los datos y se
! procede a realizar la operaci&#243;n.
  
  
  
! \section{\dpld/\dplc $\rightarrow$ \umcc}
  
+ Cuando tanto \dpld como \dplc quieren comunicarse con un grupo (\dpld se
+ comunica con el grupo de receptores del slice para mandarles operaciones y
+ \dplc se comunica con el grupo de emisores para mandarles avisos), lo hacen a
+ trav&#233;s de la red multicast, mediante el componente de entrada a ella que es
+ \umcc, de forma que mandan los datos como si fueran directamente una de las dos
+ comunicaciones anteriores y se transmiten transparentemente por la red \umc.

Index: ToDo.txt
===================================================================
RCS file: /cvsroot/plfs/doc/ToDo.txt,v
retrieving revision 1.2
retrieving revision 1.3
diff -C2 -d -r1.2 -r1.3
*** ToDo.txt	7 Jun 2005 14:47:59 -0000	1.2
--- ToDo.txt	17 Jun 2005 18:57:21 -0000	1.3
***************
*** 1,6 ****
  Multicast:
  ---------
- -&gt;<A HREF="http://www.ee.technion.ac.il/people/idish/Abstracts/araneola.html">http://www.ee.technion.ac.il/people/idish/Abstracts/araneola.html</A>
- -&gt;<A HREF="http://dsl.cs.technion.ac.il/completed_projects/Araneola/html/araneola.htm">http://dsl.cs.technion.ac.il/completed_projects/Araneola/html/araneola.htm</A>
  &gt; <A HREF="http://www.aqualab.cs.northwestern.edu/projects/Nemo.html">http://www.aqualab.cs.northwestern.edu/projects/Nemo.html</A>
  &gt; <A HREF="http://www.aqualab.cs.northwestern.edu/publications/SBirrer04RGU.pdf">http://www.aqualab.cs.northwestern.edu/publications/SBirrer04RGU.pdf</A>
--- 1,4 ----


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000012.html">[Plfs-svn] doc 03.tex,1.9,1.10
</A></li>
	<LI>Next message: <A HREF="000014.html">[Plfs-svn] doc 00-plfs.bib,1.5,1.6
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#13">[ date ]</a>
              <a href="thread.html#13">[ thread ]</a>
              <a href="subject.html#13">[ subject ]</a>
              <a href="author.html#13">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plfs-svn">More information about the Plfs-svn
mailing list</a><br>
</body></html>
