From nobody at sheep.berlios.de  Wed Jun  1 00:50:29 2005
From: nobody at sheep.berlios.de (guruburux)
Date: Wed, 1 Jun 2005 00:50:29 +0200
Subject: [Plfs-svn] doc 02.tex,1.1,1.2 05.tex,1.1,1.2
Message-ID: <200505312250.j4VMoTm25997@bat.berlios.de>

Update of /cvsroot/plfs/doc
In directory sheep:/tmp/cvs-serv30068/doc

Modified Files:
	02.tex 05.tex 
Log Message:
Modificacions a l'apartat 2 segons el que vam quedar per a la presentacio!


Index: 02.tex
===================================================================
RCS file: /cvsroot/plfs/doc/02.tex,v
retrieving revision 1.1
retrieving revision 1.2
diff -C2 -d -r1.1 -r1.2
*** 02.tex	27 May 2005 19:44:51 -0000	1.1
--- 02.tex	31 May 2005 22:50:26 -0000	1.2
***************
*** 66,69 ****
--- 66,77 ----
  \section{L?gica del sistema}
  
+ El sistema, adem?s de las funcionalidades propias de despliegue de
+ aplicaciones, debe poder realizar las siguientes operaciones para poder saber
+ como llegar y a d?nde puede llegar. Es decir, debe poder conseguir la
+ informaci?n necesaria sobre los slices disponibles en PlanetLab y sus nodos, y
+ por otra parte, debe poder descubrir su punto de entrada a la red multicast
+ desde el exterior. \\
+ De modo que el sistema debe permitir:
+ 
  \begin{itemize}
  	\item Descubrimiento de slices en PlanetLab \\
***************
*** 85,89 ****
  		Como en el caso anterior, nos hace falta preguntar a PlanetLab por los
  		nodos, tanto a nivel general, como por los asociados a un slice.
! 
  	\item Descubrimiento del nodo de entrada a la red de distribuci?n
  		multicast.
--- 93,97 ----
  		Como en el caso anterior, nos hace falta preguntar a PlanetLab por los
  		nodos, tanto a nivel general, como por los asociados a un slice.
! 	
  	\item Descubrimiento del nodo de entrada a la red de distribuci?n
  		multicast.
***************
*** 91,96 ****
  
  Con tal de minimizar la latencia y ocupaci?n innecesaria del ancho de banda,
! para todos los anteriores puntos hace falta encontrar el nodo m?s cercano que
! da un servicio concreto.
  
  Para ello, se podr?a utilizar un servicio DNS al estilo Akamai, tal como
--- 99,104 ----
  
  Con tal de minimizar la latencia y ocupaci?n innecesaria del ancho de banda,
! para todos los anteriores puntos hace falta encontrar el nodo \textbf{m?s
! cercano} que da un servicio concreto.
  
  Para ello, se podr?a utilizar un servicio DNS al estilo Akamai, tal como
***************
*** 100,111 ****
  
  
- 
  \section{Esquema de \plfs (PlanetLab File System)}
  
! %\newpage
  \begin{code}
  	/plfs
  	|-slices
  	| `-<slice>
  	|   |-key
  	|   |-passwd
--- 108,141 ----
  
  
  \section{Esquema de \plfs (PlanetLab File System)}
  
! Antes de comentar el esquema del ?rbol de ficheros y directorios de \plfs, es
! necesario dejar bien claro que hemos dise?ado el sistema con la idea de
! permitir una cierta flexibilidad al usuario y que pueda diferenciar entre dos
! tipos de ficheros:
! \begin{description}
! 
! 	\item [Los ficheros que se despliegan como shared]:\\
! 		Que todos los nodos del slice destino tendr?n por igual, bajo la
! 		condici?n que adem?s deben de ser inmutables (ej: el programa
! 		principal de la aplicaci?n).
! 	\item [Los ficheros que se despliegan como unshared]:\\
! 		Que ser?n propios de cada nodo (ej: cada nodo podr?a asumir un
! 		rol distinto si tuviera ficheros de configuraci?n propios).
! 
! \end{description}
! 
! Como veremos ahora, y durante todo el documento, esta idea es b?sica para el
! dise?o que planteamos, puesto que en varios puntos el comportamiento del
! sistema depender? del tipo de ficheros que estemos tratando.
! 
! Dicho lo cual, pasamos a presentar el esquema del ?rbol de directorios y
! ficheros como se puede apreciar en la figura:
! 
  \begin{code}
  	/plfs
  	|-slices
  	| `-<slice>
+ 	|   |-backup
  	|   |-key
  	|   |-passwd
***************
*** 121,131 ****
  \end{code}
  
  \begin{description}
  	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/key}] :\\
! 		?ste fichero contiene la clave privada ssh correspondiente al slice.
  
  	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/passwd}] :\\
! 		?ste fichero contiene el password asociado a la clave privada del
! 		slice.
  
  	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/script}] :\\
--- 151,170 ----
  \end{code}
  
+ A continuaci?n una explicaci?n de los diferentes ficheros y directorios:
+ 
  \begin{description}
+ 	
+ 	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/backup}] :\\
+ 		Este fichero contiene una URI que indica el repositorio central
+ 		de backup donde se puedan encontrar los ficheros \texttt{shared}
+ 		en su versi?n original (ver el apartado 2.x sobre Redespliegue).
+ 		
  	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/key}] :\\
! 		Este fichero contiene la clave privada ssh correspondiente al
! 		slice.
  
  	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/passwd}] :\\
! 		Este fichero contiene el password asociado a la clave privada
! 		del slice.
  
  	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/script}] :\\
***************
*** 198,202 ****
  		concreto (representado por el directorio
  		\texttt{/plfs/slices/\textless{slice\textgreater/nodes/\textless{}node\textgreater/unshared}}).
! 		Es decir, \textbf{no} permitiremos operaciones como:
  
  		\begin{itemize}
--- 237,241 ----
  		concreto (representado por el directorio
  		\texttt{/plfs/slices/\textless{slice\textgreater/nodes/\textless{}node\textgreater/unshared}}).
! 		Es decir, \textbf{NO} permitiremos operaciones como:
  
  		\begin{itemize}
***************
*** 213,217 ****
  	\item [\textit{edici?n}] :\\
  		La edici?n de un fichero provocar? el redespliegue del mismo
! 		una vez sea cerrado.
  
  	\item [\textit{lectura}] :\\
--- 252,256 ----
  	\item [\textit{edici?n}] :\\
  		La edici?n de un fichero provocar? el redespliegue del mismo
! 		una vez sea cerrado (ver apartado 2.x sobre Redespliegue)
  
  	\item [\textit{lectura}] :\\
***************
*** 288,295 ****
  \section{L?gica de Desplegamiento}
  
! Cada vez que se realiza un move/copy en el sistema de ficheros para el
! desplegamiento, los pasos a seguir son estos:
  
! \textit{Precondici?n}: El slice destino est? ya creado (\texttt{mkdir}).
  
  \begin{enumerate}
--- 327,356 ----
  \section{L?gica de Desplegamiento}
  
! Despu?s de pensar diferentes posibilidades sobre como el usuario podr?a llevar
! a cabo realmente el despliegue, mediante el sistema de ficheros, hemos llegado
! a la conclusi?n de que el usuario deber? realizar los siguientes pasos:
  
! \begin{enumarete}
! 	
! 	\item 
! 		El usuario realiza \texttt{mv/cp} de los ficheros que
! 		desea desplegar, y estos son transmitidos mediante el proceso de
! 		despliegue que a continuaci?n comentamos.
! 	
! 	\item 
! 		A continuaci?n el usuario realiza \texttt{mv/cp} del script de
! 		arranque de despliegue, que tambi?n es desplegado mediante el
! 		mismo proceso hacia los nodos destino.
! 
! \end{enumarete}
! 
! Adem?s, y como es obvio dada la naturaleza del propio sistema de ficheros, se
! deber? cumplir la siguiente precondici?n antes de poder realizar el despliegue:
! 	\textit{Precondici?n}: El slice destino est? ya creado
! 	(ej: \texttt{mkdir}).\\
! 	
! Una vez aclarado lo anterior, pasamos a describir como se realizar?a de forma
! general el despliegue de los ficheros a trav?s de los diferentes componentes
! del sistema:
  
  \begin{enumerate}
***************
*** 336,342 ****
  		presente. Para ello, primero ejecuta el \textbf{deploy} y luego
  		el \textbf{start}.
! 		En el apartado de sincronizaci?n veremos con m?s detalle las
! 		diferentes posibilidades al realizar la ejecuci?n del script
! 		(sincronizaci?n).
  		%TODO: CITE => cite de que?!
  \end{enumerate}
--- 397,403 ----
  		presente. Para ello, primero ejecuta el \textbf{deploy} y luego
  		el \textbf{start}.
! 		%En el apartado de sincronizaci?n veremos con m?s detalle las
! 		%diferentes posibilidades al realizar la ejecuci?n del script
! 		%(sincronizaci?n).  <<-------- JA NO TE SENTIT
  		%TODO: CITE => cite de que?!
  \end{enumerate}
***************
*** 347,352 ****
  % Si todo es correcto anunciar al dpld??? Escalabilidad? ACK's
  
- 
- 
  \section{Comunicaci?n Multicast}
  
--- 408,411 ----
***************
*** 376,379 ****
--- 435,446 ----
  %TODO CITE
  
+ Por otra parte el m?dulo \dpld tambi?n deber? suscribirse al grupo multicast
+ como oyente, para poder estar atento a los cambios que se produzcan en los
+ ficheros \texttt{shared} (en cuyo caso deber? realizar las operaciones que
+ veremos en el apartado 2.x de Redespliegue).
+ En consecuencia, \dpld deber? poder desuscribirse de un grupo, operaci?n que
+ realizar? o bien cuando se detenga el daemon o transcurrido cierto tiempo de
+ \textit{timeout}. %TODO aclarar este timeout.
+ 
  Para hacer que la red multicast sea fiable nos hemos planteado las siguientes
  posibilidades:
***************
*** 396,436 ****
  
  
  
! \section{Tipos de Desplegamiento}
! \label{sect-deployment}
  
! Los tipos de desplegamiento que hemos considerado interesantes son los
! siguientes:
  
! \begin{description}
! 	\item[Desplegamiento immediato]:\\
! 		Una vez desplegados los ficheros, se ejecuta el script de
! 		desplegamiento y arranque.
  
! 	\item[Desplegamiento sincronizado]:\\
! 		Gracias a la implementaci?n TRACK de Multicast, podemos hacer
! 		un desplegamiento sincronizado a dos pasos, en primer lugar
! 		realizamos el desplegamiento de los ficheros, y una vez recibido
! 		el ACK agrupado realizamos una segunda comunicaci?n multicast
! 		para ejecutar el script de desplegamiento y arranque.
  
! 	\item[Redesplegamiento]:\\
! 		Se realiza de la misma manera que un desplegamiento immediato,
! 		pero como reacci?n a una suscripci?n de un nuevo nodo, o bien
! 		al reintento de desplegamiento en un nodo en el que ha fallado
! 		un desplegamiento previo (NACK).
  
! 		% TODO ESPECIFICAR COMO ELEGIR EL TIPO DE DESPLEGAMIENTO
  
! 		% TODO(~) (Apartats3.2, 3.6 i 4.3):
! 		%
! 		%	A?adir un nodo al slice una vez ya desplegada la
! 		%aplicacion
! 		%
! 		%	Solucio: DPLC d'aquest node, ha de comunicar a DPLD que
! 		%	s'hi afegeix al qual se li ha de fer desplegament!! (3.2, 3.6)
! 		%		=> PROTOCOL DE WARNINGS AMB AUTENTIFICACIO!!! (4.1)
! 		%
! \end{description}
  
  
--- 463,525 ----
  
  
+ \section{Redespliegue}
  
! El componente \fam se encarga de la monitorizaci?n de los ficheros etiquetados
! como \texttt{shared}. De este modo, cuando se produce una modificaci?n
! incontrolada de estos ficheros, este componente avisa al \dplc para que a su
! vez lo notifique a los nodos \dpld responsables de ese fichero, es decir,
! aquellos que est?n suscritos como oyentes de ese grupo multicast, o slice, del
! cual son los responsables de los despliegues que se realicen en ?l.\\
! De modo que entenderemos por modificaci?n incontrolada, aquella que no provenga
! de uno de estos nodos responsables.
  
! Cuando los nodos \dpld responsables se percaten del aviso, proceder?n a
! redesplegar el fichero siguiendo los pasos a continuaci?n:
  
! \begin{enumarate}
  
! 	\item \dplc realiza un script->stop deteniendo moment?neamente la
! 		aplicaci?n en ese nodo corrupto.
! 	
! 	\item se despliegan los ficheros desde un nodo del mismo slice (un nodo
! 		hermano hablando desde el punto de vista de ?rboles de
! 		directorios) que no sea corrupto.
! 	
! 	\item \dplc realiza un script->start para reiniciar la aplicaci?n.
! 	
! \end{enumarate}
  
! Obviamente surge un posible problema como consecuencia de este dise?o, y es que
! podr?a suceder que por circunstancias no existiera ning?n otro nodo no corrupto
! (o bien porque todos los hermanos lo est?n, o bien porque es el ?nico nodo
! activo del slice).
  
! De modo que la soluci?n que proponemos es la de mantener en un repositorio
! central los ficheros originales desplegados como \texttt{shared}.
! La URI de este repositorio es la que se indica en el fichero \texttt{backup}
! que hemos presentado en el esquema de \plfs.
  
! Finalmente, si la recuperaci?n mediante este repositorio no pudiera llevarse a
! cabo, se invalidar?a el nodo del slice, y quedar?a como corrupto.
! 
! 
! 
! 
! 
! 
! 
! 
! 
! 
! 
! % TODO(~) (Apartats3.2, 3.6 i 4.3):
! %
! %	A?adir un nodo al slice una vez ya desplegada la
! %aplicacion
! %
! %	Solucio: DPLC d'aquest node, ha de comunicar a DPLD que
! %	s'hi afegeix al qual se li ha de fer desplegament!! (3.2, 3.6)
! %		=> PROTOCOL DE WARNINGS AMB AUTENTIFICACIO!!! (4.1)
! %
  
  

Index: 05.tex
===================================================================
RCS file: /cvsroot/plfs/doc/05.tex,v
retrieving revision 1.1
retrieving revision 1.2
diff -C2 -d -r1.1 -r1.2
*** 05.tex	27 May 2005 19:44:51 -0000	1.1
--- 05.tex	31 May 2005 22:50:26 -0000	1.2
***************
*** 6,10 ****
  
  
- 
  \section{Listado de nodos}
  
--- 6,9 ----
***************
*** 142,145 ****
  	\caption{Eliminaci?n de un nodo de un grupo}
  	\label{fig:act_-_delete}
! \end{figure}
! 
--- 141,143 ----
  	\caption{Eliminaci?n de un nodo de un grupo}
  	\label{fig:act_-_delete}
! \end{figure}
\ No newline at end of file



From nobody at sheep.berlios.de  Tue Jun  7 16:38:36 2005
From: nobody at sheep.berlios.de (xscript)
Date: Tue, 7 Jun 2005 16:38:36 +0200
Subject: [Plfs-svn] doc 02.tex,1.2,1.3 05.tex,1.2,1.3
Message-ID: <200506071438.j57Eca100498@bat.berlios.de>

Update of /cvsroot/plfs/doc
In directory sheep:/tmp/cvs-serv15429

Modified Files:
	02.tex 05.tex 
Log Message:
Petites correccions d'estil.

Index: 02.tex
===================================================================
RCS file: /cvsroot/plfs/doc/02.tex,v
retrieving revision 1.2
retrieving revision 1.3
diff -C2 -d -r1.2 -r1.3
*** 02.tex	31 May 2005 22:50:26 -0000	1.2
--- 02.tex	7 Jun 2005 14:38:34 -0000	1.3
***************
*** 71,75 ****
  informaci?n necesaria sobre los slices disponibles en PlanetLab y sus nodos, y
  por otra parte, debe poder descubrir su punto de entrada a la red multicast
! desde el exterior. \\
  De modo que el sistema debe permitir:
  
--- 71,76 ----
  informaci?n necesaria sobre los slices disponibles en PlanetLab y sus nodos, y
  por otra parte, debe poder descubrir su punto de entrada a la red multicast
! desde el exterior.
! 
  De modo que el sistema debe permitir:
  
***************
*** 93,97 ****
  		Como en el caso anterior, nos hace falta preguntar a PlanetLab por los
  		nodos, tanto a nivel general, como por los asociados a un slice.
! 	
  	\item Descubrimiento del nodo de entrada a la red de distribuci?n
  		multicast.
--- 94,98 ----
  		Como en el caso anterior, nos hace falta preguntar a PlanetLab por los
  		nodos, tanto a nivel general, como por los asociados a un slice.
! 
  	\item Descubrimiento del nodo de entrada a la red de distribuci?n
  		multicast.
***************
*** 108,111 ****
--- 109,113 ----
  
  
+ 
  \section{Esquema de \plfs (PlanetLab File System)}
  
***************
*** 114,127 ****
  permitir una cierta flexibilidad al usuario y que pueda diferenciar entre dos
  tipos de ficheros:
- \begin{description}
  
  	\item [Los ficheros que se despliegan como shared]:\\
  		Que todos los nodos del slice destino tendr?n por igual, bajo la
  		condici?n que adem?s deben de ser inmutables (ej: el programa
  		principal de la aplicaci?n).
  	\item [Los ficheros que se despliegan como unshared]:\\
  		Que ser?n propios de cada nodo (ej: cada nodo podr?a asumir un
  		rol distinto si tuviera ficheros de configuraci?n propios).
- 
  \end{description}
  
--- 116,129 ----
  permitir una cierta flexibilidad al usuario y que pueda diferenciar entre dos
  tipos de ficheros:
  
+ \begin{description}
  	\item [Los ficheros que se despliegan como shared]:\\
  		Que todos los nodos del slice destino tendr?n por igual, bajo la
  		condici?n que adem?s deben de ser inmutables (ej: el programa
  		principal de la aplicaci?n).
+ 
  	\item [Los ficheros que se despliegan como unshared]:\\
  		Que ser?n propios de cada nodo (ej: cada nodo podr?a asumir un
  		rol distinto si tuviera ficheros de configuraci?n propios).
  \end{description}
  
***************
*** 154,163 ****
  
  \begin{description}
- 	
  	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/backup}] :\\
  		Este fichero contiene una URI que indica el repositorio central
  		de backup donde se puedan encontrar los ficheros \texttt{shared}
  		en su versi?n original (ver el apartado 2.x sobre Redespliegue).
! 		
  	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/key}] :\\
  		Este fichero contiene la clave privada ssh correspondiente al
--- 156,164 ----
  
  \begin{description}
  	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/backup}] :\\
  		Este fichero contiene una URI que indica el repositorio central
  		de backup donde se puedan encontrar los ficheros \texttt{shared}
  		en su versi?n original (ver el apartado 2.x sobre Redespliegue).
! 
  	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/key}] :\\
  		Este fichero contiene la clave privada ssh correspondiente al
***************
*** 252,256 ****
  	\item [\textit{edici?n}] :\\
  		La edici?n de un fichero provocar? el redespliegue del mismo
! 		una vez sea cerrado (ver apartado 2.x sobre Redespliegue)
  
  	\item [\textit{lectura}] :\\
--- 253,258 ----
  	\item [\textit{edici?n}] :\\
  		La edici?n de un fichero provocar? el redespliegue del mismo
! 		una vez sea cerrado.
! 		%TODO \cite (ver apartado 2.x sobre Redespliegue)
  
  	\item [\textit{lectura}] :\\
***************
*** 328,353 ****
  
  Despu?s de pensar diferentes posibilidades sobre como el usuario podr?a llevar
! a cabo realmente el despliegue, mediante el sistema de ficheros, hemos llegado
  a la conclusi?n de que el usuario deber? realizar los siguientes pasos:
  
! \begin{enumarete}
! 	
  	\item 
  		El usuario realiza \texttt{mv/cp} de los ficheros que
  		desea desplegar, y estos son transmitidos mediante el proceso de
  		despliegue que a continuaci?n comentamos.
! 	
  	\item 
  		A continuaci?n el usuario realiza \texttt{mv/cp} del script de
  		arranque de despliegue, que tambi?n es desplegado mediante el
  		mismo proceso hacia los nodos destino.
! 
! \end{enumarete}
  
  Adem?s, y como es obvio dada la naturaleza del propio sistema de ficheros, se
  deber? cumplir la siguiente precondici?n antes de poder realizar el despliegue:
! 	\textit{Precondici?n}: El slice destino est? ya creado
! 	(ej: \texttt{mkdir}).\\
! 	
  Una vez aclarado lo anterior, pasamos a describir como se realizar?a de forma
  general el despliegue de los ficheros a trav?s de los diferentes componentes
--- 330,353 ----
  
  Despu?s de pensar diferentes posibilidades sobre como el usuario podr?a llevar
! a cabo realmente el despliegue mediante el sistema de ficheros, hemos llegado
  a la conclusi?n de que el usuario deber? realizar los siguientes pasos:
  
! \begin{enumerate}
  	\item 
  		El usuario realiza \texttt{mv/cp} de los ficheros que
  		desea desplegar, y estos son transmitidos mediante el proceso de
  		despliegue que a continuaci?n comentamos.
! 
  	\item 
  		A continuaci?n el usuario realiza \texttt{mv/cp} del script de
  		arranque de despliegue, que tambi?n es desplegado mediante el
  		mismo proceso hacia los nodos destino.
! \end{enumerate}
  
  Adem?s, y como es obvio dada la naturaleza del propio sistema de ficheros, se
  deber? cumplir la siguiente precondici?n antes de poder realizar el despliegue:
! 
! \textit{Precondici?n}: El slice destino est? ya creado (ej: \texttt{mkdir}).
! 
  Una vez aclarado lo anterior, pasamos a describir como se realizar?a de forma
  general el despliegue de los ficheros a trav?s de los diferentes componentes
***************
*** 408,411 ****
--- 408,413 ----
  % Si todo es correcto anunciar al dpld??? Escalabilidad? ACK's
  
+ 
+ 
  \section{Comunicaci?n Multicast}
  
***************
*** 439,442 ****
--- 441,445 ----
  ficheros \texttt{shared} (en cuyo caso deber? realizar las operaciones que
  veremos en el apartado 2.x de Redespliegue).
+ 
  En consecuencia, \dpld deber? poder desuscribirse de un grupo, operaci?n que
  realizar? o bien cuando se detenga el daemon o transcurrido cierto tiempo de
***************
*** 459,467 ****
  En nuestro caso, hemos pensado que ser?a positivo elegir la opci?n de TRACK,
  puesto que ?sta nos permitir?a que \dpld realizara los tipos de
! desplegamiento que presentamos en el apartado \ref{sect-deployment}.
  %TODO CITE
  
  
  \section{Redespliegue}
  
  El componente \fam se encarga de la monitorizaci?n de los ficheros etiquetados
--- 462,472 ----
  En nuestro caso, hemos pensado que ser?a positivo elegir la opci?n de TRACK,
  puesto que ?sta nos permitir?a que \dpld realizara los tipos de
! desplegamiento que presentamos en el apartado \ref{sect-redeployment}.
  %TODO CITE
  
  
+ 
  \section{Redespliegue}
+ \label{sect-redeployment}
  
  El componente \fam se encarga de la monitorizaci?n de los ficheros etiquetados
***************
*** 470,474 ****
  vez lo notifique a los nodos \dpld responsables de ese fichero, es decir,
  aquellos que est?n suscritos como oyentes de ese grupo multicast, o slice, del
! cual son los responsables de los despliegues que se realicen en ?l.\\
  De modo que entenderemos por modificaci?n incontrolada, aquella que no provenga
  de uno de estos nodos responsables.
--- 475,480 ----
  vez lo notifique a los nodos \dpld responsables de ese fichero, es decir,
  aquellos que est?n suscritos como oyentes de ese grupo multicast, o slice, del
! cual son los responsables de los despliegues que se realicen en ?l.
! 
  De modo que entenderemos por modificaci?n incontrolada, aquella que no provenga
  de uno de estos nodos responsables.
***************
*** 477,492 ****
  redesplegar el fichero siguiendo los pasos a continuaci?n:
  
! \begin{enumarate}
! 
  	\item \dplc realiza un script->stop deteniendo moment?neamente la
  		aplicaci?n en ese nodo corrupto.
! 	
  	\item se despliegan los ficheros desde un nodo del mismo slice (un nodo
  		hermano hablando desde el punto de vista de ?rboles de
  		directorios) que no sea corrupto.
! 	
  	\item \dplc realiza un script->start para reiniciar la aplicaci?n.
! 	
! \end{enumarate}
  
  Obviamente surge un posible problema como consecuencia de este dise?o, y es que
--- 483,496 ----
  redesplegar el fichero siguiendo los pasos a continuaci?n:
  
! \begin{enumerate}
  	\item \dplc realiza un script->stop deteniendo moment?neamente la
  		aplicaci?n en ese nodo corrupto.
! 
  	\item se despliegan los ficheros desde un nodo del mismo slice (un nodo
  		hermano hablando desde el punto de vista de ?rboles de
  		directorios) que no sea corrupto.
! 
  	\item \dplc realiza un script->start para reiniciar la aplicaci?n.
! \end{enumerate}
  
  Obviamente surge un posible problema como consecuencia de este dise?o, y es que
***************
*** 502,513 ****
  Finalmente, si la recuperaci?n mediante este repositorio no pudiera llevarse a
  cabo, se invalidar?a el nodo del slice, y quedar?a como corrupto.
- 
- 
- 
- 
- 
- 
- 
- 
  
  
--- 506,509 ----

Index: 05.tex
===================================================================
RCS file: /cvsroot/plfs/doc/05.tex,v
retrieving revision 1.2
retrieving revision 1.3
diff -C2 -d -r1.2 -r1.3
*** 05.tex	31 May 2005 22:50:26 -0000	1.2
--- 05.tex	7 Jun 2005 14:38:34 -0000	1.3
***************
*** 6,9 ****
--- 6,10 ----
  
  
+ 
  \section{Listado de nodos}
  
***************
*** 141,143 ****
  	\caption{Eliminaci?n de un nodo de un grupo}
  	\label{fig:act_-_delete}
! \end{figure}
\ No newline at end of file
--- 142,144 ----
  	\caption{Eliminaci?n de un nodo de un grupo}
  	\label{fig:act_-_delete}
! \end{figure}



From nobody at sheep.berlios.de  Tue Jun  7 16:48:01 2005
From: nobody at sheep.berlios.de (xscript)
Date: Tue, 7 Jun 2005 16:48:01 +0200
Subject: [Plfs-svn] doc ToDo.txt,1.1,1.2
Message-ID: <200506071448.j57Em1100780@bat.berlios.de>

Update of /cvsroot/plfs/doc
In directory sheep:/tmp/cvs-serv15891

Modified Files:
	ToDo.txt 
Log Message:
Afegit MON.

Index: ToDo.txt
===================================================================
RCS file: /cvsroot/plfs/doc/ToDo.txt,v
retrieving revision 1.1
retrieving revision 1.2
diff -C2 -d -r1.1 -r1.2
*** ToDo.txt	27 May 2005 19:44:51 -0000	1.1
--- ToDo.txt	7 Jun 2005 14:47:59 -0000	1.2
***************
*** 11,14 ****
--- 11,15 ----
  ----
  Esta al TODO de planetlab
+ pdns(-backend-geo)
  
  FAM:
***************
*** 16,17 ****
--- 17,23 ----
  fam
  gamin
+ 
+ Monitoritzacio:
+ --------------
+ http://cairo.cs.uiuc.edu/mon/index.html
+ http://cairo.cs.uiuc.edu/mon/monclient.html



From nobody at sheep.berlios.de  Thu Jun  9 23:42:25 2005
From: nobody at sheep.berlios.de (guruburux)
Date: Thu, 9 Jun 2005 23:42:25 +0200
Subject: [Plfs-svn] doc 00-plfs.bib,1.1,1.2 02.tex,1.3,1.4 03.tex,1.1,1.2
Message-ID: <200506092142.j59LgP109230@bat.berlios.de>

Update of /cvsroot/plfs/doc
In directory sheep:/tmp/cvs-serv10306/doc

Modified Files:
	00-plfs.bib 02.tex 03.tex 
Log Message:
Quedada poc productiva :P del 8 de Juny


Index: 00-plfs.bib
===================================================================
RCS file: /cvsroot/plfs/doc/00-plfs.bib,v
retrieving revision 1.1
retrieving revision 1.2
diff -C2 -d -r1.1 -r1.2
*** 00-plfs.bib	27 May 2005 19:44:51 -0000	1.1
--- 00-plfs.bib	9 Jun 2005 21:42:23 -0000	1.2
***************
*** 34,37 ****
--- 34,46 ----
  }
  
+ @manual{rfc-track,
+ title = {Tree-Based ACK (TRACK) Building Block for Reliable Multicast
+ Transport},
+ author = {Brian Whetten, Dah Ming Chiu, Miriam Kadansky},
+ organization = {RMT Working Group, IETF},
+ month = {December},
+ year = {2003}
+ }
+ 
  
  ######################
***************
*** 95,97 ****
--- 104,109 ----
  	note = {\\http://www.cs.arizona.edu/stork/}
  }
+ 
+ 
+ 
  

Index: 02.tex
===================================================================
RCS file: /cvsroot/plfs/doc/02.tex,v
retrieving revision 1.3
retrieving revision 1.4
diff -C2 -d -r1.3 -r1.4
*** 02.tex	7 Jun 2005 14:38:34 -0000	1.3
--- 02.tex	9 Jun 2005 21:42:23 -0000	1.4
***************
*** 434,448 ****
  
  El modo en que \dplc realiza las operaciones de inscripci?n/desuscripci?n y en
! qu? momento, lo veremos con m?s detalle en el cap?tulo \ref{TODO}.
! %TODO CITE
  
  Por otra parte el m?dulo \dpld tambi?n deber? suscribirse al grupo multicast
  como oyente, para poder estar atento a los cambios que se produzcan en los
  ficheros \texttt{shared} (en cuyo caso deber? realizar las operaciones que
! veremos en el apartado 2.x de Redespliegue).
  
  En consecuencia, \dpld deber? poder desuscribirse de un grupo, operaci?n que
  realizar? o bien cuando se detenga el daemon o transcurrido cierto tiempo de
! \textit{timeout}. %TODO aclarar este timeout.
  
  Para hacer que la red multicast sea fiable nos hemos planteado las siguientes
--- 434,447 ----
  
  El modo en que \dplc realiza las operaciones de inscripci?n/desuscripci?n y en
! qu? momento, lo veremos con m?s detalle en el apartado \ref{sect:umcc}.
  
  Por otra parte el m?dulo \dpld tambi?n deber? suscribirse al grupo multicast
  como oyente, para poder estar atento a los cambios que se produzcan en los
  ficheros \texttt{shared} (en cuyo caso deber? realizar las operaciones que
! veremos en el apartado \ref{sect:redeployment}).
  
  En consecuencia, \dpld deber? poder desuscribirse de un grupo, operaci?n que
  realizar? o bien cuando se detenga el daemon o transcurrido cierto tiempo de
! \textit{timeout} por inactividad.
  
  Para hacer que la red multicast sea fiable nos hemos planteado las siguientes
***************
*** 454,485 ****
  		decir, no se realizan ACK's.
  
! 	\item[Tree ACK (TRACK)]:\\
! 		En este caso, se env?an ACK's pero agrupados hacia la ra?z del
! 		?rbol multicast. Es decir, los ACK's de los "hijos" se agrupan
! 		en el "padre" para enviar un s?lo ACK.
  \end{description}
  
! En nuestro caso, hemos pensado que ser?a positivo elegir la opci?n de TRACK,
! puesto que ?sta nos permitir?a que \dpld realizara los tipos de
! desplegamiento que presentamos en el apartado \ref{sect-redeployment}.
! %TODO CITE
  
  
  
  \section{Redespliegue}
! \label{sect-redeployment}
  
! El componente \fam se encarga de la monitorizaci?n de los ficheros etiquetados
  como \texttt{shared}. De este modo, cuando se produce una modificaci?n
  incontrolada de estos ficheros, este componente avisa al \dplc para que a su
  vez lo notifique a los nodos \dpld responsables de ese fichero, es decir,
  aquellos que est?n suscritos como oyentes de ese grupo multicast, o slice, del
! cual son los responsables de los despliegues que se realicen en ?l.
  
! De modo que entenderemos por modificaci?n incontrolada, aquella que no provenga
! de uno de estos nodos responsables.
  
! Cuando los nodos \dpld responsables se percaten del aviso, proceder?n a
! redesplegar el fichero siguiendo los pasos a continuaci?n:
  
  \begin{enumerate}
--- 453,491 ----
  		decir, no se realizan ACK's.
  
! 	\item[Tree ACK (TRACK) \cite{rfc-track}]:\\
! 		Los receptores env?an peri?dicamente informes a el nodo padre
! 		sobre lo que han recibido y lo que no (paquetes perdidos).\\
! 		Cada padre agrega los informes de sus hijos en un TRACK y los
! 		pasa a su padre.\\
! 		Se repite el proceso hasta el nodo del que proceden los
! 		paquetes, el cual puede realizar las operaciones
! 		correspondientes seg?n el TRACK total obtenido.
! 
  \end{description}
  
! Creemos que TRACK es m?s completo y nos permitir?a realizar m?s operaciones a
! nivel de despliegue. Por ejemplo, en caso de recibir un TRACK donde se informa
! de que han fallado algunos paquetes, el cliente de multicast \umcc informar?a a
! \dpld el cual se encargar?a de reenviar los datos.
  
  
  
  \section{Redespliegue}
! \label{sect:redeployment}
  
! El componente \famon se encarga de la monitorizaci?n de los ficheros etiquetados
  como \texttt{shared}. De este modo, cuando se produce una modificaci?n
  incontrolada de estos ficheros, este componente avisa al \dplc para que a su
  vez lo notifique a los nodos \dpld responsables de ese fichero, es decir,
  aquellos que est?n suscritos como oyentes de ese grupo multicast, o slice, del
! cual son los responsables de los despliegues que se realicen en ?l, y por
! lo tanto deben encargarse de invalidar los ficheros afectados del sistema de
! ficheros \plfs.
  
! Entenderemos por modificaci?n incontrolada, aquella que no provenga de uno de
! estos nodos responsables (nodos con \dpld).
  
! Cuando \famon avisa al \dplc de ese nodo, ?ste proceder? a redesplegar el
! fichero siguiendo los pasos a continuaci?n:
  
  \begin{enumerate}
***************
*** 487,491 ****
  		aplicaci?n en ese nodo corrupto.
  
! 	\item se despliegan los ficheros desde un nodo del mismo slice (un nodo
  		hermano hablando desde el punto de vista de ?rboles de
  		directorios) que no sea corrupto.
--- 493,497 ----
  		aplicaci?n en ese nodo corrupto.
  
! 	\item se despliega el fichero desde un nodo del mismo slice (un nodo
  		hermano hablando desde el punto de vista de ?rboles de
  		directorios) que no sea corrupto.

Index: 03.tex
===================================================================
RCS file: /cvsroot/plfs/doc/03.tex,v
retrieving revision 1.1
retrieving revision 1.2
diff -C2 -d -r1.1 -r1.2
*** 03.tex	27 May 2005 19:44:51 -0000	1.1
--- 03.tex	9 Jun 2005 21:42:23 -0000	1.2
***************
*** 139,142 ****
--- 139,143 ----
  
  \section{\umcc}
+ \label{sect:umcc}
  
  ?ste es el m?dulo cliente de la red multicast, b?sicamente tiene dos
***************
*** 305,309 ****
  
  ?ste m?dulo est? situado en las m?quinas clientes de la red, y se encarga de
! monitorizar los ficheros que han sido desplegados en un nodo de un slice.
  
  Para ello debe haber una instancia corriendo en cada m?quina virtual que
--- 306,313 ----
  
  ?ste m?dulo est? situado en las m?quinas clientes de la red, y se encarga de
! monitorizar los ficheros que han sido desplegados en un nodo de un slice. De
! modo que cuando detecta una modificaci?n sobre uno de dichos ficheros, \famon
! avisa a \dplc (el cu?l se encargar? de avisar a los \dpld encargados del slice
! acerca de la invalidaci?n del fichero).
  
  Para ello debe haber una instancia corriendo en cada m?quina virtual que
***************
*** 312,315 ****
  % TODO: operaciones de viliar / dejar paths concretos?
  
! % TODO: buscar un buen programa que lo haga => famd?
! 
--- 316,318 ----
  % TODO: operaciones de viliar / dejar paths concretos?
  
! % TODO: buscar un buen programa que lo haga => famd?
\ No newline at end of file



From nobody at sheep.berlios.de  Sun Jun 12 22:13:25 2005
From: nobody at sheep.berlios.de (xscript)
Date: Sun, 12 Jun 2005 22:13:25 +0200
Subject: [Plfs-svn] doc 00-plfs.bib,1.2,1.3 00-plfs.tex,1.1,1.2 02.tex,1.5,1.6 03.tex,1.3,1.4 04.tex,1.2,1.3
Message-ID: <200506122013.j5CKDP122438@bat.berlios.de>

Update of /cvsroot/plfs/doc
In directory sheep:/tmp/cvs-serv28535

Modified Files:
	00-plfs.bib 00-plfs.tex 02.tex 03.tex 04.tex 
Log Message:
Petites correccions d'estil mes TODOs

Index: 00-plfs.bib
===================================================================
RCS file: /cvsroot/plfs/doc/00-plfs.bib,v
retrieving revision 1.2
retrieving revision 1.3
diff -C2 -d -r1.2 -r1.3
*** 00-plfs.bib	9 Jun 2005 21:42:23 -0000	1.2
--- 00-plfs.bib	12 Jun 2005 20:13:22 -0000	1.3
***************
*** 35,44 ****
  
  @manual{rfc-track,
! title = {Tree-Based ACK (TRACK) Building Block for Reliable Multicast
! Transport},
! author = {Brian Whetten, Dah Ming Chiu, Miriam Kadansky},
! organization = {RMT Working Group, IETF},
! month = {December},
! year = {2003}
  }
  
--- 35,43 ----
  
  @manual{rfc-track,
! 	title = {Tree-Based ACK (TRACK) Building Block for Reliable Multicast Transport},
! 	author = {Brian Whetten, Dah Ming Chiu, et al.},
! 	organization = {RMT Working Group, IETF},
! 	month = {December},
! 	year = {2003}
  }
  
***************
*** 104,109 ****
  	note = {\\http://www.cs.arizona.edu/stork/}
  }
- 
- 
- 
  
--- 103,105 ----

Index: 00-plfs.tex
===================================================================
RCS file: /cvsroot/plfs/doc/00-plfs.tex,v
retrieving revision 1.1
retrieving revision 1.2
diff -C2 -d -r1.1 -r1.2
*** 00-plfs.tex	27 May 2005 19:44:51 -0000	1.1
--- 00-plfs.tex	12 Jun 2005 20:13:22 -0000	1.2
***************
*** 75,83 ****
  \renewcommand{\headrulewidth}{0.4pt}
  
! \input{01.tex}
! \input{02.tex}
! \input{03.tex}
! \input{04.tex}
! \input{05.tex}
  
  \pagebreak
--- 75,83 ----
  \renewcommand{\headrulewidth}{0.4pt}
  
! \input{01}
! \input{02}
! \input{03}
! \input{04}
! \input{05}
  
  \pagebreak

Index: 02.tex
===================================================================
RCS file: /cvsroot/plfs/doc/02.tex,v
retrieving revision 1.5
retrieving revision 1.6
diff -C2 -d -r1.5 -r1.6
*** 02.tex	11 Jun 2005 18:25:53 -0000	1.5
--- 02.tex	12 Jun 2005 20:13:22 -0000	1.6
***************
*** 175,178 ****
--- 175,181 ----
  		El script tiene un s?lo par?metro que puede ser:
  
+ 		% TODO(~): NOTA MENTAL: ?al terminar la m?quina virtual, ya
+ 		% ejecutar? dicho startup->stop?
+ 		% => potser caldria comentar-ho a la documentaco (com a que ho donem per fet)
  		\begin{description}
  			\item [start:] lleva a cabo las operaciones para
***************
*** 327,330 ****
--- 330,341 ----
  \section{L?gica de Desplegamiento}
  
+ % TODO: HAY QUE AVISAR DE CUALES SE HA PODIDO DESPLEGAR Y CUALES NO
+ % que hacer con los que se han quedado a medias/han fallado?
+ % estaria bien un fichero de estado? Lista de nodos registrados en PLC en el slice, con:
+ % - ultima operacion realizada
+ % - ultima operacion fallida
+ % - ultima operacion en proceso
+ % - nodo no accesible (no responde)
+ 
  Despu?s de pensar diferentes posibilidades sobre como el usuario podr?a llevar
  a cabo realmente el despliegue mediante el sistema de ficheros, hemos llegado
***************
*** 395,398 ****
--- 406,411 ----
  		presente. Para ello, primero ejecuta el \textbf{deploy} y luego
  		el \textbf{start}.
+ 		% TODO: diferenciar primeros de ultimo fichero
+ 		% ahora el ultimo debe ser el script y se hara el deploy+start
  \end{enumerate}
  
***************
*** 455,459 ****
  		paquetes, el cual puede realizar las operaciones
  		correspondientes seg?n el TRACK total obtenido.
- 
  \end{description}
  
--- 468,471 ----
***************
*** 511,518 ****
  % TODO (!!!!!): Cuando se abren las conexiones SSH? timeout?
  %       Cuando se dan las claves y passwd ssh?
- %
- 
  
  % TODO(?): 'ls' => renew => cheksums??
- s%
- 
--- 523,526 ----

Index: 03.tex
===================================================================
RCS file: /cvsroot/plfs/doc/03.tex,v
retrieving revision 1.3
retrieving revision 1.4
diff -C2 -d -r1.3 -r1.4
*** 03.tex	11 Jun 2005 18:25:53 -0000	1.3
--- 03.tex	12 Jun 2005 20:13:22 -0000	1.4
***************
*** 20,23 ****
--- 20,24 ----
  \pldb para las que hace de intermediario, son las siguientes:
  
+ %TODO: es solo esto?
  \begin{description}
  	\item[\texttt{execute(component, operation, \ldots)}] :\\
***************
*** 28,31 ****
--- 29,33 ----
  
  
+ 
  \section{\dpld}
  
***************
*** 45,48 ****
--- 47,53 ----
  		script de inicializaci?n, y la clave y password de ssh del
  		slice.
+ 		\\
+ 		% TODO: pero ahora hay TRACK!!!
+ 		% primer intento + reintentos a solo a quien falla? (pero por multicast si cierto numero)
  		NOTA: La operaci?n no puede devolver resultado de ?xito,
  		puesto que la implementaci?n de ACKS en Multicast tiene
***************
*** 73,83 ****
  		directorio).
  		%NOTA: de que puede servir invalidar un objeto del FS?
- 		% TODO(?): validaciones? temporalidad?     ES UTIL PLANTEARSELO?
  
  	
  	\item[\texttt{getInfoFilesShared (slice\_name)}] :\\
  		Env?a una petici?n a multicast de informaci?n de los ficheros
  		de tipo \textit{shared} del slice \textit{slice\_name}.
  
  	\item[\texttt{getInfoFilesUnshared (node\_name,slice\_name)}] :\\
  		Env?a una petici?n unicast hacia el \dplc del nodo
--- 78,90 ----
  		directorio).
  		%NOTA: de que puede servir invalidar un objeto del FS?
  
+ 	% TODO(?): validaciones? temporalidad?     ES UTIL PLANTEARSELO?
  	
+ 	% TODO: quien lo utiliza?
  	\item[\texttt{getInfoFilesShared (slice\_name)}] :\\
  		Env?a una petici?n a multicast de informaci?n de los ficheros
  		de tipo \textit{shared} del slice \textit{slice\_name}.
  
+ 	% TODO: quien lo utiliza?
  	\item[\texttt{getInfoFilesUnshared (node\_name,slice\_name)}] :\\
  		Env?a una petici?n unicast hacia el \dplc del nodo
***************
*** 85,89 ****
  		tipo \textit{unshared} del slice \textit{slice\_name} para dicho
  		nodo.
- 		
  \end{description}
  
--- 92,95 ----
***************
*** 186,192 ****
  		Recibe datos procedentes de un emisor del grupo, y se los pasa
  		al m?dulo \dplc (quien se encargar? del despliegue.
! 		
  	%TODO(~): addGroup / deleteGroup ?
! 	
  	\item[\texttt{join (group\_name, node\_name)}] :\\
  		%TODO(?): hace falta el nombre o ya va bien por remitente?
--- 192,198 ----
  		Recibe datos procedentes de un emisor del grupo, y se los pasa
  		al m?dulo \dplc (quien se encargar? del despliegue.
! 
  	%TODO(~): addGroup / deleteGroup ?
! 
  	\item[\texttt{join (group\_name, node\_name)}] :\\
  		%TODO(?): hace falta el nombre o ya va bien por remitente?
***************
*** 198,202 ****
  		%TODO(?): c?mo hacer el delete de otro?
  		Permite eliminar un nodo de un grupo multicast
- 
  \end{description}
  
--- 204,207 ----
***************
*** 226,230 ****
  multicast en modo usuario, como es Araneola \cite{Araneola}.
  
! %TODO(??): EXPLICAR ARANEAOLA SI ES LA UNICA SOLUCIO
  %TODO: Operaciones?
  
--- 231,235 ----
  multicast en modo usuario, como es Araneola \cite{Araneola}.
  
! %TODO(!!!): EXPLICAR ARANEAOLA SI ES LA UNICA SOLUCIO
  %TODO: Operaciones?
  
***************
*** 266,274 ****
  % TODO(?): seguridad?
  \begin{description}
  	\item[\texttt{getInfoNodeSlice(slice\_name)}] :\\
  		Obtiene la informaci?n de los ficheros tipo \textit{unshared}
  		de la aplicaci?n desplegada en el slice \textit{slice\_name} en
  		el mismo nodo que el propio \dplc.
! 		
  	\item[\texttt{warnDpldNewNodeInSlice (slice\_name, node\_name)}] :\\
  		Avisa a los \dpld encargados del slice \textit{slice\_name} de
--- 271,281 ----
  % TODO(?): seguridad?
  \begin{description}
+ 	% TODO: que es _la informacion_?
  	\item[\texttt{getInfoNodeSlice(slice\_name)}] :\\
  		Obtiene la informaci?n de los ficheros tipo \textit{unshared}
  		de la aplicaci?n desplegada en el slice \textit{slice\_name} en
  		el mismo nodo que el propio \dplc.
! 
! 	% TODO: ELIMINAR: llama a la operacion, no la ofrece
  	\item[\texttt{warnDpldNewNodeInSlice (slice\_name, node\_name)}] :\\
  		Avisa a los \dpld encargados del slice \textit{slice\_name} de
***************
*** 276,294 ****
  		slice y que por lo tanto se le deber?a realziar el despliegue de
  		la aplicaci?n.
! 	
  	\item[\texttt{deployAppToSlice (slice\_name, ...)}] :\\
  		Realiza la conexi?n por ssh al slice destino, despliega la
! 		aplicaci?n y ejecuta el script de inicializaci?n.	
! 	
  	\item[\texttt{registerToDplc (node\_name, slice\_name)}] :\\
  		Un nodo se registra en \dplc como integrante del slice
  		slice\_name al que se va realizar desplegamiento de
  		aplicaciones.
! 	
  	\item[\texttt{stopAppInSlice (slice\_name)}] :
  		El \dplc del nodo ejecutar? script->stop en el slice
  		\textit{slice\_name} de dicho nodo. Es decir, detendr? la
  		aplicaci?n desplegada en ?l.
! 	
  	\item[\texttt{invalidateObj (slice\_name, name)}] :\\
  		% TODO(~): utilizar identificador de obj diferente del path?
--- 283,302 ----
  		slice y que por lo tanto se le deber?a realziar el despliegue de
  		la aplicaci?n.
! 
  	\item[\texttt{deployAppToSlice (slice\_name, ...)}] :\\
  		Realiza la conexi?n por ssh al slice destino, despliega la
! 		aplicaci?n y ejecuta el script de inicializaci?n.
! 
  	\item[\texttt{registerToDplc (node\_name, slice\_name)}] :\\
  		Un nodo se registra en \dplc como integrante del slice
  		slice\_name al que se va realizar desplegamiento de
  		aplicaciones.
! 
! 	% TODO: para que? se para sola... hace falta ofrecer la opcion de pararla?
  	\item[\texttt{stopAppInSlice (slice\_name)}] :
  		El \dplc del nodo ejecutar? script->stop en el slice
  		\textit{slice\_name} de dicho nodo. Es decir, detendr? la
  		aplicaci?n desplegada en ?l.
! 
  	\item[\texttt{invalidateObj (slice\_name, name)}] :\\
  		% TODO(~): utilizar identificador de obj diferente del path?
***************
*** 297,313 ****
  		directorio)
  
! 		% TODO(?): validaciones? temporalidad?
! 		
  \end{description}
  
  % TODO(~): se?alar los triggers de join / delete de los grupos
  
  
  
  \section{\famon}
  
! Las operaciones que debe ofrecer \famon son las siguientes:
  
  % TODO: operaciones de viliar / dejar paths concretos?
  
! % TODO: buscar un buen programa que lo haga => famd?
\ No newline at end of file
--- 305,384 ----
  		directorio)
  
! 	% TODO(?): validaciones? temporalidad?
  \end{description}
  
  % TODO(~): se?alar los triggers de join / delete de los grupos
  
+ % TODO:
+ %Cuando se a?ade un nodo a un slice en el que se realiza desplegamiento, se
+ %tiene que informar al m?dulo \dpld, puesto que es posible que ya se hubiera
+ %realizado un desplagamiento previo a dicho slice y \dpld tenga que realizar un
+ %deployment al nodo en concreto.
+ %
+ %Para ello hemos pensado en un protocolo de "warnings" en el que \dplc realizara
+ %comunicaciones multicast hacia los nodos \dpld, comunic?ndoles:
+ %	- nodo
+ %	- slice
+ %
+ % ===>>> LA QUAL COSA ACABARIA AMB L'OPERACIO QUE HE AFEGIT addNodeToSlice de
+ %	dpld
+ %
+ % Como diferenciar al entrar un nodo en un grupo de si ya esta al dia o no?
+ % - no se hace nada (solo hay cambios si esta presente en un deploy expreso)
+ % - tiene la ultima version de los ficheros (numeracion + indicador de ultima version (*))
+ % - los tiene igual que la mayoria (checksums + problema bizantino)
+ % - los tiene igual que un repositorio (checksums + repositorio - bamboo? -)
+ %
+ % (*) Puede ser:
+ % o el nunmero mas grande que den los miembros del grupo
+ %   - deben responder todos
+ %   - alguien puede enga?arnos y dar un numero equivocado?
+ %   -> dplc nos firma la version -> garantiza la validez (como el algodon, 
+ %     dplc nuca enga?a)
+ % o el numero que den la mayoria
+ %   - deben responder todos
+ %   - podemos tirar a una version anterior (pq la mayoria de ahora no estaban
+ %     presentes en un deployment anterior y decidieron al connectar que la
+ %     version era esta)
+ % o lo que digan los dpld (mayoria o maximo)
+ %   - debe haber dpld en marcha
+ %   + #dpld < #dplc
+ % o lo que diga un repositorio central
+ %   - poca escalabilidad
+ %   - punto unico de fallada
+ %   + resultados fiables
+ %
+ % Al detectar un desfase de las versiones (nodo corrupto?), que hacer:
+ % - protocolo de las transparencias
+ % - parar y esperar al siguiente deployment
+ 
  
  
  \section{\famon}
  
! ?ste m?dulo est? situado en las m?quinas clientes de la red, y se encarga de
! monitorizar los ficheros que han sido desplegados en un nodo de un slice.
  
+ Para ello debe haber una instancia corriendo en cada m?quina virtual que
+ corresponda a un slice que funciona a traves de \plfs.
+ 
+ % TODO: quien/cuando se arranca?
+ % TODO: que se vilgila?
+ % - ficheros desplegados expresamente
+ % - ficheros resultado de (genereados despues de) desplegar y/o arrancar la aplicacion
+ %   (como se detecta que crea la aplicacion y que lo hace el usuario u otro proceso que no es
+ %   la aplicacion desplegada?)
+ %   (si se detecta, como diferenciar shared/unshared?)
+ %
+ % Yo solo vigilaria lo desplegado expresamente.
+ 
+ % TODO (no va aqui)
+ % script->deploy : depliega la aplicacion y guarda una lista de ficheros extra generados en la posible descompresion
+ %    (mejor script->install)
+ % script->remove : elimina los ficheros instalados, para poder instalar otra vez (posible nueva version)
+ % script->update?
+ 
+ % Las operaciones que debe ofrecer \famon son las siguientes:
  % TODO: operaciones de viliar / dejar paths concretos?
  
! % TODO: buscar un buen programa que lo haga => famd, gamin?

Index: 04.tex
===================================================================
RCS file: /cvsroot/plfs/doc/04.tex,v
retrieving revision 1.2
retrieving revision 1.3
diff -C2 -d -r1.2 -r1.3
*** 04.tex	11 Jun 2005 18:25:53 -0000	1.2
--- 04.tex	12 Jun 2005 20:13:22 -0000	1.3
***************
*** 7,10 ****
--- 7,11 ----
  
  
+ 
  \section{\kplfs $\rightarrow$ \uplfs}
  
***************
*** 31,38 ****
  utilizaremos un \textbf{dispositivo} para comunicar ambos componentes.
  
- \subsection{Tipos de mensajes}
  
  
! \section{\kplfs $\rightarrow$ \uplfs}
  Cuando se realice una operaci?n ("ls","mv", etc) en el sistema de ficheros
  \plfs, el m?dulo de kernel \kplfs que implementa estas operaciones, deber?
--- 32,39 ----
  utilizaremos un \textbf{dispositivo} para comunicar ambos componentes.
  
  
+ \subsection{Tipos de mensajes}
  
! % TODO: hace falta esta explicacion? ya esta hecha antes...
  Cuando se realice una operaci?n ("ls","mv", etc) en el sistema de ficheros
  \plfs, el m?dulo de kernel \kplfs que implementa estas operaciones, deber?
***************
*** 43,49 ****
  siguiente formato:
  
  	Campo1: Operacion ("ls", "mv", ...)
  	Campo2: Parametros (fichero, directorio...)
! 	
  
  \section{\uplfs $\rightarrow$ \pldb}
--- 44,52 ----
  siguiente formato:
  
+ % TODO: aclararlo con las operaciones de 02.tex/03.tex
  	Campo1: Operacion ("ls", "mv", ...)
  	Campo2: Parametros (fichero, directorio...)
! 
! 
  
  \section{\uplfs $\rightarrow$ \pldb}
***************
*** 58,62 ****
--- 61,67 ----
  
  
+ 
  \section{\uplfs $\rightarrow$ \dpld}
+ 
  % TODO: forma de comunicacion
  % TODO: formato de los mensajes



From nobody at sheep.berlios.de  Sun Jun 12 23:46:14 2005
From: nobody at sheep.berlios.de (xscript)
Date: Sun, 12 Jun 2005 23:46:14 +0200
Subject: [Plfs-svn] doc 00-plfs.tex,1.2,1.3
Message-ID: <200506122146.j5CLkE124307@bat.berlios.de>

Update of /cvsroot/plfs/doc
In directory sheep:/tmp/cvs-serv1493

Modified Files:
	00-plfs.tex 
Log Message:
Pasado el corrector ortografico

Index: 00-plfs.tex
===================================================================
RCS file: /cvsroot/plfs/doc/00-plfs.tex,v
retrieving revision 1.2
retrieving revision 1.3
diff -C2 -d -r1.2 -r1.3
*** 00-plfs.tex	12 Jun 2005 20:13:22 -0000	1.2
--- 00-plfs.tex	12 Jun 2005 21:46:11 -0000	1.3
***************
*** 46,50 ****
  \title{
    \textbf{
!   Desplegamiento Paralelitzado \\
    de Aplicaciones en PlanetLab \\
    a Trav?s del Sistema de Ficheros
--- 46,50 ----
  \title{
    \textbf{
!   Desplegamiento Paralelizado \\
    de Aplicaciones en PlanetLab \\
    a Trav?s del Sistema de Ficheros



From nobody at sheep.berlios.de  Sun Jun 12 23:52:29 2005
From: nobody at sheep.berlios.de (xscript)
Date: Sun, 12 Jun 2005 23:52:29 +0200
Subject: [Plfs-svn] doc 00-plfs.bib,1.3,1.4
Message-ID: <200506122152.j5CLqT124419@bat.berlios.de>

Update of /cvsroot/plfs/doc
In directory sheep:/tmp/cvs-serv2478

Modified Files:
	00-plfs.bib 
Log Message:
Reordenacion de la bilbiografia

Index: 00-plfs.bib
===================================================================
RCS file: /cvsroot/plfs/doc/00-plfs.bib,v
retrieving revision 1.3
retrieving revision 1.4
diff -C2 -d -r1.3 -r1.4
*** 00-plfs.bib	12 Jun 2005 20:13:22 -0000	1.3
--- 00-plfs.bib	12 Jun 2005 21:52:27 -0000	1.4
***************
*** 2,11 ****
  @manual{PLFS,
  	title = {PlanetLab File System},
! 	note = {\\http://developer.berlios.de/projects/plfs}
  }
  
  @manual{ParPipelinedDist,
  	title = {Parallel pipelined wide-area file distribution},
! 	note = {\\http://cs199.planet-lab.org/ideas.html}
  }
  
--- 2,11 ----
  @manual{PLFS,
  	title = {PlanetLab File System},
! 	note = {\url{http://developer.berlios.de/projects/plfs}}
  }
  
  @manual{ParPipelinedDist,
  	title = {Parallel pipelined wide-area file distribution},
! 	note = {\url{http://cs199.planet-lab.org/ideas.html}}
  }
  
***************
*** 14,43 ****
  # Multicast
  
  @manual{ESM,
  	title = {End System Multicast (ESM)},
! 	note = {\\http://esm.cs.cmu.edu/}
  }
  
  @manual{LSAM,
  	title = {Large-Scale Active Multicast},
! 	note = {\\http://www.isi.edu/lsam/}
  }
  
  @manual{LMDD,
  	title = {Logistical Multicast for Data Distribution},
! 	note = {\\http://loci.cs.utk.edu/modules.php?name=Publications&d_op=ViewPublication&lid=243}
! }
! 
! @manual{Araneola,
! 	title = {Araneola},
! 	note = {\\http://}
! }
! 
! @manual{rfc-track,
! 	title = {Tree-Based ACK (TRACK) Building Block for Reliable Multicast Transport},
! 	author = {Brian Whetten, Dah Ming Chiu, et al.},
! 	organization = {RMT Working Group, IETF},
! 	month = {December},
! 	year = {2003}
  }
  
--- 14,45 ----
  # Multicast
  
+ @manual{Araneola,
+ 	title = {Araneola},
+ 	note = {\url{http://}}
+ }
+ 
+ @misc{rfc-track,
+ 	title = {Tree-Based ACK (TRACK) Building Block for Reliable Multicast Transport},
+ 	author = {Brian Whetten et al.},
+ 	howpublished = {RMT Working Group, IETF},
+ 	month = {December},
+ 	year = {2003}
+ }
+ 
+ #--- no utilizados ---
+ 
  @manual{ESM,
  	title = {End System Multicast (ESM)},
! 	note = {\url{http://esm.cs.cmu.edu}}
  }
  
  @manual{LSAM,
  	title = {Large-Scale Active Multicast},
! 	note = {\url{http://www.isi.edu/lsam}}
  }
  
  @manual{LMDD,
  	title = {Logistical Multicast for Data Distribution},
! 	note = {\url{http://loci.cs.utk.edu/modules.php?name=Publications&d_op=ViewPublication&lid=243}}
  }
  
***************
*** 46,105 ****
  # FileSystems
  
  @manual{Intermezzo,
  	title = {Intermezzo File System},
! 	note = {\\http://www.inter-mezzo.org/}
  }
  
  
  ######################
! # Xarxes
  
  @manual{RON,
  	title = {Resilient Overlay Networks},
! 	note = {\\http://nms.lcs.mit.edu/projects/ron/}
  }
  
  
  ######################
! # Desconegut
! 
! @manual{NWS,
! 	title = {Network Weather System},
! 	note = {\\http://nws.cs.ucsb.edu/}
! }
  
! @manual{RSTB,
! 	title = {A Replicated Server Testbed},
! 	note = {\\http://www.cs.utk.edu/~mbeck/replication.html}
  }
  
! @manual{PLSDT,
! 	title = {PlanetLab Slice Deploy Toolkit},
! 	note = {\\http://jabber.services.planet-lab.org/php/software/tools.php}
  }
  
  @manual{PLC,
  	title = {PlanetLab Central API Documentation},
! 	note = {\\http://www.planet-lab.org/doc/api/plc\_api/}
  }
  
! @manual{CoDNS,
! 	title = {CoDNS},
! 	note = {\\http://codeen.cs.princeton.edu/codns/}
  }
  
! @manual{Codeen,
! 	title = {Codeen},
! 	note = {\\http://codeen.cs.princeton.edu/}
  }
  
  @manual{Bamboo,
  	title = {The Bamboo Distributed Hash Table},
! 	note = {\\http://bamboo-dht.org/}
  }
  
  @manual{Stork,
  	title = {Stork},
! 	note = {\\http://www.cs.arizona.edu/stork/}
  }
  
--- 48,113 ----
  # FileSystems
  
+ #--- no utilizados ---
+ 
  @manual{Intermezzo,
  	title = {Intermezzo File System},
! 	note = {\url{http://www.inter-mezzo.org/}}
  }
  
  
  ######################
! # Redes
! 
! #--- no utilizados ---
  
  @manual{RON,
  	title = {Resilient Overlay Networks},
! 	note = {\url{http://nms.lcs.mit.edu/projects/ron/}}
  }
  
  
  ######################
! # Otros
  
! @manual{Codeen,
! 	title = {Codeen},
! 	note = {\url{http://codeen.cs.princeton.edu/}}
  }
  
! @manual{CoDNS,
! 	title = {CoDNS},
! 	note = {\url{http://codeen.cs.princeton.edu/codns/}}
  }
  
  @manual{PLC,
  	title = {PlanetLab Central API Documentation},
! 	note = {\url{http://www.planet-lab.org/doc/api/plc_api/}}
  }
  
! #--- no utilizados ---
! 
! @manual{NWS,
! 	title = {Network Weather System},
! 	note = {\url{http://nws.cs.ucsb.edu/}}
  }
  
! @manual{RSTB,
! 	title = {A Replicated Server Testbed},
! 	note = {\url{http://www.cs.utk.edu/~mbeck/replication.html}}
! }
! 
! @manual{PLSDT,
! 	title = {PlanetLab Slice Deploy Toolkit},
! 	note = {\url{http://jabber.services.planet-lab.org/php/software/tools.php}}
  }
  
  @manual{Bamboo,
  	title = {The Bamboo Distributed Hash Table},
! 	note = {\url{http://bamboo-dht.org/}}
  }
  
  @manual{Stork,
  	title = {Stork},
! 	note = {\url{http://www.cs.arizona.edu/stork/}}
  }
  



From nobody at sheep.berlios.de  Mon Jun 13 00:00:34 2005
From: nobody at sheep.berlios.de (xscript)
Date: Mon, 13 Jun 2005 00:00:34 +0200
Subject: [Plfs-svn] doc 02.tex,1.6,1.7
Message-ID: <200506122200.j5CM0Y124587@bat.berlios.de>

Update of /cvsroot/plfs/doc
In directory sheep:/tmp/cvs-serv3298

Modified Files:
	02.tex 
Log Message:
Pasado el corrector ortografico

Index: 02.tex
===================================================================
RCS file: /cvsroot/plfs/doc/02.tex,v
retrieving revision 1.6
retrieving revision 1.7
diff -C2 -d -r1.6 -r1.7
*** 02.tex	12 Jun 2005 20:13:22 -0000	1.6
--- 02.tex	12 Jun 2005 22:00:32 -0000	1.7
***************
*** 343,353 ****
  
  \begin{enumerate}
! 	\item 
! 		El usuario realiza \texttt{mv/cp} de los ficheros que
  		desea desplegar, y estos son transmitidos mediante el proceso de
  		despliegue que a continuaci?n comentamos.
  
! 	\item 
! 		A continuaci?n el usuario realiza \texttt{mv/cp} del script de
  		arranque de despliegue, que tambi?n es desplegado mediante el
  		mismo proceso hacia los nodos destino.
--- 343,351 ----
  
  \begin{enumerate}
! 	\item El usuario realiza \texttt{mv/cp} de los ficheros que
  		desea desplegar, y estos son transmitidos mediante el proceso de
  		despliegue que a continuaci?n comentamos.
  
! 	\item A continuaci?n el usuario realiza \texttt{mv/cp} del script de
  		arranque de despliegue, que tambi?n es desplegado mediante el
  		mismo proceso hacia los nodos destino.



From nobody at sheep.berlios.de  Mon Jun 13 00:08:15 2005
From: nobody at sheep.berlios.de (xscript)
Date: Mon, 13 Jun 2005 00:08:15 +0200
Subject: [Plfs-svn] doc 03.tex,1.4,1.5 04.tex,1.3,1.4
Message-ID: <200506122208.j5CM8F124786@bat.berlios.de>

Update of /cvsroot/plfs/doc
In directory sheep:/tmp/cvs-serv3727

Modified Files:
	03.tex 04.tex 
Log Message:
Pasado el corrector ortografico

Index: 03.tex
===================================================================
RCS file: /cvsroot/plfs/doc/03.tex,v
retrieving revision 1.4
retrieving revision 1.5
diff -C2 -d -r1.4 -r1.5
*** 03.tex	12 Jun 2005 20:13:22 -0000	1.4
--- 03.tex	12 Jun 2005 22:08:13 -0000	1.5
***************
*** 34,38 ****
  Las operaciones que \dpld ofrece son las siguientes:
  
! % TODO: faltan las credenciales de autentificacion, o mejor un paso previo de
  % autentificaci?n?!?!?!?!
  % TODO(~): faltan los parametros de que desplegar, etc
--- 34,38 ----
  Las operaciones que \dpld ofrece son las siguientes:
  
! % TODO: faltan las credenciales de autentificaci?n, o mejor un paso previo de
  % autentificaci?n?!?!?!?!
  % TODO(~): faltan los parametros de que desplegar, etc
***************
*** 70,74 ****
  		producen en un slice de un nodo concreto.
  		% TODO(?): se puede utilizar un grupo de oyentes a cambios en
! 		% grupos de dplc, asi no hace falta el registro...
  
  	\item[\texttt{invalidateObj (slice\_name, name)}] :\\
--- 70,74 ----
  		producen en un slice de un nodo concreto.
  		% TODO(?): se puede utilizar un grupo de oyentes a cambios en
! 		% grupos de dplc, as? no hace falta el registro...
  
  	\item[\texttt{invalidateObj (slice\_name, name)}] :\\
***************
*** 79,83 ****
  		%NOTA: de que puede servir invalidar un objeto del FS?
  
! 	% TODO(?): validaciones? temporalidad?     ES UTIL PLANTEARSELO?
  	
  	% TODO: quien lo utiliza?
--- 79,83 ----
  		%NOTA: de que puede servir invalidar un objeto del FS?
  
! 	% TODO(?): validaciones? temporalidad?     ES ?TIL PLANTE?RSELO?
  	
  	% TODO: quien lo utiliza?
***************
*** 174,178 ****
  
  % TODO(~): seguridad a la hora de hacer cambios? umcc va en la m?quina de
! % administracion (se puede modificar lo que se quiera)      .... I????
  %TODO(?): ``dns''?  ------- PQ EL GRUPID NO POT SER SLICE_NAME?
  %			    TB SERVIRIA SI VOLEM ENVIAR A SLICE
--- 174,178 ----
  
  % TODO(~): seguridad a la hora de hacer cambios? umcc va en la m?quina de
! % administraci?n (se puede modificar lo que se quiera)      .... I????
  %TODO(?): ``dns''?  ------- PQ EL GRUPID NO POT SER SLICE_NAME?
  %			    TB SERVIRIA SI VOLEM ENVIAR A SLICE
***************
*** 212,220 ****
  % TODO(~): estudiarlo!!!
  % El punto que a?n no hemos aclarado es si el propio identificador de grupo
! % multicast ser? el identificador de slice (VOTO POR ELLO!!!!!!!!!!!) o algun
  % otro de m?s gen?rico (que requerir?a una nueva capa de administraci?n de
  % grupos multicast).
  %
! % Depende del software de multicast que se utilize! (araneola)
  
  
--- 212,220 ----
  % TODO(~): estudiarlo!!!
  % El punto que a?n no hemos aclarado es si el propio identificador de grupo
! % multicast ser? el identificador de slice (VOTO POR ELLO!!!!!!!!!!!) o alg?n
  % otro de m?s gen?rico (que requerir?a una nueva capa de administraci?n de
  % grupos multicast).
  %
! % Depende del software de multicast que se utilice! (araneola)
  
  
***************
*** 222,231 ****
  \section{\umcr}
  
! Estos nodos se encargan de hacer la transimisi?n multicast, con tal de hacer
  llegar los datos a los \umcc del grupo de destino.
! Asumiremos que estos nodos pertencen a un slice administrativo en el cual no se
  realizan modificaciones o configuraciones maliciosas.
  
! C?mo todos los accesos a la red multicast se hacen a traves de \umcc, se puede
  f?cilmente utilizar una implementaci?n ya existente de una red overlay
  multicast en modo usuario, como es Araneola \cite{Araneola}.
--- 222,231 ----
  \section{\umcr}
  
! Estos nodos se encargan de hacer la transmisi?n multicast, con tal de hacer
  llegar los datos a los \umcc del grupo de destino.
! Asumiremos que estos nodos pertenecen a un slice administrativo en el cual no se
  realizan modificaciones o configuraciones maliciosas.
  
! C?mo todos los accesos a la red multicast se hacen a trav?s de \umcc, se puede
  f?cilmente utilizar una implementaci?n ya existente de una red overlay
  multicast en modo usuario, como es Araneola \cite{Araneola}.
***************
*** 238,242 ****
  \section{\dplc}
  
! % TODO: pertenece a un slice administrado correctamente (no habra
  % modificaciones ni configuraciones maliciosas)
  
--- 238,242 ----
  \section{\dplc}
  
! % TODO: pertenece a un slice administrado correctamente (no habr?
  % modificaciones ni configuraciones maliciosas)
  
***************
*** 255,259 ****
  
  \begin{description}
! 	\item Realizar una preautentificaci?n, en la que dplc se autentifica con los
  		slices a los que se llevar? acabo el deployment de aplicaciones, de modo que en
  		las comunicaciones no haga falta realizar el env?o de clave y password y tampoco
--- 255,260 ----
  
  \begin{description}
! 	\item Realizar una pre-autentificaci?n, en la que dplc se autentifica
! con los
  		slices a los que se llevar? acabo el deployment de aplicaciones, de modo que en
  		las comunicaciones no haga falta realizar el env?o de clave y password y tampoco
***************
*** 271,275 ****
  % TODO(?): seguridad?
  \begin{description}
! 	% TODO: que es _la informacion_?
  	\item[\texttt{getInfoNodeSlice(slice\_name)}] :\\
  		Obtiene la informaci?n de los ficheros tipo \textit{unshared}
--- 272,276 ----
  % TODO(?): seguridad?
  \begin{description}
! 	% TODO: que es _la informaci?n_?
  	\item[\texttt{getInfoNodeSlice(slice\_name)}] :\\
  		Obtiene la informaci?n de los ficheros tipo \textit{unshared}
***************
*** 277,285 ****
  		el mismo nodo que el propio \dplc.
  
! 	% TODO: ELIMINAR: llama a la operacion, no la ofrece
  	\item[\texttt{warnDpldNewNodeInSlice (slice\_name, node\_name)}] :\\
  		Avisa a los \dpld encargados del slice \textit{slice\_name} de
  		que se ha agregado un nuevo nodo \textit{node\_name} a dicho a
! 		slice y que por lo tanto se le deber?a realziar el despliegue de
  		la aplicaci?n.
  
--- 278,286 ----
  		el mismo nodo que el propio \dplc.
  
! 	% TODO: ELIMINAR: llama a la operaci?n, no la ofrece
  	\item[\texttt{warnDpldNewNodeInSlice (slice\_name, node\_name)}] :\\
  		Avisa a los \dpld encargados del slice \textit{slice\_name} de
  		que se ha agregado un nuevo nodo \textit{node\_name} a dicho a
! 		slice y que por lo tanto se le deber?a realizar el despliegue de
  		la aplicaci?n.
  
***************
*** 293,297 ****
  		aplicaciones.
  
! 	% TODO: para que? se para sola... hace falta ofrecer la opcion de pararla?
  	\item[\texttt{stopAppInSlice (slice\_name)}] :
  		El \dplc del nodo ejecutar? script->stop en el slice
--- 294,299 ----
  		aplicaciones.
  
! 	% TODO: para que? se para sola... hace falta ofrecer la opci?n de
! 	% pararla?
  	\item[\texttt{stopAppInSlice (slice\_name)}] :
  		El \dplc del nodo ejecutar? script->stop en el slice
***************
*** 326,330 ****
  % Como diferenciar al entrar un nodo en un grupo de si ya esta al dia o no?
  % - no se hace nada (solo hay cambios si esta presente en un deploy expreso)
! % - tiene la ultima version de los ficheros (numeracion + indicador de ultima version (*))
  % - los tiene igual que la mayoria (checksums + problema bizantino)
  % - los tiene igual que un repositorio (checksums + repositorio - bamboo? -)
--- 328,333 ----
  % Como diferenciar al entrar un nodo en un grupo de si ya esta al dia o no?
  % - no se hace nada (solo hay cambios si esta presente en un deploy expreso)
! % - tiene la ultima version de los ficheros (numeraci?n + indicador de ultima
! %   version (*))
  % - los tiene igual que la mayoria (checksums + problema bizantino)
  % - los tiene igual que un repositorio (checksums + repositorio - bamboo? -)

Index: 04.tex
===================================================================
RCS file: /cvsroot/plfs/doc/04.tex,v
retrieving revision 1.3
retrieving revision 1.4
diff -C2 -d -r1.3 -r1.4
*** 04.tex	12 Jun 2005 20:13:22 -0000	1.3
--- 04.tex	12 Jun 2005 22:08:13 -0000	1.4
***************
*** 35,42 ****
  \subsection{Tipos de mensajes}
  
! % TODO: hace falta esta explicacion? ya esta hecha antes...
! Cuando se realice una operaci?n ("ls","mv", etc) en el sistema de ficheros
! \plfs, el m?dulo de kernel \kplfs que implementa estas operaciones, deber?
! informar dichos eventos al m?dulo a nivel usuario \uplfs (el cual deber?
  realizar las operaciones pertinentes).
  Para ello, hemos pensado que el m?dulo \kplfs realizar?a la comunicaci?n de
--- 35,42 ----
  \subsection{Tipos de mensajes}
  
! % TODO: hace falta esta explicaci?n? ya esta hecha antes...
! Cuando se realice una operaci?n (\textt{ls},\texttt{mv}, etc) en el sistema de
! ficheros \plfs, el m?dulo de kernel \kplfs que implementa estas operaciones,
! deber? informar dichos eventos al m?dulo a nivel usuario \uplfs (el cual deber?
  realizar las operaciones pertinentes).
  Para ello, hemos pensado que el m?dulo \kplfs realizar?a la comunicaci?n de
***************
*** 45,49 ****
  
  % TODO: aclararlo con las operaciones de 02.tex/03.tex
! 	Campo1: Operacion ("ls", "mv", ...)
  	Campo2: Parametros (fichero, directorio...)
  
--- 45,49 ----
  
  % TODO: aclararlo con las operaciones de 02.tex/03.tex
! 	Campo1: Operaci?n ("ls", "mv", ...)
  	Campo2: Parametros (fichero, directorio...)
  
***************
*** 52,56 ****
  \section{\uplfs $\rightarrow$ \pldb}
  %TODO: REVISAR :PPPP
! Cuando el usuario quierer mirar el contenido del directorio de slices o de
  nodos de un slice, y \kplfs remite dicha operaci?n a \uplfs, este realiza la
  consulta de dicha informaci?n llamando a las operaciones que ofrece \pldb, por
--- 52,56 ----
  \section{\uplfs $\rightarrow$ \pldb}
  %TODO: REVISAR :PPPP
! Cuando el usuario quiere mirar el contenido del directorio de slices o de
  nodos de un slice, y \kplfs remite dicha operaci?n a \uplfs, este realiza la
  consulta de dicha informaci?n llamando a las operaciones que ofrece \pldb, por
***************
*** 64,68 ****
  \section{\uplfs $\rightarrow$ \dpld}
  
! % TODO: forma de comunicacion
  % TODO: formato de los mensajes
  
--- 64,68 ----
  \section{\uplfs $\rightarrow$ \dpld}
  
! % TODO: forma de comunicaci?n
  % TODO: formato de los mensajes
  
***************
*** 71,75 ****
  \section{\dpld $\rightarrow$ \pldb}
  
! %TODO(~) Analogamente a \uplfs -> \pldb
  
  
--- 71,75 ----
  \section{\dpld $\rightarrow$ \pldb}
  
! %TODO(~) An?logamente a \uplfs -> \pldb
  
  
***************
*** 137,141 ****
  	\centering
  	\includegraphics[scale=0.5]{paq_-_uplfs-pldb.eps}
! 	\caption{Paquete generico \uplfs $\rightarrow$ \dplc}
  	\label{fig:paq_-_uplfs-pldb}
  \end{figure}
--- 137,141 ----
  	\centering
  	\includegraphics[scale=0.5]{paq_-_uplfs-pldb.eps}
! 	\caption{Paquete gen?rico \uplfs $\rightarrow$ \dplc}
  	\label{fig:paq_-_uplfs-pldb}
  \end{figure}
***************
*** 161,165 ****
  		paquete
  
! 	\item[Clave/Passwordd privados ssh:] clave y password privados de ssh para
  		que	\dplc se comunique con el slice destino
  
--- 161,166 ----
  		paquete
  
! 	\item[Clave/Password privados ssh:] clave y password privados de ssh
! para
  		que	\dplc se comunique con el slice destino
  



From nobody at sheep.berlios.de  Mon Jun 13 03:32:26 2005
From: nobody at sheep.berlios.de (xscript)
Date: Mon, 13 Jun 2005 03:32:26 +0200
Subject: [Plfs-svn] doc 02.tex,1.7,1.8 05.tex,1.3,1.4 03.tex,1.5,1.6 04.tex,1.4,1.5
Message-ID: <200506130132.j5D1WQ130295@bat.berlios.de>

Update of /cvsroot/plfs/doc
In directory sheep:/tmp/cvs-serv17023

Modified Files:
	02.tex 05.tex 03.tex 04.tex 
Log Message:
Explicaciones de seguridad

Index: 02.tex
===================================================================
RCS file: /cvsroot/plfs/doc/02.tex,v
retrieving revision 1.7
retrieving revision 1.8
diff -C2 -d -r1.7 -r1.8
*** 02.tex	12 Jun 2005 22:00:32 -0000	1.7
--- 02.tex	13 Jun 2005 01:32:21 -0000	1.8
***************
*** 31,34 ****
--- 31,35 ----
  		comunicaciones (en nuestro caso, ser?a \dpld).
  
+ 	% TODO: intercambiar dpld <--> dplc ?!?! tendria mas sentido?
  	\item [\dpld:] Deployer Daemon\\
  		Componente encargado de mandar las ?rdenes de desplegamiento y
***************
*** 103,110 ****
  cercano} que da un servicio concreto.
  
  Para ello, se podr?a utilizar un servicio DNS al estilo Akamai, tal como
  utiliza google, o algo como CoDNS \cite{CoDNS}, subproyecto de Codeen
  \cite{Codeen}, que nos dan la IP del nodo m?s cercano que proporciona un
! servicio conreto.
  
  
--- 104,168 ----
  cercano} que da un servicio concreto.
  
+ % + TODO: consensuar
  Para ello, se podr?a utilizar un servicio DNS al estilo Akamai, tal como
  utiliza google, o algo como CoDNS \cite{CoDNS}, subproyecto de Codeen
  \cite{Codeen}, que nos dan la IP del nodo m?s cercano que proporciona un
! servicio conreto. Cabe notar, adem?s, que ?ste es un servicio que PlanetLab
! planea dar a todos sus nodos y que tiene como tarea pendiente implementar.
! 
! Como el servicio de DNS se utiliza siempre de forma transparente, aunque d? un
! servicio no habitual de localidad, no lo pondremos en el esquema de nuestros
! componentes para mejorar su claridad.
! 
! 
! 
! \section{Seguridad}
! 
! % TODO: seguramente deber? moverse de sitio
! Puesto que la autentificaci?n de todos los clientes es obligatoria en
! PlanetLab, cada vez que un cliente (\dpld en uestro caso) intente una operaci?n
! con un cliente (\dplc) con el que no est? autentificado, deber? primero relizar
! el proceso de autentificaci?n.
! 
! Dicho proceso va dirigido a proporcionar la clave y password SSH a los \dplc y
! verificar que pueden realizar la conexi?n (que no cerrar?n hasta
! pasado un cierto tiempo de inactividad, para evitar la costosa operaci?n de
! realizar la conexi?n SSH), verificando as? la autenticidad del acceso del
! cliente a un slice concreto.
! 
! Para ello, cada \dpld tiene su par de llaves, que llamaremos $K_{pub_{d}}$ y
! $K_{priv_{d}}$ para las claves p?blica y privada, respectivamente, y los \dplc,
! que son administrados por una ?nica identidad de confianza, tienen todos un
! mismo par de llaves, que llamaremos $K_{pub_{c}}$ y $K_{priv_{c}}$.
! 
! %TODO: como controlar que los datos vienen de alguien ya autentificado?
! 
! %TODO: como avisa un nodo a un cliente que este no esta autentificado?
! 
! Los pasos a seguir son:
! 
! \begin{enumerate}
! 	\item Se consulta con PlanetLab Central y se escoge un nodo cualquiera
! 		del slice con el que nos queremos comunicar.
! 		\label{step:auth_get_node}
! 
! 	\item Se pide la clave p?blica del nodo seleccionado, $K_{pub_{c}}$, y
! 		se le da la p?blica del cliente, $K_{pub_{d}}$.\\
! 		Si el nodo no responde, se vuleve al paso
! 		\ref{step:auth_get_node}).
! 
! 	\item Se hace una petici?n de registro unicast al nodo, con la clave y
! 		password SSH encriptados con $K_{pub_{c}}$, juntamente con
! 		$K_{pub_{d}}$ para el env?o encriptado de datos sensibles
! 		(listado de ficheros, etc.).\\
! 		Si la operaci?n falla, se aborta el proceso de registro.
! 		Si el nodo no responde, se vuleve al paso
! 		\ref{step:auth_get_node}).
! 
! 	\item Se hace una petici?n de registro multicast al grupo, con la clave
! 		y password SSH encriptados con $K_{pub_{c}}$, juntamente con
! 		$K_{pub_{d}}$.
! \end{enumerate}
! % - TODO: consensuar
  
  

Index: 05.tex
===================================================================
RCS file: /cvsroot/plfs/doc/05.tex,v
retrieving revision 1.3
retrieving revision 1.4
diff -C2 -d -r1.3 -r1.4
*** 05.tex	7 Jun 2005 14:38:34 -0000	1.3
--- 05.tex	13 Jun 2005 01:32:21 -0000	1.4
***************
*** 7,10 ****
--- 7,36 ----
  
  
+ \section{Autentificaci?n}
+ 
+ Para la autentificaci?n con \dplc y verificaci?n de los datos firmados de ?ste,
+ hace falta previamente conocer su clave p?blica.
+ %TODO: que pasa al recibir datos firmados/encriptados con una clave que no
+ %conocemos?
+ 
+ Con tal de acceder un cliente (\dpld) a un nodo (\dplc), ?ste debe primero
+ registrarse envi?ndole la clave y password de SSH \textbf{encriptados} con la
+ clave p?blica de \dplc (para evitar que informaci?n sensible sea capturada).
+ 
+ Para ello, hay dos primitivas de autentificaci?n, una que hace una
+ autentificaci?n con respuesta (destinada a un solo nodo), y otra que hace la
+ autentificaci?n con todo el grupo multicast, pero que no da ning?n resultado.
+ 
+ El proceso completo es el que se muestra en la figura \ref{fig:act_-_auth}.
+ 
+ \begin{figure}[h]
+ 	\centering
+ 	\includegraphics[scale=1.0]{act_-_auth.eps}
+ 	\caption{Proceso de autentificaci?n}
+ 	\label{fig:act_-_auth}
+ \end{figure}
+ 
+ 
+ 
  \section{Listado de nodos}
  

Index: 03.tex
===================================================================
RCS file: /cvsroot/plfs/doc/03.tex,v
retrieving revision 1.5
retrieving revision 1.6
diff -C2 -d -r1.5 -r1.6
*** 03.tex	12 Jun 2005 22:08:13 -0000	1.5
--- 03.tex	13 Jun 2005 01:32:21 -0000	1.6
***************
*** 1,4 ****
--- 1,6 ----
  \chapter{Dise?o de los componentes}
  
+ %TODO: se?alar qu? m?dulos encriptan/desencriptan datos
+ 
  \textbf{\textit{En este cap?tulo definimos las principales funcionalidades que
  deben implementar cada uno de los componentes.}}

Index: 04.tex
===================================================================
RCS file: /cvsroot/plfs/doc/04.tex,v
retrieving revision 1.4
retrieving revision 1.5
diff -C2 -d -r1.4 -r1.5
*** 04.tex	12 Jun 2005 22:08:13 -0000	1.4
--- 04.tex	13 Jun 2005 01:32:21 -0000	1.5
***************
*** 1,4 ****
--- 1,6 ----
  \chapter{Comunicaci?n entre los componentes}
  
+ %TODO: se?alar qu? comunicaciones van encriptadas/desencriptadas
+ 
  \textbf{\textit{En este cap?tulo comentamos diferentes implementaciones de la
  comunicaci?n entre componentes del sistema. Como veremos, nos centraremos en
***************
*** 36,41 ****
  
  % TODO: hace falta esta explicaci?n? ya esta hecha antes...
! Cuando se realice una operaci?n (\textt{ls},\texttt{mv}, etc) en el sistema de
! ficheros \plfs, el m?dulo de kernel \kplfs que implementa estas operaciones,
  deber? informar dichos eventos al m?dulo a nivel usuario \uplfs (el cual deber?
  realizar las operaciones pertinentes).
--- 38,43 ----
  
  % TODO: hace falta esta explicaci?n? ya esta hecha antes...
! Cuando se realice una operaci?n (\texttt{ls},\texttt{mv}, etc) en el sistema
! de ficheros \plfs, el m?dulo de kernel \kplfs que implementa estas operaciones,
  deber? informar dichos eventos al m?dulo a nivel usuario \uplfs (el cual deber?
  realizar las operaciones pertinentes).



From nobody at sheep.berlios.de  Tue Jun 14 02:55:29 2005
From: nobody at sheep.berlios.de (xscript)
Date: Tue, 14 Jun 2005 02:55:29 +0200
Subject: [Plfs-svn] doc 02.tex,1.9,1.10 03.tex,1.7,1.8
Message-ID: <200506140055.j5E0tT109921@bat.berlios.de>

Update of /cvsroot/plfs/doc
In directory sheep:/tmp/cvs-serv23438

Modified Files:
	02.tex 03.tex 
Log Message:
Correcciones de estilo

Index: 02.tex
===================================================================
RCS file: /cvsroot/plfs/doc/02.tex,v
retrieving revision 1.9
retrieving revision 1.10
diff -C2 -d -r1.9 -r1.10
*** 02.tex	13 Jun 2005 19:57:46 -0000	1.9
--- 02.tex	14 Jun 2005 00:55:26 -0000	1.10
***************
*** 337,341 ****
  	\item Descubrimiento del nodo de entrada a la red de distribuci?n
  		multicast.
- 
  \end{itemize}
  
--- 337,340 ----
***************
*** 357,360 ****
--- 356,360 ----
  
  \section{L?gica de Despliegue}
+ \label{sect:deployment}
  
  % TODO: HAY QUE AVISAR DE CUALES SE HA PODIDO DESPLEGAR Y CUALES NO
***************
*** 613,617 ****
  	\item Cambio del par de claves del \dplc, de forma que no
  		se pueden verificar las operaciones enviadas por \dpld.
- 	
  \end{itemize}
  
--- 613,616 ----

Index: 03.tex
===================================================================
RCS file: /cvsroot/plfs/doc/03.tex,v
retrieving revision 1.7
retrieving revision 1.8
diff -C2 -d -r1.7 -r1.8
*** 03.tex	13 Jun 2005 19:57:46 -0000	1.7
--- 03.tex	14 Jun 2005 00:55:27 -0000	1.8
***************
*** 10,15 ****
  \section{\kplfs}
  Este m?dulo implementa la interfaz del VFS, y las operaciones que permite las
! hemos descrito en el cap?tulo anterior en el apartado de L?gica de
! Despliegue 2.5.
  
  % TODO(?): invalidate
--- 10,14 ----
  \section{\kplfs}
  Este m?dulo implementa la interfaz del VFS, y las operaciones que permite las
! hemos descrito en el cap?tulo anterior en el apartado \ref{sect:deployment}.
  
  % TODO(?): invalidate
***************
*** 108,116 ****
  	\item[\texttt{getSlices}] :\\
  		Permite obtener la lista de slices que se han dado de alta en
! PlanetLab
  
  	\item[\texttt{getNodes}] :\\
  		Permite obtener la lista de nodos que se han dado de alta en
! PlanetLab
  
  	\item[\texttt{getSlice (slice\_name)}] :\\
--- 107,115 ----
  	\item[\texttt{getSlices}] :\\
  		Permite obtener la lista de slices que se han dado de alta en
! 		PlanetLab
  
  	\item[\texttt{getNodes}] :\\
  		Permite obtener la lista de nodos que se han dado de alta en
! 		PlanetLab
  
  	\item[\texttt{getSlice (slice\_name)}] :\\
***************
*** 185,189 ****
  	\item[\texttt{group\_id getGroup (group\_name)}] :\\
  		Permite obtener el identificador, dentro de la red \umc, de un
! grupo.
  
  	\item[\texttt{setOptions (group\_id, options[])}] :\\
--- 184,188 ----
  	\item[\texttt{group\_id getGroup (group\_name)}] :\\
  		Permite obtener el identificador, dentro de la red \umc, de un
! 		grupo.
  
  	\item[\texttt{setOptions (group\_id, options[])}] :\\



From nobody at sheep.berlios.de  Tue Jun 14 21:03:19 2005
From: nobody at sheep.berlios.de (xscript)
Date: Tue, 14 Jun 2005 21:03:19 +0200
Subject: [Plfs-svn] doc 03.tex,1.8,1.9
Message-ID: <200506141903.j5EJ3J111665@bat.berlios.de>

Update of /cvsroot/plfs/doc
In directory sheep:/tmp/cvs-serv13652

Modified Files:
	03.tex 
Log Message:
Estrategia de sincronizacion

Index: 03.tex
===================================================================
RCS file: /cvsroot/plfs/doc/03.tex,v
retrieving revision 1.8
retrieving revision 1.9
diff -C2 -d -r1.8 -r1.9
*** 03.tex	14 Jun 2005 00:55:27 -0000	1.8
--- 03.tex	14 Jun 2005 19:03:17 -0000	1.9
***************
*** 81,85 ****
  
  	% TODO(?): validaciones? temporalidad?     ES ?TIL PLANTE?RSELO?
! 	
  	% TODO: quien lo utiliza?
  	\item[\texttt{getInfoFilesShared (slice\_name)}] :\\
--- 81,85 ----
  
  	% TODO(?): validaciones? temporalidad?     ES ?TIL PLANTE?RSELO?
! 
  	% TODO: quien lo utiliza?
  	\item[\texttt{getInfoFilesShared (slice\_name)}] :\\
***************
*** 329,333 ****
  %   - deben responder todos
  %   - alguien puede enga?arnos y dar un numero equivocado?
! %   -> dplc nos firma la version -> garantiza la validez (como el algodon, 
  %     dplc nuca enga?a)
  % o el numero que den la mayoria
--- 329,333 ----
  %   - deben responder todos
  %   - alguien puede enga?arnos y dar un numero equivocado?
! %   -> dplc nos firma la version -> garantiza la validez (como el algodon,
  %     dplc nuca enga?a)
  % o el numero que den la mayoria
***************
*** 347,350 ****
--- 347,421 ----
  % - protocolo de las transparencias
  % - parar y esperar al siguiente deployment
+ %
+ % ---------------------------------------------------------------
+ % => Posible estrategia de sincronizacion por numero de version:
+ % ---------------------------------------------------------------
+ %
+ % => Precondicion: queremos que todos los nodos tengan el mismo shared
+ %     siempre!
+ %
+ % => Formato de version:
+ % - numeracion consecutiva
+ % - numeracion consecutiva con separadores (.), siendo el override del usuario
+ %   el que permite avanzar a una version superior no consecutiva 1.9 -> 2.0
+ % - fecha en formato universal (estan los nodos de PL en hora?); como los
+ %   deploys no son muy frecuentes, es bastante fiable aun con desfases de
+ %   reloj
+ %
+ % => Como obtener nodos cualesquiera
+ % hacer una peticion de la lista a PLC y seleccionar uno aleatoriamente
+ %
+ % => Pasos
+ % - cada nodo en su slice guarda un fichero con el numero de version que se le
+ %   ha desplegado (si no existe, se supono que no hay despliegue previo)
+ %   (posibilidad de ver un fichero en unshared?)
+ %   (posibilidad de ver un fichero en shared con el valor maximo actual?)
+ %
+ % - dpld obtiene el numero de version que actualmente hay en la red
+ %   * cuando:
+ %     - al hacer el deploy, pregunta
+ %     - en alguna operacion anterior ya lo va haciendo en background por si a
+ %       caso
+ %     - al hacer el registro unicast, el resultado es la version actual
+ %   * como:
+ %     - repositorio central (a modo transaccional, solo se actualiza el valor
+ %       si se hace el deploy de como minimo N nodos - 1? quizas mas si
+ %       utilizamos tecnicas de fallo bizantino? -)
+ %     - se guarda el valor en la maquina dpld (demasiado rigido, no podemos
+ %       hacer deploy desde diferentes sitios)
+ %     - se le hace poner un valor en un fichero al usuario (esta bien como
+ %       posibilidad para forzar el valor, pero no como obligacion)
+ %     - el valor que nos den de una sola consulta (podria estar desfasado)
+ %     - el valor maximo de N preguntas
+ %       * si el valor se guarda en el slice, podria estar falseado? a quien le
+ %         interesa si es su propio slice?
+ %       * si el valor se guarda en dplc, seguro que siempre es el ultimo que
+ %         dplc ha visto, no?
+ %     - el valor mas ``votado'', por si hubiera alguno falseado (lo habria?)
+ %
+ % - hace el deploy indicando el nuevo numero de version
+ %
+ % - existe la posibilidad de que los dplc hagan piggybacking indicando su
+ %   version actual, para que dpld:
+ %   * se ponga al dia (hay algun problema en coger la mayor aunque solo la
+ %     tenga uno? -> pero que no sea de un corrupto!)
+ %   * dpld le indique que necesita actualizacion y
+ %     - le haga un deploy unicast
+ %     - le indique un vecino cercano con la version ``buena''
+ %
+ % - al arrancar o recuperar la conexion un dplc, obtiene el numero ``bueno''
+ %   de version tal como lo hace dpld
+ %
+ % - si esta desactualizado, obtiene la nueva version de un peer no corrupto
+ %
+ % - cada dplc guarda los ultimos N nodos (incluyendo dpld) que le han
+ %   preguntado su version, si se le cambia la version, lo notifica a estos
+ %
+ % - cuando llega una notificacion de cambio de version, se hace un redeploy
+ %   desde ese peer originador del mensaje
+ %   [ deploy virico para los que se han quedado rezagados ]
+ %
+ % - existencia de un fichero en /plfs/slices/<slice> que al hacerle un cat
+ %   intenta contactar todos los nodos y mostrar su estado
  
  



From nobody at sheep.berlios.de  Wed Jun 15 20:55:39 2005
From: nobody at sheep.berlios.de (xscript)
Date: Wed, 15 Jun 2005 20:55:39 +0200
Subject: [Plfs-svn] doc 03.tex,1.9,1.10
Message-ID: <200506151855.j5FItdI22989@bat.berlios.de>

Update of /cvsroot/plfs/doc
In directory sheep:/tmp/cvs-serv8474

Modified Files:
	03.tex 
Log Message:
Optimizacion del redeploy

Index: 03.tex
===================================================================
RCS file: /cvsroot/plfs/doc/03.tex,v
retrieving revision 1.9
retrieving revision 1.10
diff -C2 -d -r1.9 -r1.10
*** 03.tex	14 Jun 2005 19:03:17 -0000	1.9
--- 03.tex	15 Jun 2005 18:55:35 -0000	1.10
***************
*** 373,376 ****
--- 373,377 ----
  %
  % - dpld obtiene el numero de version que actualmente hay en la red
+ %   (no necesario si es version por fecha)
  %   * cuando:
  %     - al hacer el deploy, pregunta
***************
*** 418,421 ****
--- 419,438 ----
  % - existencia de un fichero en /plfs/slices/<slice> que al hacerle un cat
  %   intenta contactar todos los nodos y mostrar su estado
+ %
+ % => interesante la deteccion de que nodos estan en la misma subred (fichero
+ %    de conf de dplc con las redes locales?)
+ %    al entrar en funcionamiento, lo pueden hacer en grupo (corte de luz o
+ %    reconexion a inet - gracias al RFC TRACK multicast, se da aviso de
+ %    eliminacion del grupo multicast, verdad? -)
+ %
+ % => las caidas son aleatorias, por lo que afectan a nodo separados (bloqueos
+ %    de la maquina) o a una institucion o parte de ella (red local)
+ %    => no hace falta un redeploy multicast (no habra tantos a la vez como
+ %       para hacerlo preferible y viable)
+ %    => designar un cabeza de grupo que lo pide a un peer exterior y luego
+ %       hace un deploy unicast a
+ %       - la direccion multicast del grupo institucional
+ %       - cada peer local (si no se puede por multicast... se dara el
+ %         caso?)
  
  



From nobody at sheep.berlios.de  Fri Jun 17 20:57:24 2005
From: nobody at sheep.berlios.de (xscript)
Date: Fri, 17 Jun 2005 20:57:24 +0200
Subject: [Plfs-svn] doc 00-plfs.bib,1.4,1.5 00-plfs.tex,1.3,1.4 02.tex,1.10,1.11 03.tex,1.10,1.11 04.tex,1.6,1.7 ToDo.txt,1.2,1.3
Message-ID: <200506171857.j5HIvOI11873@bat.berlios.de>

Update of /cvsroot/plfs/doc
In directory sheep:/tmp/cvs-serv968

Modified Files:
	00-plfs.bib 00-plfs.tex 02.tex 03.tex 04.tex ToDo.txt 
Log Message:
Finalizacion de la documentacion

Index: 00-plfs.bib
===================================================================
RCS file: /cvsroot/plfs/doc/00-plfs.bib,v
retrieving revision 1.4
retrieving revision 1.5
diff -C2 -d -r1.4 -r1.5
*** 00-plfs.bib	12 Jun 2005 21:52:27 -0000	1.4
--- 00-plfs.bib	17 Jun 2005 18:57:21 -0000	1.5
***************
*** 15,32 ****
  
  @manual{Araneola,
! 	title = {Araneola},
! 	note = {\url{http://}}
! }
! 
! @misc{rfc-track,
! 	title = {Tree-Based ACK (TRACK) Building Block for Reliable Multicast Transport},
! 	author = {Brian Whetten et al.},
! 	howpublished = {RMT Working Group, IETF},
! 	month = {December},
! 	year = {2003}
  }
  
- #--- no utilizados ---
- 
  @manual{ESM,
  	title = {End System Multicast (ESM)},
--- 15,25 ----
  
  @manual{Araneola,
! 	title = {Araneola: A Scalable Reliable Multicast System for Dynamic
! 		Environments},
! 	note =
! {\url{http://dsl.cs.technion.ac.il/completed_projects/Araneola/html/araneola.htm
! }}
  }
  
  @manual{ESM,
  	title = {End System Multicast (ESM)},
***************
*** 34,37 ****
--- 27,38 ----
  }
  
+ @manual{LMDD,
+ 	title = {Logistical Multicast for Data Distribution},
+ 	note =
+ {\\\url{
+ http://loci.cs.utk.edu/modules.php?name=Publications&d_op=ViewPublication&lid=24
+ 3}}
+ }
+ 
  @manual{LSAM,
  	title = {Large-Scale Active Multicast},
***************
*** 39,45 ****
  }
  
! @manual{LMDD,
! 	title = {Logistical Multicast for Data Distribution},
! 	note = {\url{http://loci.cs.utk.edu/modules.php?name=Publications&d_op=ViewPublication&lid=243}}
  }
  
--- 40,49 ----
  }
  
! @misc{rfc-track,
! 	title = {Tree-Based ACK (TRACK) Building Block for Reliable Multicast Transport},
! 	author = {Brian Whetten et al.},
! 	howpublished = {RMT Working Group, IETF},
! 	month = {December},
! 	year = {2003}
  }
  
***************
*** 80,83 ****
--- 84,97 ----
  }
  
+ @manual{FAM,
+ 	title = {File Alteration Monitor},
+ 	note = {\url{http://oss.sgi.com/projects/fam/}}
+ }
+ 
+ @manual{Gamin,
+ 	title = {Gamin the File Alteration Monitor},
+ 	note = {\url{http://www.gnome.org/~veillard/gamin/}}
+ }
+ 
  @manual{PLC,
  	title = {PlanetLab Central API Documentation},
***************
*** 111,113 ****
  	note = {\url{http://www.cs.arizona.edu/stork/}}
  }
- 
--- 125,126 ----

Index: 00-plfs.tex
===================================================================
RCS file: /cvsroot/plfs/doc/00-plfs.tex,v
retrieving revision 1.3
retrieving revision 1.4
diff -C2 -d -r1.3 -r1.4
*** 00-plfs.tex	12 Jun 2005 21:46:11 -0000	1.3
--- 00-plfs.tex	17 Jun 2005 18:57:21 -0000	1.4
***************
*** 79,83 ****
  \input{03}
  \input{04}
! \input{05}
  
  \pagebreak
--- 79,83 ----
  \input{03}
  \input{04}
! %\input{05}
  
  \pagebreak

Index: 02.tex
===================================================================
RCS file: /cvsroot/plfs/doc/02.tex,v
retrieving revision 1.10
retrieving revision 1.11
diff -C2 -d -r1.10 -r1.11
*** 02.tex	14 Jun 2005 00:55:26 -0000	1.10
--- 02.tex	17 Jun 2005 18:57:21 -0000	1.11
***************
*** 8,11 ****
--- 8,12 ----
  
  \section{Componentes}
+ \label{sect:components}
  
  En este apartado mostraremos un esquema inicial de los principales
***************
*** 66,69 ****
--- 67,71 ----
  
  \section{Esquema de \plfs (PlanetLab File System)}
+ \label{sect:plfsschema}
  
  Antes de comentar el esquema del ?rbol de ficheros y directorios de \plfs, es
***************
*** 90,93 ****
--- 92,97 ----
  ficheros como se puede apreciar en la figura:
  
+ \newpage
+ 
  \begin{code}
  	/plfs
***************
*** 99,105 ****
--- 103,111 ----
  	|   |-key
  	|   |-passwd
+ 	|   |-status
  	|   |-script
  	|   |-nodes
  	|   | `-<node>
+ 	|   |   |-version
  	|   |   `-unshared
  	|   `-shared
***************
*** 137,142 ****
  		del slice.
  
  	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/script}] :\\
! 		?ste fichero es un shell script que se ejecutar? en cada m?quina
  		una vez hecho el despliegue.
  		\\
--- 143,152 ----
  		del slice.
  
+ 	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/status}] :\\
+ 		Este fichero permite conocer el estado de despliegue de los
+ 		nodos del slice.
+ 
  	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/script}] :\\
! 		Este fichero es un shell script que se ejecutar? en cada m?quina
  		una vez hecho el despliegue.
  		\\
***************
*** 154,160 ****
--- 164,180 ----
  			\item [deploy:] lleva a cabo las operaciones necesarias
  				para preparar el arranque del servicio
+ 				(instalaci?n de la aplicaci?n y logueo de los
+ 				ficheros generados durante ?sta)
+ 
+ 			\item [clean:] elimina los ficheros que ha desplegado
+ 				el \texttt{script$\rightarrow$deploy}
  		\end{description}
  
  	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/nodes/\textless{}
+ 		node\textgreater/version}] :\\
+ 		Este fichero contiene el n?mero de version del ?ltimo
+ 		despliegue (ver apartado \ref{sect:versioning}).
+ 		
+ 	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/nodes/\textless{}
  		node\textgreater/unshared}] :\\
  		?ste directorio es propio de cada nodo en el contexto del slice
***************
*** 180,183 ****
--- 200,204 ----
  
  \section{Operaciones de \plfs}
+ \label{sect:plfsoperations}
  
  Las posibles operaciones que \plfs permite, y por ende, que \kplfs implemente
***************
*** 302,305 ****
--- 323,327 ----
  
  \section{Descubrimiento de entidades del sistema}
+ \label{sect:discovery}
  
  El sistema, adem?s de las funcionalidades propias de despliegue de
***************
*** 358,372 ****
  \label{sect:deployment}
  
- % TODO: HAY QUE AVISAR DE CUALES SE HA PODIDO DESPLEGAR Y CUALES NO
- %
- % que hacer con los que se han quedado a medias/han fallado?
- %
- % estaria bien un fichero de estado? Lista de nodos registrados en PLC en el
- % slice, con:
- % - ultima operacion realizada
- % - ultima operacion fallida
- % - ultima operacion en proceso
- % - nodo no accesible (no responde)
- 
  Despu?s de pensar diferentes posibilidades sobre como el usuario podr?a llevar
  a cabo realmente el despliegue mediante el sistema de ficheros, hemos llegado
--- 380,383 ----
***************
*** 378,390 ****
  		despliegue que a continuaci?n comentamos.
  
! 	\item A continuaci?n el usuario realiza \texttt{mv/cp} del script de
! 		arranque de despliegue, que tambi?n es desplegado mediante el
  		mismo proceso hacia los nodos destino.
  \end{enumerate}
  
  Adem?s, y como es obvio dada la naturaleza del propio sistema de ficheros, se
! deber? cumplir la siguiente precondici?n antes de poder realizar el despliegue:
  
! \textit{Precondici?n}: El slice destino est? ya creado (ej: \texttt{mkdir}).
  
  Una vez aclarado lo anterior, pasamos a describir como se realizar?a de forma
--- 389,402 ----
  		despliegue que a continuaci?n comentamos.
  
! 	\item A continuaci?n el usuario realiza \texttt{mv/cp/touch} del script
! 		de arranque de despliegue, que tambi?n es desplegado mediante el
  		mismo proceso hacia los nodos destino.
  \end{enumerate}
  
  Adem?s, y como es obvio dada la naturaleza del propio sistema de ficheros, se
! deber? cumplir la siguiente precondici?n antes de poder realizar el
! despliegue:\\
  
! \textit{Precondici?n}: El slice destino est? ya creado (ej: \texttt{mkdir}).\\
  
  Una vez aclarado lo anterior, pasamos a describir como se realizar?a de forma
***************
*** 406,410 ****
  	\item \dpld $\rightarrow$ \umcc \\
  		Se comunica el slice destino, se dan los ficheros a desplegar y
! 		el shell script con los comandos a ejecutar una vez desplegados.
  
  	\item \umcc $\rightarrow$ red \umcr \\
--- 418,423 ----
  	\item \dpld $\rightarrow$ \umcc \\
  		Se comunica el slice destino, se dan los ficheros a desplegar y
! 		el shell script con los comandos a ejecutar una vez
! 		desplegados, indicando la versi?n que se est? desplegando.
  
  	\item \umcc $\rightarrow$ red \umcr \\
***************
*** 433,451 ****
  	\item \dplc $\rightarrow$ slice destino (ssh) \\
  		Se despliega la aplicaci?n (se copian los ficheros), y una vez
! 		hecho esto, se ejecuta el shell script adjuntado, en caso de
! 		estar presente. Para ello, primero ejecuta el \textbf{deploy} y
! 		luego el \textbf{start}.
! 		% TODO: diferenciar primeros de ultimo fichero
! 		% ahora el ultimo debe ser el script y se hara el deploy+start
  \end{enumerate}
  
! % TODO: NOTA:
! % En caso de haber un fallo en cualquiera de los dos pasos anteriores,
! % se devuelve un error al dpld origen (a modo de NACK) ??????
! % Si todo es correcto anunciar al dpld??? Escalabilidad? ACK's
  
  
  
  \section{Comunicaci?n Multicast}
  
  Como hemos comentado, el despliegue de los ficheros se hace sobre una red
--- 446,474 ----
  	\item \dplc $\rightarrow$ slice destino (ssh) \\
  		Se despliega la aplicaci?n (se copian los ficheros), y una vez
! 		hecho esto, se ejecuta el shell script (que es el ?ltimo
! 		fichero en llegar). Para ello, primero se desinstala la
! 		anterior aplicaci?n (\texttt{script$\rightarrow$clean}), se
! 		copian los nuevos ficheros, y luego se despliega la aplicaci?n
! 		(\texttt{script$\rightarrow$deploy}) y se pone en marcha
! 		(\texttt{script$\rightarrow$start}).
  \end{enumerate}
  
! Como se ve en el apartado \ref{sect:redeployment}, no es necesario controlar
! los casos en que algun despliegue no se lleve a cabo en alg?n nodo, ya que
! entre ellos mismos se encargar?n de mantener la coherencia del despliegue.
! 
! El fichero \texttt{/plfs/slices/\textless{}slice\textgreater/status} contiene,
! para cada nodo del slice, si se ha podido contactar con ?l y, en caso
! afirmativo, en qu? estado de despligue se encuentra el nodo.
! 
! Esta informaci?n se deriva a trav?s del contenido del acceso a los ficheros
! \texttt{/plfs/slices/\textless{}slice\textgreater/nodes/\textless{}
! node\textgreater/version}, dejando comprobar en qu? estado se encuentra el
! despliegue al comparar la versi?n de los nodos con la actual.
  
  
  
  \section{Comunicaci?n Multicast}
+ \label{sect:multicast}
  
  Como hemos comentado, el despliegue de los ficheros se hace sobre una red
***************
*** 500,507 ****
  \end{description}
  
! Creemos que TRACK es m?s completo y nos permitir?a realizar m?s operaciones a
! nivel de despliegue. Por ejemplo, en caso de recibir un TRACK donde se informa
! de que han fallado algunos paquetes, el cliente de multicast \umcc informar?a a
! \dpld el cual se encargar?a de reenviar los datos.
  
  
--- 523,555 ----
  \end{description}
  
! Creemos que TRACK es m?s completo y permitir?a realizar extensiones al sistema,
! como por ejemplo a modo de monitorizaci?n o posibilidad de extender el modelo
! de despliegue a, por ejemplo, exigir un m?nimo de N nodos.
! 
! 
! 
! \section{Versionamiento de despliegues}
! \label{sect:versioning}
! 
! Como los nodos de los slices pueden recibir varios despliegues en el tiempo y de
! manera secuencial, y queremos tener los nodos de un mismo slice sincronizados
! al ?ltimo despliegue realizado, necesitamos una manera de controlar las
! versiones que nos permita detectar las situaciones que comentamos en el
! apartado \ref{sect:redeployment}.
! 
! Como presuponemos que los despliegues son relativamente poco frecuentes, y no
! soportamos la posibilidad de realizar despliegues simult?neos (en parte por que
! no tienen sentido por la naturaleza del problema), consideramos que la fecha y
! la hora en un formato com?n entre todos los nodos (ej:UTC o GMT), son
! identificadores suficientes de la versi?n de un despliegue, presuponiendo que
! los nodos de PlanetLab est?n sincronizados a esta hora con unos l?mites de
! error aceptables.
! 
! Este n?mero de versi?n es accesible a trav?s del fichero
! \texttt{/plfs/slices/\textless{}slice\textgreater/nodes/\textless{}
! node\textgreater/version}.
! 
! Cabe destacar un tipo de versi?n especial que llamaremos ``versi?n corrupta''
! que ser? considerada como la m?s vieja de todas las versiones.
  
  
***************
*** 510,551 ****
  \label{sect:redeployment}
  
! El componente \famon se encarga de la monitorizaci?n de los ficheros etiquetados
! como \texttt{shared}. De este modo, cuando se produce una modificaci?n
! incontrolada de estos ficheros, este componente avisa al \dplc para que a su
! vez lo notifique a los nodos \dpld responsables de ese fichero, es decir,
! aquellos que est?n suscritos como oyentes de ese grupo multicast, o slice, del
! cual son los responsables de los despliegues que se realicen en ?l, y por
! lo tanto deben encargarse de invalidar los ficheros afectados del sistema de
! ficheros \plfs.
  
! Entenderemos por modificaci?n incontrolada, aquella que no provenga de uno de
! estos nodos responsables (nodos con \dpld).
  
! Cuando \famon avisa al \dplc de ese nodo, ?ste proceder? a redesplegar el
! fichero siguiendo los pasos a continuaci?n:
  
  \begin{enumerate}
! 	\item \dplc realiza un script->stop deteniendo moment?neamente la
! 		aplicaci?n en ese nodo corrupto.
  
! 	\item se despliega el fichero desde un nodo del mismo slice (un nodo
! 		hermano hablando desde el punto de vista de ?rboles de
! 		directorios) que no sea corrupto.
  
! 	\item \dplc realiza un script->start para reiniciar la aplicaci?n.
  \end{enumerate}
  
  Obviamente surge un posible problema como consecuencia de este dise?o, y es que
! podr?a suceder que por circunstancias no existiera ning?n otro nodo no corrupto
  (o bien porque todos los hermanos lo est?n, o bien porque es el ?nico nodo
  activo del slice).
  
  De modo que la soluci?n que proponemos es la de mantener en un repositorio
! central los ficheros originales desplegados como \texttt{shared}.
  La URI de este repositorio es la que se indica en el fichero \texttt{backup}
  que hemos presentado en el esquema de \plfs.
  
! Finalmente, si la recuperaci?n mediante este repositorio no pudiera llevarse a
! cabo, se invalidar?a el nodo del slice, y quedar?a como corrupto.
  
  
--- 558,656 ----
  \label{sect:redeployment}
  
! Existen dos casos en los que es necesario un redespliegue:
  
! \begin{description}
! 	\item[Ca?da de nodos] : \\
! 		Los nodos en PlanetLab "caen" de forma aleatoria, o bien un nodo
! 		concreto (cuelgue de la m?quina, reinicio, p?rdida
! 		de conectividad, etc.) o por grupos institucionales
! 		(fallo el?ctrico de una zona, ca?da del acceso a la red, etc.).
! 		Estas situaciones se detectan en caso de rearrancar \dplc (por
! 		una ca?da de la m?quina) o bien cuando el nodo queda reconectado
! 		a la red multicast (despu?s de haber sido expulsado por
! 		falta de respuesta). \\
! 		Ser? necesario realizar un redespliegue, si al volver al
! 		grupo, el nodo detecta que hay una versi?n m?s nueva de
! 		despliegue.
! 	
! 	\item[Modificaci?n de ficheros] : \\
! 		Cuando se produce una modificaci?n incontrolada (que no viene 
! 		de uno de los \dpld registrados) de los ficheros de un nodo al
! 		que se le ha realizado un despliegue, y estos son de tipo
! 		\textit{shared}, \famon se encarga de detectarlo y avisar a
! 		\dplc, que se encargar? de avisar a los \dpld
! 		registrados para que hagan una invalidaci?n de
! 		dichos ficheros.
! 		\\
! 		En este caso es necesario adem?s marcar la versi?n como
! 		``versi?n corrupta'' (nunca ser? rearrancada una
! 		aplicaci?n marcada de esta manera).
! \end{description}
  
! Cuando se detectan uno de estos dos casos, antes de nada, se detiene la
! aplicaci?n mediante el script de despliegue (script$\rightarrow$stop) y
! posteriormente se realiza el proceso de redespliegue.
! 
! Para realizar el redespliegue es necesario que los \dplc mantengan una lista de
! \textit{peers} para cada slice al que pertenece. Cada lista contiene N
! nodos con el ?ltimo n?mero de versi?n conocido para cada uno de ellos (a modo de
! \textit{vector clock}).
! 
! Para obtener esta lista, cuando \dplc arranca obtiene las direcciones de N nodos
! cualesquiera mediante \pldb y a continuaci?n les pregunta a cada uno de ellos
! su n?mero de versi?n dando a cambio el suyo propio.
! 
! En caso de que un nodo no responda se sustituye por otro que no
! est? en la lista de forma aleatoria. An?logamente, en el caso de que est? en la
! lista pero posteriormente se pierda la conexi?n con ?l (ej: detecci?n por pings
! peri?dicamente con per?odos largos para no saturar la red), tambi?n ser?
! sustituido.
! 
! En el caso de que sea el propio nodo el que haya perdido la conexi?n, \dplc
! quedar? suspendido hasta reactivar la conexi?n, momento en el que intentar?
! revalidar las entradas de su tabla de \textit{peers}.
! 
! El proceso de redespliegue (cuando sea necesario tal como hemos comentado
! antes) se realizar? mediante los siguientes pasos:
  
  \begin{enumerate}
! 	\item En caso de no tener los datos de ssh, \dplc se los pedir? a alg?n
! 		nodo de la lista para poder abrir las conexiones.
  
! 	\item Una vez abiertas las conexiones, se pide un despliegue unicast a
! 		uno de los \textit{peers} de la lista.
  
! 	\item Se procede al despliegue de los ficheros actualizando as? la
! 		versi?n.
  \end{enumerate}
  
  Obviamente surge un posible problema como consecuencia de este dise?o, y es que
! podr?a suceder que por circunstancias no existiera ning?n nodo con una versi?n
  (o bien porque todos los hermanos lo est?n, o bien porque es el ?nico nodo
  activo del slice).
  
  De modo que la soluci?n que proponemos es la de mantener en un repositorio
! central los ficheros originales desplegados como \texttt{shared}, de modo que
! si \dplc no puede contactar con ning?n nodo no corrupto pueda descargar estos
! ficheros.
! 
  La URI de este repositorio es la que se indica en el fichero \texttt{backup}
  que hemos presentado en el esquema de \plfs.
  
! Para evitar un colapso de la red los cambios de versi?n no se informan de
! immediato al producirse dicho cambio (excepto del cambio de versi?n a ``versi?n
! corrupta'' que produce un aviso instant?neo), sino que se utilza un mecanismo
! de coherencia relajada a trav?s de los mensajes que se env?an los
! \textit{peers} para actualizar sus listas de versiones, aprovechando la
! entrop?a que propicia el hecho de que cada uno env?a dichos mensajes en tiempos
! muy probablemente diferentes (porque han empezado en un instante distinto).
! 
! Como mejora, ser?a interesante detectar qu? nodos est?n en una misma subred, es
! decir, que forman parte de una misma instituci?n y comparten una red mucho m?s
! r?pida que Internet (ya sea por configuraci?n manual o por detecci?n a trav?s
! de las interf?cies configuradas), para as? delegar un posible proceso de
! \textit{redeployment} de todo el grupo a un solo nodo, que luego haria el mismo
! \textit{redeployment} al resto de nodos del grupo (hasta pudiendo as?
! aprovecharse de las capacidades de multicast del nivel de enlace).
  
  
***************
*** 577,586 ****
  
  \begin{enumerate}
! 	\item Se consulta con PlanetLab Central y se escoge un nodo cualquiera
  		del slice con el que nos queremos comunicar.
  		\label{step:auth_get_node}
  
  	\item Se pide la clave p?blica del nodo seleccionado, $K_{pub_{c}}$, y
! 		se le da la p?blica del cliente, $K_{pub_{d}}$.\\
  		Si el nodo no responde, se vuleve al paso
  		\ref{step:auth_get_node}).
--- 682,691 ----
  
  \begin{enumerate}
! 	\item Se consulta con \pldb y se escoge un nodo cualquiera 
  		del slice con el que nos queremos comunicar.
  		\label{step:auth_get_node}
  
  	\item Se pide la clave p?blica del nodo seleccionado, $K_{pub_{c}}$, y
! 		se le da la p?blica del \dpld, $K_{pub_{d}}$.\\
  		Si el nodo no responde, se vuleve al paso
  		\ref{step:auth_get_node}).
***************
*** 599,603 ****
  \end{enumerate}
  
- %TODO: como avisa un nodo a un cliente que este no esta autentificado?
  Hay varios casos en los que una operaci?n realizada por un \dpld no podr?a ser
  llevada a cabo en un \dplc:
--- 704,707 ----
***************
*** 616,633 ****
  
  La soluci?n que hemos pensado para estos tres casos anteriores, es la de
! realizar nuevos registros. En el primer caso, \dplc avisar?a mediante un
! \textit{warning} (ver apartado \ref{sect:warnings}), y \dpld realizar?a un
  registro unicast en ese nodo. En los otros casos \dpld realizar?a de
  nuevo el registro multicast cada cierto tiempo (pol?tica de revalidaci?n).
  Cabe notar que las conexiones que \dpld mantiene con los slices tienen un
! \textit{timeout} asociado, de modo que una vez expirado se perder?n los datos
! de dichas conexiones y si posteriormente se quisiera realizar una operaci?n en
! alguno de esos slices se deber? reiniciar de cero el proceso de registro.
  
  En el caso en que se hayan cambiado la clave y/o el password ssh y que la
! conexi?n ssh est? cerrada, el \dpld recibir?a algunos \textit{warnings}
! procedentes de los \dplc destinatarios con la conexi?n ssh cerrada (que ser?n
! una minor?a debido a la pol?tica de revalidaci?n de registros comentada
! anteriormente).
  
  Como los datos desplegados por \dpld van firmados para evitar problemas de
--- 720,744 ----
  
  La soluci?n que hemos pensado para estos tres casos anteriores, es la de
! realizar nuevos registros. En el primer caso, \dplc avisar?a mediante una
! operaci?n (ver apartado \ref{sect:dplc}), y \dpld realizar?a un
  registro unicast en ese nodo. En los otros casos \dpld realizar?a de
  nuevo el registro multicast cada cierto tiempo (pol?tica de revalidaci?n).
+ 
  Cabe notar que las conexiones que \dpld mantiene con los slices tienen un
! \textit{timeout} asociado, de modo que una vez expirado se perder?n dichas
! conexiones y si posteriormente se quisiera realizar una operaci?n en
! alguno de esos slices se deber? reiniciar de cero el proceso de registro
! (momento en el que se intentar? revalidar los fiheros que tubiera cacheados el
! \dpld).
! 
! Ahora bien, \dplc no pierde los datos de conexi?n (clave y password ssh),
! puesto que como se explica en el apartado \ref{sect:redeployment}, ?stos son
! necesarios para mantener la coherencia del slice.
  
  En el caso en que se hayan cambiado la clave y/o el password ssh y que la
! conexi?n ssh est? cerrada, el \dpld recibir?a algunas operaciones de aviso (ver
! apartado \ref{sect:dpld}) procedentes de los \dplc destinatarios con la conexi?n
! ssh cerrada (que ser?n una minor?a debido a la pol?tica de revalidaci?n de
! registros comentada anteriormente).
  
  Como los datos desplegados por \dpld van firmados para evitar problemas de
***************
*** 635,639 ****
  deber? propagar la clave p?blica mediante un registro multicast a los slices
  con los que tuviera una relaci?n abierta.
- 
- % TODO(?): 'ls' => renew => cheksums??
- % => invalidar els fitxers cada X temps
--- 746,747 ----

Index: 03.tex
===================================================================
RCS file: /cvsroot/plfs/doc/03.tex,v
retrieving revision 1.10
retrieving revision 1.11
diff -C2 -d -r1.10 -r1.11
*** 03.tex	15 Jun 2005 18:55:35 -0000	1.10
--- 03.tex	17 Jun 2005 18:57:21 -0000	1.11
***************
*** 9,30 ****
  
  \section{\kplfs}
  Este m?dulo implementa la interfaz del VFS, y las operaciones que permite las
! hemos descrito en el cap?tulo anterior en el apartado \ref{sect:deployment}.
  
! % TODO(?): invalidate
  
  
  
  \section{\uplfs}
  
! Las operaciones que \uplfs ofrece, adem?s de las especificadas en el m?dulo
! \pldb para las que hace de intermediario, son las siguientes:
  
- %TODO: es solo esto?
  \begin{description}
! 	\item[\texttt{execute(component, operation, \ldots)}] :\\
  		Llama a la funci?n \texttt{operation} del componente
  		\texttt{component} con los par?metros extra que se indiquen (en caso
  		de indicar alguno).
  \end{description}
  
--- 9,49 ----
  
  \section{\kplfs}
+ 
  Este m?dulo implementa la interfaz del VFS, y las operaciones que permite las
! hemos descrito en el cap?tulo anterior en el apartado \ref{sect:plfsoperations}.
  
! Las operaciones que ofrece, adem?s de las del VFS, son:
! 
! \begin{description}
! 	\item[\texttt{invalidate (slice\_name, type, path)}] :\\
! 		Invalida el fichero determinado por \texttt{path} del slice
! 		\texttt{slice\_name} del tipo \texttt{type} (\textit{shared} o
! 		\textit{unshared}).
! \end{description}
  
  
  
  \section{\uplfs}
+ \label{sect:uplfs}
  
! Cuando el usuario quiere mirar el contenido del directorio de slices o de
! nodos de un slice, y \kplfs remite dicha operaci?n a \uplfs, este realiza la
! consulta de dicha informaci?n llamando a las operaciones que ofrece \pldb, por
! ejemplo mediante llamadas a procedimiento remoto (como podr?a ser Java RMI,
! soportado por PlanetLab Central).
! 
! Una vez obtenida la informaci?n, \uplfs proceder?a a pas?rsela a \kplfs el cual
! actualizar?a los inodos de \plfs y mostrar?a el resultado al usuario.
! 
! Por ello, las operaciones que \uplfs ofrece, adem?s de las del VFS para las que
! hace de intermediario hacia \kplfs, son las siguientes:
  
  \begin{description}
! 	\item[\texttt{execute (component, operation, \ldots)}] :\\
  		Llama a la funci?n \texttt{operation} del componente
  		\texttt{component} con los par?metros extra que se indiquen (en caso
  		de indicar alguno).
+ 		\\
+ 		Los posibles componentes actualmente son \dpld o \pldb.
  \end{description}
  
***************
*** 35,98 ****
  Las operaciones que \dpld ofrece son las siguientes:
  
- % TODO: faltan las credenciales de autentificaci?n, o mejor un paso previo de
- % autentificaci?n?!?!?!?!
- % TODO(~): faltan los parametros de que desplegar, etc
- % TODO(~): explicar fichero de comandos etc otra vez? 		NOO!!
- % TODO(~): poner los resultados de las ops
  \begin{description}
! 	\item[\texttt{void deployAppToSlice (slice\_name, files[], script, key,
! 		passwd,...)}] :\\
! 		Realiza la comunicaci?n con el componente \umcc para desplegar
! 		una aplicaci?n (files) a trav?s de la red \umcr en el grupo
! 		indicado por el slice\_name. Para ello tambi?n requerir? el
! 		script de inicializaci?n, y la clave y password de ssh del
  		slice.
  		\\
! 		% TODO: pero ahora hay TRACK!!!
! 		% primer intento + reintentos a solo a quien falla? (pero por multicast si cierto numero)
! 		NOTA: La operaci?n no puede devolver resultado de ?xito,
! 		puesto que la implementaci?n de ACKS en Multicast tiene
! 		problemas de escalabilidad, y por lo que hace a los NACKS
! 		no creemos que sea muy bueno mantener el \dpld esperando a ver
! 		si recibe alguno.
  
! 	\item[\texttt{ void deployAppToNode (slice\_name,
! 		node\_name,files,script, key, passwd) }] : \\
! 		Lo mismo que deployAppToSlice pero hacia un nodo concreto.
! 		?til, si se a?ade un nodo a un slice al que se ha realizado ya
! 		el despliegue.
  
! 	\item[\texttt{void addNodeToSlice (slice\_name, node\_name)}] :\\
! 		Crea el directorio que representa al nodo en el sistema de
! 		ficheros y posteriormente ejecuta \texttt{deployAppToNode}.
  
! 	\item[\texttt{registerToDplc (node\_name, slice\_name)}] :\\
! 		Se registra el nodo de \dpld como oyente de los cambios que se
! 		producen en un slice de un nodo concreto.
! 		% TODO(?): se puede utilizar un grupo de oyentes a cambios en
! 		% grupos de dplc, as? no hace falta el registro...
  
! 	\item[\texttt{invalidateObj (slice\_name, name)}] :\\
! 		% TODO(?): utilizar identificador de obj diferente del path?
! 		% hace falta nombre del slice? (path completo)
! 		Invalida un objeto del sistema de ficheros (un fichero o un
! 		directorio).
! 		%NOTA: de que puede servir invalidar un objeto del FS?
  
! 	% TODO(?): validaciones? temporalidad?     ES ?TIL PLANTE?RSELO?
  
! 	% TODO: quien lo utiliza?
! 	\item[\texttt{getInfoFilesShared (slice\_name)}] :\\
! 		Env?a una petici?n a multicast de informaci?n de los ficheros
! 		de tipo \textit{shared} del slice \textit{slice\_name}.
  
! 	% TODO: quien lo utiliza?
! 	\item[\texttt{getInfoFilesUnshared (node\_name,slice\_name)}] :\\
! 		Env?a una petici?n unicast hacia el \dplc del nodo
! 		\textit{node\_name} de informaci?n acerca de los ficheros de
! 		tipo \textit{unshared} del slice \textit{slice\_name} para dicho
! 		nodo.
  \end{description}
  
  
  
--- 54,144 ----
  Las operaciones que \dpld ofrece son las siguientes:
  
  \begin{description}
! 	\item[\texttt{deployToSlice (slice\_name, path\_from, path\_to)}]
! 		:\\
! 		Realiza la comunicaci?n con los componentes \dplc para desplegar
! 		un fichero \textit{shared} (\texttt{path\_from}) al slice
! 		\texttt{slice\_name} en \texttt{path\_to}.
! 		\\
! 		Para ello, lee el fichero origen y env?a los datos al grupo
! 		multicast que representa el slice a trav?s de \umcc.
! 		\\
! 		?sta operaci?n provoca un aumento del n?mero de versi?n del
  		slice.
+ 
+ 	\item[\texttt{deployToNode (slice\_name, node\_name, path\_from,
+ 		path\_to) }] :\\
+ 		Lo mismo que \texttt{deployToSlice} pero hacia un nodo concreto
+ 		para desplegar un fichero \textit{unshared}.
+ 
+ 	\item[\texttt{getInfoShared (slice\_name, path)}] :\\
+ 		Hace una petici?n de informaci?n de un objeto \textit{shared}
+ 		del sistema de ficheros del slice \texttt{slice\_name} a un nodo
+ 		cualquiera.
  		\\
! 		Si la petici?n falla, lo intentar? con otro nodo cualquiera,
! 		hasta llegar a un n?mero m?ximo de reintentos.
  
! 	\item[\texttt{getInfoUnshared (slice\_name, node\_name, path)}] :\\
! 		Hace una petici?n de informaci?n de un objeto \textit{unshared}
! 		del sistema de ficheros del slice \texttt{slice\_name} al nodo
! 		\texttt{node\_name}.
! 		\\
! 		Si la operaci?n falla, se reintentar? un n?mero finito de veces.
  
! 	\item[\texttt{getFileShared (slice\_name, path)}] :\\
! 		Hace una petici?n de un objeto \textit{shared} del
! 		sistema de ficheros del slice \texttt{slice\_name} a un nodo
! 		cualquiera.
! 		\\
! 		Si la petici?n falla, lo intentar? con otro nodo cualquiera,
! 		hasta llegar a un n?mero m?ximo de reintentos.
  
! 	\item[\texttt{getFileUnshared (slice\_name, node\_name, path)}] :\\
! 		Hace una petici?n de un objeto \textit{unshared} del sistema de
! 		ficheros del slice \texttt{slice\_name} al nodo
! 		\texttt{node\_name}.
! 		\\
! 		Si la operaci?n falla, se reintentar? un n?mero finito de veces.
  
! 	\item[\texttt{deleteFileShared (slice\_name, path)}] :\\
! 		Hace la petici?n de eliminaci?n de un objeto del sistema de
! 		ficheros (\texttt{path}) de todo el slice \texttt{slice\_name}.
! 		\\
! 		?sta operaci?n provoca un aumento del n?mero de versi?n del
! 		slice.
  
! 	\item[\texttt{deleteFileUnshared (slice\_name, node\_name, path)}] :\\
! 		Hace la petici?n de eliminaci?n de un objeto del sistema de
! 		ficheros (\texttt{path}) de un nodo \texttt{node\_name} del
! 		slice \texttt{slice\_name}.
! 		\\
! 		Si la operaci?n falla, se reintentar? un n?mero finito de veces.
  
! 	\item[\texttt{getSliceNodeVersion (slice\_name, node\_name)}] :\\
! 		Permite obtener la versi?n del slice \texttt{slice\_name} desde
! 		el nodo \texttt{node\_name}.
  
! 	\item[\texttt{registerNeeded (slice\_name, node\_name)}] :\\
! 		Petici?n de necesidad de registro.
! 	
! 	\item[\texttt{addNodeToSlice (slice\_name, node\_name)}] :\\
! 		Informa de la adici?n de un nodo a un slice.
! 
! 	\item[\texttt{removeNodeToSlice (slice\_name, node\_name)}] :\\
! 		Informa de la eliminaci?n de un nodo a un slice.
! 
! 	\item[\texttt{invalidate (slice\_name, node\_name, type, path)}] :\\
! 		Invalida un objeto del sistema de ficheros (un fichero o un
! 		directorio).
  \end{description}
  
+ En todas las anteriores operaciones, excepto las cuatro ?ltimas, se puede dar el
+ caso de que el/los \dplc destinatarios de las operaciones no hayan pasado por
+ un proceso de registro desde el \dpld or?gen, por lo que se puede recibir un
+ aviso de necesidad de registro, al cual reaccionar? con un registro
+ (\textit{unicast} o \textit{multicast} seg?n el n?mero de destinatarios de la
+ operaci?n).
+ 
  
  
***************
*** 102,151 ****
  nodos de PlanetLab.
  
! En concreto, las operaciones que ofrece (API), son:
  
  \begin{description}
! 	\item[\texttt{getSlices}] :\\
  		Permite obtener la lista de slices que se han dado de alta en
  		PlanetLab
  
! 	\item[\texttt{getNodes}] :\\
  		Permite obtener la lista de nodos que se han dado de alta en
  		PlanetLab
  
! 	\item[\texttt{getSlice (slice\_name)}] :\\
! 		Permite obtener informaci?n de un slice
  
! 	\item[\texttt{getNode (node\_name)}] :\\
! 		Permite obtener informaci?n de un nodo
  
! 	\item[\texttt{getSliceNodes (slice\_name)}] :\\
! 		Permite obtener los nodos que hay en un slice
  
  	\item[\texttt{getNodeSlices (node\_name)}] :\\
! 		Permite obtener los slices de un nodo
  
  	\item[\texttt{getSliceNode (slice\_name, node\_name)}] :\\
  		Permite obtener informaci?n de un nodo de un slice
  
! 	\item[\texttt{getNodeSlice (node\_name, slice\_name)}] :\\
! 		Permite obtener informaci?n de un slice de un nodo
! 
! 	\item[\texttt{addSlice (slice\_name)}] :\\
! 		Permite a?adir un slice
! 
! 	\item[\texttt{addNode (node\_name)}] :\\
! 		Permite a?adir un nodo
! 
! 	\item[\texttt{addSliceNode (slice\_name, node\_name)}] :\\
! 		Permite a?adir un nodo en un slice
! 
! 	\item[\texttt{removeSlice (slice\_name)}] :\\
! 		Permite eliminar un slice
! 
! 	\item[\texttt{removeNode (node\_name)}] :\\
! 		Permite eliminar un nodo
  
! 	\item[\texttt{removeSliceNode (slice\_name, node\_name)}] :\\
! 		Permite eliminar un nodo de un slice
  \end{description}
  
--- 148,185 ----
  nodos de PlanetLab.
  
! Las operaciones que ofrece son:
  
  \begin{description}
! 	\item[\texttt{getSlices ()}] :\\
  		Permite obtener la lista de slices que se han dado de alta en
  		PlanetLab
  
! 	\item[\texttt{getNodes ()}] :\\
  		Permite obtener la lista de nodos que se han dado de alta en
  		PlanetLab
  
! 	\item[\texttt{getSliceNodes (slice\_name)}] :\\
! 		Permite obtener la lista de los nodos que hay en un slice
  
! 	\item[\texttt{getSliceNodesAny (slice\_name, num)}] :\\
! 		Permite obtener una lista de \texttt{num} nodos cualesquiera de
! 		un slice
  
! 	\item[\texttt{getSliceNodesNumber (slice\_name)}] :\\
! 		Permite obtener el numero de nodos que hay en un slice
  
  	\item[\texttt{getNodeSlices (node\_name)}] :\\
! 		Permite obtener la lista de los slices de un nodo
  
  	\item[\texttt{getSliceNode (slice\_name, node\_name)}] :\\
  		Permite obtener informaci?n de un nodo de un slice
  
! 	\item[\texttt{getSliceNodeAny (slice\_name)}] :\\
! 		Permite obtener un nodo cualquiera de un slice
  
! 	\item[\texttt{getSliceNodeNearest (slice\_name)}] :\\
! 		Permite obtener el nodo mas cercado de un slice
! 		(aprovech?ndose, si se da el caso, de un sistema DNS con
! 		soporte para ``localidad'').
  \end{description}
  
***************
*** 158,184 ****
  \label{sect:umcc}
  
! ?ste es el m?dulo cliente de la red multicast, b?sicamente tiene dos
! funciones:
  
  \begin{description}
! 	\item Enviar los datos a desplegar al nodo \umcr m?s cercano.
  
! 	\item Recibir los datos a desplegar y hacerlos llegar al m?dulo dplc.
  \end{description}
  
  El primer caso, se da cuando \dpld ordena el despliegue de una
  aplicaci?n/servicio hacia un slice destino. En este caso deber? comunicarse
! con el m?dulo umcr m?s cercano para comunicarle los datos necesarios.
! El segundo caso, se da cuando un nodo \umcr le comunica que se debe desplegar
! una aplicaci?n/servicio en el nodo d?nde \umcc reside, en este caso deber?
! comunicare con el m?dulo \dplc para hacerle llegar los datos de despliegue.
  
  De modo que las operaciones que ofrece, son:
  
- % TODO(~): seguridad a la hora de hacer cambios? umcc va en la m?quina de
- % administraci?n (se puede modificar lo que se quiera)      .... I????
- %TODO(?): ``dns''?  ------- PQ EL GRUPID NO POT SER SLICE_NAME?
- %			    TB SERVIRIA SI VOLEM ENVIAR A SLICE
- %			    DPLD WARNINGS
  \begin{description}
  	\item[\texttt{group\_id getGroup (group\_name)}] :\\
--- 192,216 ----
  \label{sect:umcc}
  
! ?ste es el m?dulo cliente de la red multicast, que b?sicamente tiene las
! dos siguientes funcionalidades:
  
  \begin{description}
! 	\item Enviar los datos al nodo \umcr m?s cercano para que los haga
! 		llegar al grupo de destino.
  
! 	\item Recibir los datos procedentes de \umcr y hacerlos llegar al
! 		componente al que van destinados.
  \end{description}
  
  El primer caso, se da cuando \dpld ordena el despliegue de una
  aplicaci?n/servicio hacia un slice destino. En este caso deber? comunicarse
! con el m?dulo \umcr m?s cercano para comunicarle los datos necesarios.
! 
! El segundo caso, se da cuando un nodo \umcc comunica a \dplc que se debe
! desplegar una aplicaci?n/servicio en el nodo d?nde ?ste reside. En este caso
! deber? comunicase con ?l para hacerle llegar los datos.
  
  De modo que las operaciones que ofrece, son:
  
  \begin{description}
  	\item[\texttt{group\_id getGroup (group\_name)}] :\\
***************
*** 187,211 ****
  
  	\item[\texttt{setOptions (group\_id, options[])}] :\\
! 		%TODO(?): QUE FA AIXO??
! 		%TODO(?): necesario? depende del soft multicast de debajo
  
  	\item[\texttt{sendData (group\_id, data)}] :\\
! 		Permite enviar datos a un grupo
  
  	\item[\texttt{recieveData (group\_id, data)}] :\\
! 		Recibe datos procedentes de un emisor del grupo, y se los pasa
! 		al m?dulo \dplc (quien se encargar? del despliegue.
  
! 	%TODO(~): addGroup / deleteGroup ?
  
! 	\item[\texttt{join (group\_name, node\_name)}] :\\
! 		%TODO(?): hace falta el nombre o ya va bien por remitente?
! 		%TODO(?): c?mo hacer el join de otro?
! 		Permite a?adir un nodo a un grupo multicast
  
! 	\item[\texttt{delete (group\_id, node\_name)}] :\\
! 		%TODO(?): hace falta el nombre o ya va bien por remitente?
! 		%TODO(?): c?mo hacer el delete de otro?
! 		Permite eliminar un nodo de un grupo multicast
  \end{description}
  
--- 219,246 ----
  
  	\item[\texttt{setOptions (group\_id, options[])}] :\\
! 		Permite cambiar las posibles opciones que permita la
! 		implementaci?n concreta del software multicast que usemos.
  
+ 	\item[\texttt{setOptions (group\_id, options[])}] :\\
+ 		Permite obtener las posibles opciones que permita la
+ 		implementaci?n concreta del software multicast que usemos.
+ 	
  	\item[\texttt{sendData (group\_id, data)}] :\\
! 		Permite enviar datos a un grupo.
  
  	\item[\texttt{recieveData (group\_id, data)}] :\\
! 		Permite recibir datos procedentes de un emisor del grupo.
  
! 	\item[\texttt{joinSender (group\_id, node\_name)}] :\\
! 		Permite a?adir un nodo emisor a un grupo multicast.
  
! 	\item[\texttt{deleteSender (group\_id, node\_name)}] :\\
! 		Permite eliminar un nodo emisor de un grupo multicast.
  
! 	\item[\texttt{joinReceiver (group\_id, node\_name)}] :\\
! 		Permite a?adir un nodo receptor a un grupo multicast.
! 
! 	\item[\texttt{deleteReceiver (group\_id, node\_name)}] :\\
! 		Permite eliminar un nodo receptor de un grupo multicast.
  \end{description}
  
***************
*** 214,224 ****
  permite que se puedan utilizar otros proyectos ya realizados.
  
! % TODO(~): estudiarlo!!!
! % El punto que a?n no hemos aclarado es si el propio identificador de grupo
! % multicast ser? el identificador de slice (VOTO POR ELLO!!!!!!!!!!!) o alg?n
! % otro de m?s gen?rico (que requerir?a una nueva capa de administraci?n de
! % grupos multicast).
! %
! % Depende del software de multicast que se utilice! (araneola)
  
  
--- 249,255 ----
  permite que se puedan utilizar otros proyectos ya realizados.
  
! Un punto que queda sin controlar es el acceso restringido a la red multicast,
! es decir faltar?a una autentificaci?n de emisores, por lo que se podr?an
! realizar ataques DoS desde cualquier m?quina a la red.
  
  
***************
*** 228,240 ****
  Estos nodos se encargan de hacer la transmisi?n multicast, con tal de hacer
  llegar los datos a los \umcc del grupo de destino.
  Asumiremos que estos nodos pertenecen a un slice administrativo en el cual no se
  realizan modificaciones o configuraciones maliciosas.
  
! C?mo todos los accesos a la red multicast se hacen a trav?s de \umcc, se puede
  f?cilmente utilizar una implementaci?n ya existente de una red overlay
! multicast en modo usuario, como es Araneola \cite{Araneola}.
  
! %TODO(!!!): EXPLICAR ARANEAOLA SI ES LA UNICA SOLUCIO
! %TODO: Operaciones?
  
  
--- 259,275 ----
  Estos nodos se encargan de hacer la transmisi?n multicast, con tal de hacer
  llegar los datos a los \umcc del grupo de destino.
+ 
  Asumiremos que estos nodos pertenecen a un slice administrativo en el cual no se
  realizan modificaciones o configuraciones maliciosas.
  
! Como todos los accesos a la red multicast se hacen a trav?s de \umcc, se puede
  f?cilmente utilizar una implementaci?n ya existente de una red overlay
! multicast en modo usuario
  
! En cuanto a las implementaciones, hemos encontrado varias \cite{Araneola, ESM,
! LMDD, LSAM}, pero consideramos que Araneola \cite{Araneola} se ajusta m?s a
! nuestras necesidades que las otras implementaciones por el hecho de haber sido
! dise?ada para entornos din?micos y con la fiabilidad en mente, adem?s de ser un
! sistema de comunicaci?n de M a N.
  
  
***************
*** 242,438 ****
  \section{\dplc}
  
- % TODO: pertenece a un slice administrado correctamente (no habr?
- % modificaciones ni configuraciones maliciosas)
- 
  ?ste m?dulo, situado en cada una de las m?quinas clientes (o receptoras de las
  aplicaciones de las que queremos hacer el despliegue), se sit?a en un
! slice propio (donde estar?n todos los nodos con ?ste m?dulo), y al recibir los
! datos de \umcc acceder? a s? mismo por ssh con tal de poner los ficheros al
! slice destino y seguidamente ejecutar el shell script asociado, en caso de
! estar presente.
  
! Para realizar esta conexi?n por ssh, el nodo del slice dplc necesita los datos
  de clave y password ssh para que \dplc pueda hacer una conexi?n a la propia
  m?quina y acceder a la m?quina virtual asociada al slice de destino.
  
! Para ello, se utiliza el proceso de autentificaci?n descrito en el apartado
! \ref{sect:security}.
  
  De modo que las operaciones que debe implementar este m?dulo son las
  siguientes:
  
- % TODO(?): seguridad?
  \begin{description}
! 	% TODO: que es _la informaci?n_?
! 	\item[\texttt{getInfoNodeSlice(slice\_name)}] :\\
! 		Obtiene la informaci?n de los ficheros tipo \textit{unshared}
! 		de la aplicaci?n desplegada en el slice \textit{slice\_name} en
! 		el mismo nodo que el propio \dplc.
  
! 	% TODO: ELIMINAR: llama a la operaci?n, no la ofrece
! 	\item[\texttt{warnDpldNewNodeInSlice (slice\_name, node\_name)}] :\\
! 		Avisa a los \dpld encargados del slice \textit{slice\_name} de
! 		que se ha agregado un nuevo nodo \textit{node\_name} a dicho a
! 		slice y que por lo tanto se le deber?a realizar el despliegue de
! 		la aplicaci?n.
  
! 	\item[\texttt{deployAppToSlice (slice\_name, ...)}] :\\
! 		Realiza la conexi?n por ssh al slice destino, despliega la
! 		aplicaci?n y ejecuta el script de inicializaci?n.
  
! 	\item[\texttt{registerToDplc (node\_name, slice\_name)}] :\\
! 		Un nodo se registra en \dplc como integrante del slice
! 		slice\_name al que se va realizar despliegue de
! 		aplicaciones.
  
! 	% TODO: para que? se para sola... hace falta ofrecer la opci?n de
! 	% pararla?
! 	\item[\texttt{stopAppInSlice (slice\_name)}] :
! 		El \dplc del nodo ejecutar? script->stop en el slice
! 		\textit{slice\_name} de dicho nodo. Es decir, detendr? la
! 		aplicaci?n desplegada en ?l.
  
! 	\item[\texttt{invalidateObj (slice\_name, name)}] :\\
! 		% TODO(~): utilizar identificador de obj diferente del path?
! 		% hace falta nombre del slice? (path completo) <<-- PATH COMPLET
! 		Invalida un objeto del sistema de ficheros (un fichero o un
! 		directorio)
  
! 	% TODO(?): validaciones? temporalidad?
  \end{description}
  
! % TODO(~): se?alar los triggers de join / delete de los grupos
  
! % TODO:
! %Cuando se a?ade un nodo a un slice en el que se realiza despliegue, se
! %tiene que informar al m?dulo \dpld, puesto que es posible que ya se hubiera
! %realizado un desplagamiento previo a dicho slice y \dpld tenga que realizar un
! %deployment al nodo en concreto.
! %
! %Para ello hemos pensado en un protocolo de "warnings" en el que \dplc realizara
! %comunicaciones multicast hacia los nodos \dpld, comunic?ndoles:
! %	- nodo
! %	- slice
! %
! % ===>>> LA QUAL COSA ACABARIA AMB L'OPERACIO QUE HE AFEGIT addNodeToSlice de
! %	dpld
! %
! % Como diferenciar al entrar un nodo en un grupo de si ya esta al dia o no?
! % - no se hace nada (solo hay cambios si esta presente en un deploy expreso)
! % - tiene la ultima version de los ficheros (numeraci?n + indicador de ultima
! %   version (*))
! % - los tiene igual que la mayoria (checksums + problema bizantino)
! % - los tiene igual que un repositorio (checksums + repositorio - bamboo? -)
! %
! % (*) Puede ser:
! % o el nunmero mas grande que den los miembros del grupo
! %   - deben responder todos
! %   - alguien puede enga?arnos y dar un numero equivocado?
! %   -> dplc nos firma la version -> garantiza la validez (como el algodon,
! %     dplc nuca enga?a)
! % o el numero que den la mayoria
! %   - deben responder todos
! %   - podemos tirar a una version anterior (pq la mayoria de ahora no estaban
! %     presentes en un deployment anterior y decidieron al connectar que la
! %     version era esta)
! % o lo que digan los dpld (mayoria o maximo)
! %   - debe haber dpld en marcha
! %   + #dpld < #dplc
! % o lo que diga un repositorio central
! %   - poca escalabilidad
! %   - punto unico de fallada
! %   + resultados fiables
! %
! % Al detectar un desfase de las versiones (nodo corrupto?), que hacer:
! % - protocolo de las transparencias
! % - parar y esperar al siguiente deployment
! %
! % ---------------------------------------------------------------
! % => Posible estrategia de sincronizacion por numero de version:
! % ---------------------------------------------------------------
! %
! % => Precondicion: queremos que todos los nodos tengan el mismo shared
! %     siempre!
! %
! % => Formato de version:
! % - numeracion consecutiva
! % - numeracion consecutiva con separadores (.), siendo el override del usuario
! %   el que permite avanzar a una version superior no consecutiva 1.9 -> 2.0
! % - fecha en formato universal (estan los nodos de PL en hora?); como los
! %   deploys no son muy frecuentes, es bastante fiable aun con desfases de
! %   reloj
! %
! % => Como obtener nodos cualesquiera
! % hacer una peticion de la lista a PLC y seleccionar uno aleatoriamente
! %
! % => Pasos
! % - cada nodo en su slice guarda un fichero con el numero de version que se le
! %   ha desplegado (si no existe, se supono que no hay despliegue previo)
! %   (posibilidad de ver un fichero en unshared?)
! %   (posibilidad de ver un fichero en shared con el valor maximo actual?)
! %
! % - dpld obtiene el numero de version que actualmente hay en la red
! %   (no necesario si es version por fecha)
! %   * cuando:
! %     - al hacer el deploy, pregunta
! %     - en alguna operacion anterior ya lo va haciendo en background por si a
! %       caso
! %     - al hacer el registro unicast, el resultado es la version actual
! %   * como:
! %     - repositorio central (a modo transaccional, solo se actualiza el valor
! %       si se hace el deploy de como minimo N nodos - 1? quizas mas si
! %       utilizamos tecnicas de fallo bizantino? -)
! %     - se guarda el valor en la maquina dpld (demasiado rigido, no podemos
! %       hacer deploy desde diferentes sitios)
! %     - se le hace poner un valor en un fichero al usuario (esta bien como
! %       posibilidad para forzar el valor, pero no como obligacion)
! %     - el valor que nos den de una sola consulta (podria estar desfasado)
! %     - el valor maximo de N preguntas
! %       * si el valor se guarda en el slice, podria estar falseado? a quien le
! %         interesa si es su propio slice?
! %       * si el valor se guarda en dplc, seguro que siempre es el ultimo que
! %         dplc ha visto, no?
! %     - el valor mas ``votado'', por si hubiera alguno falseado (lo habria?)
! %
! % - hace el deploy indicando el nuevo numero de version
! %
! % - existe la posibilidad de que los dplc hagan piggybacking indicando su
! %   version actual, para que dpld:
! %   * se ponga al dia (hay algun problema en coger la mayor aunque solo la
! %     tenga uno? -> pero que no sea de un corrupto!)
! %   * dpld le indique que necesita actualizacion y
! %     - le haga un deploy unicast
! %     - le indique un vecino cercano con la version ``buena''
! %
! % - al arrancar o recuperar la conexion un dplc, obtiene el numero ``bueno''
! %   de version tal como lo hace dpld
! %
! % - si esta desactualizado, obtiene la nueva version de un peer no corrupto
! %
! % - cada dplc guarda los ultimos N nodos (incluyendo dpld) que le han
! %   preguntado su version, si se le cambia la version, lo notifica a estos
! %
! % - cuando llega una notificacion de cambio de version, se hace un redeploy
! %   desde ese peer originador del mensaje
! %   [ deploy virico para los que se han quedado rezagados ]
! %
! % - existencia de un fichero en /plfs/slices/<slice> que al hacerle un cat
! %   intenta contactar todos los nodos y mostrar su estado
! %
! % => interesante la deteccion de que nodos estan en la misma subred (fichero
! %    de conf de dplc con las redes locales?)
! %    al entrar en funcionamiento, lo pueden hacer en grupo (corte de luz o
! %    reconexion a inet - gracias al RFC TRACK multicast, se da aviso de
! %    eliminacion del grupo multicast, verdad? -)
! %
! % => las caidas son aleatorias, por lo que afectan a nodo separados (bloqueos
! %    de la maquina) o a una institucion o parte de ella (red local)
! %    => no hace falta un redeploy multicast (no habra tantos a la vez como
! %       para hacerlo preferible y viable)
! %    => designar un cabeza de grupo que lo pide a un peer exterior y luego
! %       hace un deploy unicast a
! %       - la direccion multicast del grupo institucional
! %       - cada peer local (si no se puede por multicast... se dara el
! %         caso?)
  
  
--- 277,387 ----
  \section{\dplc}
  
  ?ste m?dulo, situado en cada una de las m?quinas clientes (o receptoras de las
  aplicaciones de las que queremos hacer el despliegue), se sit?a en un
! slice propio (donde estar?n todos los nodos con ?ste m?dulo), que
! consideraremos administrado correctamente (por lo que no habr? configuraciones
! ni modificaciones maliciosas) y al recibir los datos de \umcc acceder? a s?
! mismo por ssh con tal de poner los ficheros al slice destino y seguidamente
! ejecutar el shell script asociado, en caso de estar presente.
  
! Para realizar esta conexi?n por ssh, el nodo del slice \dplc necesita los datos
  de clave y password ssh para que \dplc pueda hacer una conexi?n a la propia
  m?quina y acceder a la m?quina virtual asociada al slice de destino.
  
! Para ello, se utiliza el proceso de registro descrito en el apartado
! \ref{sect:security}. Pero los datos de conexi?n tambi?n puede obtenerse de otro
! nodo \textit{peer} \dplc, como se detalla en el apartado
! \ref{sect:redeployment}.
  
  De modo que las operaciones que debe implementar este m?dulo son las
  siguientes:
  
  \begin{description}
! 	\item[\texttt{deployToSlice (slice\_name, path\_to, file, type,
! 		new\_version)}] :\\
! 		Despliega el fichero \texttt{file} al path indicado a trav?s de
! 		la conexi?n SSH.
! 		\\
! 		El fichero puede ser \textit{shared} o \textit{unshared} seg?n
! 		indique \texttt{type}, guardando siempre una relaci?n de qu?
! 		tipo es cada fichero.
! 		\\
! 		?sta operaci?n provoca un aumento del n?mero de versi?n del
! 		slice a \texttt{new\_version}.
  
! 	\item[\texttt{getInfo (slice\_name, path)}] :\\
! 		Obtiene la informaci?n de un objeto del sistema de ficheros
! 		(necesario tanto para listar los contenidos de directorios, como
! 		para que \dpld los revalide al re-registrarse).
  
! 	\item[\texttt{getFile (slice\_name, path)}] :\\
! 		Hace una petici?n de un objeto del sistema de ficheros
! 		del slice \texttt{slice\_name}.
! 	
! 	\item[\texttt{deleteFile (slice\_name, path, new\_version)}] :\\
! 		Hace la petici?n de eliminaci?n de un objeto del sistema de
! 		ficheros (\texttt{path}) de todo el slice \texttt{slice\_name}.
! 		\\
! 		?sta operaci?n provoca un aumento del n?mero de versi?n del
! 		slice a \texttt{new\_version}.
  
! 	\item[\texttt{getVersion (slice\_name)}] :\\
! 		Devuelve la versi?n que el nodo tiene del slice
! 		\texttt{slice\_name}.
  
! 	\item[\texttt{getKey (slice\_name, $K_{pub_{d}}$)}] :\\
! 		Informa de la clave p?blica remota y devuelve la propia clave
! 		p?blica.
  
! 	\item[\texttt{getSSH (slice\_name)}] :\\
! 		Devuelve la clave y password SSH del slice \texttt{slice\_name}.
  
! 	\item[\texttt{ping (slice\_name, node\_name, version)}] :\\
! 		Informa de la versi?n de despliegue que
! 		\texttt{node\_name} tiene del slice \texttt{slice\_name}
! 		desplegada.
! 		\\
! 		Como consecuencia, el nodo receptor realiza un \texttt{ping} al
! 		nodo emisor.
! 
! 	\item[\texttt{needDeploy (slice\_name, node\_name, path)}] :\\
! 		Informa de la necesidad de \texttt{node\_name} de recibir un
! 		deploy de \texttt{path} del slice \texttt{slice\_name}.
! 
! 	\item[\texttt{registerWithResponse (slice\_name, node\_name, $K_{SSH}$,
! 		$P_{SSH}$, $K_{pub}$)}] :\\
! 		Permite a \texttt{node\_name} registrarse en \dplc como
! 		integrante del slice y devuelve un resultado para informar si
! 		se ha podido realizar la conexi?n SSH.
! 
! 	\item[\texttt{register (slice\_name, node\_name, $K_{SSH}$, $P_{SSH}$,
! 		$K_{pub}$)}] :\\
! 		Permite a \texttt{node\_name} registrarse en \dplc como
! 		integrante del slice.
! 
! 	\item[\texttt{joined (slice\_name)}] :\\
! 		Informa de la adici?n del nodo al grupo multicast del slice
! 		\texttt{slice\_name}.
! 
! 	\item[\texttt{deleted (slice\_name)}] :\\
! 		Informa de la eliminaci?n del nodo al grupo multicast del slice
! 		\texttt{slice\_name}.
! 
! 	\item[\texttt{invalidate (slice\_name, path)}] :\\
! 		Invalida un objeto del sistema de ficheros (un fichero o un
! 		directorio).
! 		\\
! 		Si el fichero estaba en la lista, que \dplc mantiene,
! 		como \textit{shared}, ejecuta un \texttt{script->stop}.
  \end{description}
  
! Como consecuencia de las operaciones de registro, \dplc intenta abrir la
! conexi?n SSH con los datos obtenidos (si no estaba abierta), o los verifica (si
! ya estaba abierta), iniciando o reseteando el contador de \textit{timeout} del
! registro y arrancando una instancia de \famon para ?se slice si no estaba ya en
! marcha.
  
! Cuando ``salta'' el \textit{timeout}, si no quedan m?s registros para ?se
! slice, se apaga \famon y se cierra la conexi?n SSH.
  
  
***************
*** 444,467 ****
  
  Para ello debe haber una instancia corriendo en cada m?quina virtual que
! corresponda a un slice que funciona a traves de \plfs.
  
! % TODO: quien/cuando se arranca?
! % TODO: que se vilgila?
! % - ficheros desplegados expresamente
! % - ficheros resultado de (genereados despues de) desplegar y/o arrancar la aplicacion
! %   (como se detecta que crea la aplicacion y que lo hace el usuario u otro proceso que no es
! %   la aplicacion desplegada?)
! %   (si se detecta, como diferenciar shared/unshared?)
! %
! % Yo solo vigilaria lo desplegado expresamente.
  
! % TODO (no va aqui)
! % script->deploy : depliega la aplicacion y guarda una lista de ficheros extra generados en la posible descompresion
! %    (mejor script->install)
! % script->remove : elimina los ficheros instalados, para poder instalar otra vez (posible nueva version)
! % script->update?
  
! % Las operaciones que debe ofrecer \famon son las siguientes:
! % TODO: operaciones de viliar / dejar paths concretos?
  
! % TODO: buscar un buen programa que lo haga => famd, gamin?
--- 393,414 ----
  
  Para ello debe haber una instancia corriendo en cada m?quina virtual que
! corresponda a un slice que funciona a traves de \plfs, siendo arrancado por el
! correspondiente \dplc en el momento en que hay alg?n \dpld registrado.
  
! Las operaciones que nos interesan son las siguientes:
  
! \begin{description}
! 	\item[\texttt{watch (path)}] :\\
! 		Activa la monitorizaci?n sobre \texttt{path}
  
! 	\item[\texttt{unwatch (path)}] :\\
! 		Desactiva la monitorizaci?n sobre \texttt{path}
  
! 	\item[\texttt{stop ()}] :\\
! 		Detiene \famon
! \end{description}
! 
! Hemos encontrado diversas implementaciones \cite{FAM, Gamin}, y de ellas hemos
! preferido \textit{Gamin} \cite{Gamin}, por implementar un subconjunto m?s
! sencillo de las operaciones que define el modelo de \textit{FAM} definido por
! \textit{SGI} y siendo un programa que consume as? menos recursos.

Index: 04.tex
===================================================================
RCS file: /cvsroot/plfs/doc/04.tex,v
retrieving revision 1.6
retrieving revision 1.7
diff -C2 -d -r1.6 -r1.7
*** 04.tex	13 Jun 2005 19:57:46 -0000	1.6
--- 04.tex	17 Jun 2005 18:57:21 -0000	1.7
***************
*** 8,14 ****
  implementar en cada una, para asegurar autenticidad y confidencialidad.}}
  
  
  
! \section{\kplfs $\rightarrow$ \uplfs}
  
  Para que la aplicaci?n en modo usuario pueda enterarse de las operaciones
--- 8,32 ----
  implementar en cada una, para asegurar autenticidad y confidencialidad.}}
  
+ Las comunicaciones entre componentes de un mismo nodo funcionan a modo de
+ llamadas ``normales'' entre partes de un mismo programa (aunque se carguen en
+ forma de \textit{plugins}).
  
+ La comunicaci?n entre componentes de nodos separados se realiza siempre mediante
+ el paso de mensajes que siguen ?ste formato:
  
! \begin{enumerate}
! 	\item IDoperaci?n
! 
! 	\item Remitente
! 
! 	\item Datos de la operaci?n (par?metros o resultado)
! \end{enumerate}
! 
! Seguidamente pasamos a concretar los detalles de comunicaci?n de los diferentes
! componentes.
! 
! 
! 
! \section{\kplfs $\longleftrightarrow$ \uplfs}
  
  Para que la aplicaci?n en modo usuario pueda enterarse de las operaciones
***************
*** 17,21 ****
  Para realizar esta comunicaci?n de eventos, hay varias posibilidades:
  
- % TODO(~): hay mas posibilidades?
  \begin{description}
  	\item [Dispositivo:]
--- 35,38 ----
***************
*** 34,180 ****
  utilizaremos un \textbf{dispositivo} para comunicar ambos componentes.
  
- 
- \subsection{Tipos de mensajes}
- 
- % TODO: hace falta esta explicaci?n? ya esta hecha antes...
  Cuando se realice una operaci?n (\texttt{ls},\texttt{mv}, etc) en el sistema
  de ficheros \plfs, el m?dulo de kernel \kplfs que implementa estas operaciones,
  deber? informar dichos eventos al m?dulo a nivel usuario \uplfs (el cual deber?
! realizar las operaciones pertinentes).
! Para ello, hemos pensado que el m?dulo \kplfs realizar?a la comunicaci?n de
! estos eventos a \uplfs mediante paso de mensajes, que deber?n tener el
! siguiente formato:
! 
! % TODO: aclararlo con las operaciones de 02.tex/03.tex
! 	Campo1: Operaci?n ("ls", "mv", ...)
! 	Campo2: Parametros (fichero, directorio...)
! 
! 
! 
! \section{\uplfs $\rightarrow$ \pldb}
! %TODO: REVISAR :PPPP
! Cuando el usuario quiere mirar el contenido del directorio de slices o de
! nodos de un slice, y \kplfs remite dicha operaci?n a \uplfs, este realiza la
! consulta de dicha informaci?n llamando a las operaciones que ofrece \pldb, por
! ejemplo mediante llamadas a procedimiento remoto (como podr?a ser Java RMI).
  
! Una vez obtenida la informaci?n, \uplfs proceder?a a pas?rsela a \kplfs el cual
! actualizar?a los inodos de \plfs y mostrar?a el resultado al usuario.
  
  
  
! \section{\uplfs $\rightarrow$ \dpld}
  
! % TODO: forma de comunicaci?n
! % TODO: formato de los mensajes
  
  
  
! \section{\dpld $\rightarrow$ \pldb}
  
! %TODO(~) An?logamente a \uplfs -> \pldb
  
  
  
  \section{\dpld $\rightarrow$ \dplc}
- % TODO: problema de comunicaci?n directa: puertos de destino?
- % no es lo mismo dpld->dplc que dpld->ummc->dplc!
- 
- En primer lugar antes de realizar ninguna operaci?n de despliegue o consulta a
- slice destinos, debemos realizar el proceso de registro y
- autentificaci?n siguiente:
- 
- \begin{enumerate}
- 	\item \dpld obtiene la clave p?blica del slice \dplc.
- 
- 	\item \dpld selecciona un nodo cualquiera del slice \dplc.
- 
- 	\item \dpld se registra en dicho nodo cifrando con la clave p?blica de
- 		\dplc la clave y password de ssh, y adem?s manda su propia clave
- 		p?blica.
- 
- 	\item El nodo \dplc descifra la clave y password ssh e intenta la conexi?n
- 		ssh al slice destino. En caso negativo se manda un mensaje de retorno
- 		y se aborta el proceso de registro.
- 
- 		\begin{center}
- 			\texttt{ssh \textless{}slice\_destino\textgreater{}@localhost -i
- 			\textless{}clave\_privada\textgreater}
- 		\end{center}
- 
- 	\item El nodo \dplc manda un mensaje a \dpld aceptando el registro. En el
- 		caso que dicho mensaje de aceptaci?n tarde un cierto tiempo
- 		\textit{timeout} se repiten los pasos anteriores con otro nodo \dplc.
- 
- 	\item \dpld env?a registro multicast al slice destino con el password y
- 		clave ssh cifrados con la clave p?blica de \dplc y adem?s env?a la
- 		clave p?blica de \dpld.
- 
- 	\item Todos los nodos \dplc a los que llega el mensaje, realizan la
- 		conexi?n ssh y almacenan la asociaci?n \textless slice destino - \dpld
- 		origen - clave p?blica del \dpld \textgreater para futuras
- 		comunicaciones.
- \end{enumerate}
  
! Una vez realizado el proceso de registro, cualquier operaci?n (despliegue,
! consulta, etc) se realizar? siguiendo los pasos siguientes:
! 
! \begin{enumerate}
! 	\item \dpld cifra los datos a enviar con la clave p?blica de \dplc.
! 
! 	\item \dpld crea una firma de los datos con su clave privada.
! 
! 	\item \dplc comprueba la firma y descifra los datos.
! 
! 	\item \dplc realiza la operaci?n deseada.
! \end{enumerate}
! 
! Si en alg?n momento se contacta con alg?n nodo \dplc reiniciado o nuevo con el
! que no se est? registrado, se procede a realizar un registro unicast nuevo.
! 
! 
! \subsection{Tipos de mensajes}
! 
! \begin{figure}[h]
! 	\centering
! 	\includegraphics[scale=0.5]{paq_-_uplfs-pldb.eps}
! 	\caption{Paquete gen?rico \uplfs $\rightarrow$ \dplc}
! 	\label{fig:paq_-_uplfs-pldb}
! \end{figure}
! 
! 
! \subsubsection{Datos}
! % TODO: En caso de que el paquete contenga el campo \texttt{Cmd}.....
! 
! \begin{figure}[h]
! 	\centering
! 	\includegraphics[scale=0.5]{paq_-_uplfs-pldb_-_data.eps}
! 	\caption{Paquete de datos \uplfs $\rightarrow$ \dplc}
! 	\label{fig:paq_-_uplfs-pldb_-_data}
! \end{figure}
! 
! % TODO: Datos => lista ficheros + ficheros?
! %       (kernel permite agrupaci?n de ficheros en un env?o?)
! % TODO: si ya se pre-acredita, no hace falta ahora enviar passwd+clave
! % TODO: otros tipos de paquete?
! 
! \begin{description}
! 	\item[Slice destino:] nombre del slice destino al que va destinado el
! 		paquete
! 
! 	\item[Clave/Password privados ssh:] clave y password privados de ssh
! para
! 		que	\dplc se comunique con el slice destino
! 
! 	\item[Datos:] datos a desplegar en el slice destino
! 
! 	\item[Shell script:] shell script a ejecutar una vez desplegados los
! 		ficheros (optativo)
! \end{description}
! 
! 
! \subsubsection{Registro de oyente}
  
! % TODO Formate de paquete de registro
  
  
--- 51,87 ----
  utilizaremos un \textbf{dispositivo} para comunicar ambos componentes.
  
  Cuando se realice una operaci?n (\texttt{ls},\texttt{mv}, etc) en el sistema
  de ficheros \plfs, el m?dulo de kernel \kplfs que implementa estas operaciones,
  deber? informar dichos eventos al m?dulo a nivel usuario \uplfs (el cual deber?
! realizar las operaciones pertinentes, comunic?ndose con los diferentes
! componentes disponibles).
  
! En ?ste caso, aunque los componentes est?n en el mismo nodo, deben enviarse
! mensajes a trav?s del dispositivo de control, mensajes que tienen el mismo
! formato que se ha comentado al inicio del cap?tulo, pero sin el par?metro de
! \texttt{Remitente}.
  
  
  
! \section{\uplfs/\dpld $\rightarrow$ \pldb}
  
! Llamada interna de funciones.
  
  
  
! \section{\uplfs $\longleftrightarrow$ \dpld}
  
! Llamada interna de funciones.
  
  
  
  \section{\dpld $\rightarrow$ \dplc}
  
! Todas las comunicaciones van cifradas con $K_{pub_{c}}$ (para asegurar que s?lo
! los \dplc puedan leer los contenidos) y firmadas con $K_{priv_{d}}$ (para poder
! as? comprovar su autenticidad).
  
! Luego, en el destino, se comprueba la firma, se descifran los datos y se
! procede a realizar la operaci?n.
  
  
***************
*** 183,210 ****
  \label{sect:warnings}
  
! Los nodos \dplc deben informar de los cambios en los grupos slice destino
! a los \dpld que est?n registrados en ellos para esos slice.
! 
! Los cambios pueden ser:
! \begin{itemize}
! 	\item Se a?ade un nuevo nodo al slice.
! 
! 	\item Se quita un nodo del slice.
! 
! 	\item Modificaci?n de ficheros (invalidaci?n).
  
! 	\item Se requiere autentificaci?n.
! \end{itemize}
  
- Para anunciar estos cambios hemos dise?ado un paquete de warning mediante el
- cual los nodos \dplc los anuncian a los \dpld registrados.
  
- %TODO figura paquete warning
- Campo 1: tipo de cambio.
- Campo 2: string (nodo o fichero).
- Campo 3: shared o unshared
- ...
  
! Para autentificar los paquetes de warning se firman con la clave privada
! de \dplc.
  
--- 90,108 ----
  \label{sect:warnings}
  
! Todas las comunicaciones van cifradas con $K_{pub_{d}}$ (para asegurar que s?lo
! los \dplc puedan leer los contenidos) y firmadas con $K_{priv_{c}}$ (para poder
! as? comprovar su autenticidad).
  
! Luego, en el destino, se comprueba la firma, se descifran los datos y se
! procede a realizar la operaci?n.
  
  
  
! \section{\dpld/\dplc $\rightarrow$ \umcc}
  
+ Cuando tanto \dpld como \dplc quieren comunicarse con un grupo (\dpld se
+ comunica con el grupo de receptores del slice para mandarles operaciones y
+ \dplc se comunica con el grupo de emisores para mandarles avisos), lo hacen a
+ trav?s de la red multicast, mediante el componente de entrada a ella que es
+ \umcc, de forma que mandan los datos como si fueran directamente una de las dos
+ comunicaciones anteriores y se transmiten transparentemente por la red \umc.

Index: ToDo.txt
===================================================================
RCS file: /cvsroot/plfs/doc/ToDo.txt,v
retrieving revision 1.2
retrieving revision 1.3
diff -C2 -d -r1.2 -r1.3
*** ToDo.txt	7 Jun 2005 14:47:59 -0000	1.2
--- ToDo.txt	17 Jun 2005 18:57:21 -0000	1.3
***************
*** 1,6 ****
  Multicast:
  ---------
- ->http://www.ee.technion.ac.il/people/idish/Abstracts/araneola.html
- ->http://dsl.cs.technion.ac.il/completed_projects/Araneola/html/araneola.htm
  > http://www.aqualab.cs.northwestern.edu/projects/Nemo.html
  > http://www.aqualab.cs.northwestern.edu/publications/SBirrer04RGU.pdf
--- 1,4 ----



From nobody at sheep.berlios.de  Sun Jun 19 15:00:35 2005
From: nobody at sheep.berlios.de (xscript)
Date: Sun, 19 Jun 2005 15:00:35 +0200
Subject: [Plfs-svn] doc 00-plfs.bib,1.5,1.6
Message-ID: <200506191300.j5JD0ZI10792@bat.berlios.de>

Update of /cvsroot/plfs/doc
In directory sheep:/tmp/cvs-serv10121

Modified Files:
	00-plfs.bib 
Log Message:
Actualizacion de la Bibliografia

Index: 00-plfs.bib
===================================================================
RCS file: /cvsroot/plfs/doc/00-plfs.bib,v
retrieving revision 1.5
retrieving revision 1.6
diff -C2 -d -r1.5 -r1.6
*** 00-plfs.bib	17 Jun 2005 18:57:21 -0000	1.5
--- 00-plfs.bib	19 Jun 2005 13:00:32 -0000	1.6
***************
*** 15,23 ****
  
  @manual{Araneola,
! 	title = {Araneola: A Scalable Reliable Multicast System for Dynamic
! 		Environments},
! 	note =
! {\url{http://dsl.cs.technion.ac.il/completed_projects/Araneola/html/araneola.htm
! }}
  }
  
--- 15,20 ----
  
  @manual{Araneola,
! 	title = {Araneola: A Scalable Reliable Multicast System for Dynamic Environments},
! 	note = {\url{http://dsl.cs.technion.ac.il/completed_projects/Araneola/html/araneola.htm}}
  }
  
***************
*** 29,36 ****
  @manual{LMDD,
  	title = {Logistical Multicast for Data Distribution},
! 	note =
! {\\\url{
! http://loci.cs.utk.edu/modules.php?name=Publications&d_op=ViewPublication&lid=24
! 3}}
  }
  
--- 26,30 ----
  @manual{LMDD,
  	title = {Logistical Multicast for Data Distribution},
! 	note = {\\\url{http://loci.cs.utk.edu/modules.php?name=Publications&d_op=ViewPublication&lid=243}}
  }
  
***************
*** 48,60 ****
  }
  
  
  ######################
  # FileSystems
  
- #--- no utilizados ---
- 
  @manual{Intermezzo,
  	title = {Intermezzo File System},
! 	note = {\url{http://www.inter-mezzo.org/}}
  }
  
--- 42,110 ----
  }
  
+ @manual{nemo,
+ 	title = {Nemo - Resilient Overlay Multicast},
+ 	author = {Stefan Birrer and Fabi?n E. Bustamante},
+ 	note = {\url{http://www.aqualab.cs.northwestern.edu/projects/Nemo.html}}
+ }
+ 
+ @misc{nemo-res-p2p-mcast,
+ 	title = {Resilient Peer-to-Peer Multicast from the Ground Up},
+ 	author = {Stefan Birrer and Fabi?n E. Bustamante},
+ 	howpublished = {IEEE Network Computing and Applications - Workshop on
+ 		Adaptive Grid Computing, Cambridge},
+ 	month = {August-September},
+ 	year = {2004},
+ 	note =
+ {\url{http://www.aqualab.cs.northwestern.edu/publications/SBirrer04RGU.pdf}}
+ }
+ 
+ @manual{stealth,
+ 	title = {Stealth Multicast},
+ 	note = {\url{http://www.cse.nd.edu/~striegel/research/stealth}}
+ }
+ 
  
  ######################
  # FileSystems
  
  @manual{Intermezzo,
  	title = {Intermezzo File System},
! 	note = {\url{http://www.inter-mezzo.org}}
! }
! 
! 
! @manual{libfs,
! 	title = {Creating Linux virtual filesystems},
! 	note = {\url{http://lwn.net/Articles/57369}}
! }
! 
! @manual{lk,
! 	title = {The Linux Kernel: The Linux Virtual File System},
! 	note = {\url{http://www.win.tue.nl/~aeb/linux/lk/lk-8.html}}
! }
! 
! @manual{lk24i,
! 	title = {Linux Kernel 2.4 Internals: Virtual File System (VFS)},
! 	note = {\\\url{http://www.moses.uklinux.net/patches/lki-3.html}}
! }
! 
! @manual{lkapi,
! 	title = {The Linux Kernel API: The Linux VFS},
! 	note = {\\\url{http://kernelnewbies.org/documents/kdoc/kernel-api/vfs.html}}
! }
! 
! @manual{lkmpg,
! 	title = {The Linux Kernel Module Programming Guide},
! 	note = {\url{http://tldp.org/LDP/lkmpg}}
! }
! 
! @manual{ovfs,
! 	title = {Overview of the Virtual File System},
! 	note = {\url{http://www.atnf.csiro.au/people/rgooch/linux/vfs.txt}}
! }
! 
! @manual{tlk,
! 	title = {The Linux Kernel: The File System},
! 	note = {\url{http://en.tldp.org/LDP/tlk/fs/filesystem.html}}
  }
  
***************
*** 63,68 ****
  # Redes
  
- #--- no utilizados ---
- 
  @manual{RON,
  	title = {Resilient Overlay Networks},
--- 113,116 ----
***************
*** 74,77 ****
--- 122,130 ----
  # Otros
  
+ @manual{Bamboo,
+ 	title = {The Bamboo Distributed Hash Table},
+ 	note = {\url{http://bamboo-dht.org}}
+ }
+ 
  @manual{Codeen,
  	title = {Codeen},
***************
*** 94,126 ****
  }
  
! @manual{PLC,
! 	title = {PlanetLab Central API Documentation},
! 	note = {\url{http://www.planet-lab.org/doc/api/plc_api/}}
  }
  
- #--- no utilizados ---
- 
  @manual{NWS,
  	title = {Network Weather System},
! 	note = {\url{http://nws.cs.ucsb.edu/}}
  }
  
! @manual{RSTB,
! 	title = {A Replicated Server Testbed},
! 	note = {\url{http://www.cs.utk.edu/~mbeck/replication.html}}
  }
  
  @manual{PLSDT,
  	title = {PlanetLab Slice Deploy Toolkit},
! 	note = {\url{http://jabber.services.planet-lab.org/php/software/tools.php}}
  }
  
! @manual{Bamboo,
! 	title = {The Bamboo Distributed Hash Table},
! 	note = {\url{http://bamboo-dht.org/}}
  }
  
  @manual{Stork,
  	title = {Stork},
! 	note = {\url{http://www.cs.arizona.edu/stork/}}
  }
--- 147,183 ----
  }
  
! @manual{MON,
! 	title = {Management Overlay Networks (MON)},
! 	note = {\url{http://cairo.cs.uiuc.edu/mon}}
  }
  
  @manual{NWS,
  	title = {Network Weather System},
! 	note = {\url{http://nws.cs.ucsb.edu}}
  }
  
! @manual{PLC,
! 	title = {PlanetLab Central API Documentation},
! 	note = {\url{http://www.planet-lab.org/doc/api/plc_api/}}
  }
  
  @manual{PLSDT,
  	title = {PlanetLab Slice Deploy Toolkit},
! 	note =
! {\\\url{http://jabber.services.planet-lab.org/php/software/tools.php}}
  }
  
! @manual{PowerDNS,
! 	title = {PowerDNS},
! 	note = {\url{http://www.powerdns.com}}
! }
! 
! @manual{RSTB,
! 	title = {A Replicated Server Testbed},
! 	note = {\url{http://www.cs.utk.edu/~mbeck/replication.html}}
  }
  
  @manual{Stork,
  	title = {Stork},
! 	note = {\url{http://www.cs.arizona.edu/stork}}
  }



From nobody at sheep.berlios.de  Sun Jun 19 15:00:52 2005
From: nobody at sheep.berlios.de (xscript)
Date: Sun, 19 Jun 2005 15:00:52 +0200
Subject: [Plfs-svn] doc ToDo.txt,1.3,NONE misc.txt,1.1,NONE
Message-ID: <200506191300.j5JD0qI10798@bat.berlios.de>

Update of /cvsroot/plfs/doc
In directory sheep:/tmp/cvs-serv10208

Removed Files:
	ToDo.txt misc.txt 
Log Message:
Actualizacion de la Bibliografia

--- ToDo.txt DELETED ---

--- misc.txt DELETED ---



From nobody at sheep.berlios.de  Sun Jun 19 15:03:41 2005
From: nobody at sheep.berlios.de (xscript)
Date: Sun, 19 Jun 2005 15:03:41 +0200
Subject: [Plfs-svn] doc 01.tex,1.2,1.3 02.tex,1.11,1.12 03.tex,1.11,1.12 04.tex,1.7,1.8
Message-ID: <200506191303.j5JD3fI10829@bat.berlios.de>

Update of /cvsroot/plfs/doc
In directory sheep:/tmp/cvs-serv10384

Modified Files:
	01.tex 02.tex 03.tex 04.tex 
Log Message:
Ultimas correcciones

Index: 01.tex
===================================================================
RCS file: /cvsroot/plfs/doc/01.tex,v
retrieving revision 1.2
retrieving revision 1.3
diff -C2 -d -r1.2 -r1.3
*** 01.tex	13 Jun 2005 19:57:46 -0000	1.2
--- 01.tex	19 Jun 2005 13:03:39 -0000	1.3
***************
*** 2,6 ****
  
  Tanto la documentaci?n como la implementaci?n se encuentran disponibles en la
! p?gina del proyecto \cite{PLFS}.
  
  Cabe notar que, como inicialmente nos centraremos en las operaciones de
--- 2,7 ----
  
  Tanto la documentaci?n como la implementaci?n se encuentran disponibles en la
! p?gina del proyecto \cite{PLFS}, que basa su idea inicial en la propuesta de
! \cite{ParPipelinedDist}.
  
  Cabe notar que, como inicialmente nos centraremos en las operaciones de

Index: 02.tex
===================================================================
RCS file: /cvsroot/plfs/doc/02.tex,v
retrieving revision 1.11
retrieving revision 1.12
diff -C2 -d -r1.11 -r1.12
*** 02.tex	17 Jun 2005 18:57:21 -0000	1.11
--- 02.tex	19 Jun 2005 13:03:39 -0000	1.12
***************
*** 32,36 ****
  		comunicaciones (en nuestro caso, ser?a \dpld).
  
! 	% TODO: intercambiar dpld <--> dplc ?!?! tendria mas sentido?
  	\item [\dpld:] Deployer Daemon\\
  		Componente encargado de mandar las ?rdenes de despliegue y
--- 32,36 ----
  		comunicaciones (en nuestro caso, ser?a \dpld).
  
! 	% TODO: intercambiar dpld <--> dplc ?!?! tendr?a mas sentido?
  	\item [\dpld:] Deployer Daemon\\
  		Componente encargado de mandar las ?rdenes de despliegue y
***************
*** 128,132 ****
  		funcione todo el sistema de seguridad (ver apartado
  		\ref{sect:segurity}).
! 	
  	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/backup}] :\\
  		Este fichero contiene una URI que indica el repositorio central
--- 128,132 ----
  		funcione todo el sistema de seguridad (ver apartado
  		\ref{sect:segurity}).
! 
  	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/backup}] :\\
  		Este fichero contiene una URI que indica el repositorio central
***************
*** 136,140 ****
  
  	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/key}] :\\
! 		Este fichero contiene la clave privada ssh correspondiente al
  		slice.
  
--- 136,140 ----
  
  	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/key}] :\\
! 		Este fichero contiene la clave privada SSH correspondiente al
  		slice.
  
***************
*** 175,179 ****
  		Este fichero contiene el n?mero de version del ?ltimo
  		despliegue (ver apartado \ref{sect:versioning}).
! 		
  	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/nodes/\textless{}
  		node\textgreater/unshared}] :\\
--- 175,179 ----
  		Este fichero contiene el n?mero de version del ?ltimo
  		despliegue (ver apartado \ref{sect:versioning}).
! 
  	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/nodes/\textless{}
  		node\textgreater/unshared}] :\\
***************
*** 338,342 ****
  		Al acceder al sistema de ficheros (\kplfs-\uplfs), hay que
  		preguntar a un servicio de PlanetLab si existe o no un slice
! 		dado (al hacer un move de un ejectuable a un directorio que es
  		un slice) o cu?les existen (al hacer un ls para ver los slices
  		disponibles).
--- 338,342 ----
  		Al acceder al sistema de ficheros (\kplfs-\uplfs), hay que
  		preguntar a un servicio de PlanetLab si existe o no un slice
! 		dado (al hacer un move de un ejecutable a un directorio que es
  		un slice) o cu?les existen (al hacer un ls para ver los slices
  		disponibles).
***************
*** 345,349 ****
  		PlanetLab de BD \textbf{m?s cercano} (por ahora solo hay uno, y
  		es \textit{PlanetLab Central}) y preguntarle:
! 		
  		\begin{itemize}
  			\item si existe un slice concreto
--- 345,349 ----
  		PlanetLab de BD \textbf{m?s cercano} (por ahora solo hay uno, y
  		es \textit{PlanetLab Central}) y preguntarle:
! 
  		\begin{itemize}
  			\item si existe un slice concreto
***************
*** 367,373 ****
  Para ello, se podr?a utilizar un servicio DNS al estilo Akamai, tal como
  utiliza google, o algo como CoDNS \cite{CoDNS}, subproyecto de Codeen
! \cite{Codeen}, que nos dan la IP del nodo m?s cercano que proporciona un
! servicio conreto. Cabe notar, adem?s, que ?ste es un servicio que PlanetLab
! planea dar a todos sus nodos y que tiene como tarea pendiente implementar.
  
  Como el servicio de DNS se utiliza siempre de forma transparente, aunque d? un
--- 367,374 ----
  Para ello, se podr?a utilizar un servicio DNS al estilo Akamai, tal como
  utiliza google, o algo como CoDNS \cite{CoDNS}, subproyecto de Codeen
! \cite{Codeen}, o PowerDNS \cite{PowerDNS} con su backend geogr?fico, que nos dan
! la IP del nodo m?s cercano que proporciona un servicio conreto. Cabe notar,
! adem?s, que ?ste es un servicio que PlanetLab planea dar a todos sus nodos y que
! tiene como tarea pendiente implementar.
  
  Como el servicio de DNS se utiliza siempre de forma transparente, aunque d? un
***************
*** 380,386 ****
  \label{sect:deployment}
  
  Despu?s de pensar diferentes posibilidades sobre como el usuario podr?a llevar
! a cabo realmente el despliegue mediante el sistema de ficheros, hemos llegado
! a la conclusi?n de que el usuario deber? realizar los siguientes pasos:
  
  \begin{enumerate}
--- 381,391 ----
  \label{sect:deployment}
  
+ \nocite{Bamboo}
+ 
  Despu?s de pensar diferentes posibilidades sobre como el usuario podr?a llevar
! a cabo realmente el despliegue mediante el sistema de ficheros, y mirar
! otros proyectos dedicados al despliegue de aplicaciones \cite{PLSDT, RSTB,
! Stork}, u otras aplicaciones distribuidas para PlanetLab \cite{MON, NWS}, hemos
! llegado a la conclusi?n de que el usuario deber? realizar los siguientes pasos:
  
  \begin{enumerate}
***************
*** 396,400 ****
  Adem?s, y como es obvio dada la naturaleza del propio sistema de ficheros, se
  deber? cumplir la siguiente precondici?n antes de poder realizar el
! despliegue:\\
  
  \textit{Precondici?n}: El slice destino est? ya creado (ej: \texttt{mkdir}).\\
--- 401,406 ----
  Adem?s, y como es obvio dada la naturaleza del propio sistema de ficheros, se
  deber? cumplir la siguiente precondici?n antes de poder realizar el
! despliegue:
! \\
  
  \textit{Precondici?n}: El slice destino est? ya creado (ej: \texttt{mkdir}).\\
***************
*** 444,448 ****
  		El m?dulo \umcc hace llegar los paquetes a desplegar al \dplc.
  
! 	\item \dplc $\rightarrow$ slice destino (ssh) \\
  		Se despliega la aplicaci?n (se copian los ficheros), y una vez
  		hecho esto, se ejecuta el shell script (que es el ?ltimo
--- 450,454 ----
  		El m?dulo \umcc hace llegar los paquetes a desplegar al \dplc.
  
! 	\item \dplc $\rightarrow$ slice destino (SSH) \\
  		Se despliega la aplicaci?n (se copian los ficheros), y una vez
  		hecho esto, se ejecuta el shell script (que es el ?ltimo
***************
*** 455,464 ****
  
  Como se ve en el apartado \ref{sect:redeployment}, no es necesario controlar
! los casos en que algun despliegue no se lleve a cabo en alg?n nodo, ya que
  entre ellos mismos se encargar?n de mantener la coherencia del despliegue.
  
  El fichero \texttt{/plfs/slices/\textless{}slice\textgreater/status} contiene,
  para cada nodo del slice, si se ha podido contactar con ?l y, en caso
! afirmativo, en qu? estado de despligue se encuentra el nodo.
  
  Esta informaci?n se deriva a trav?s del contenido del acceso a los ficheros
--- 461,470 ----
  
  Como se ve en el apartado \ref{sect:redeployment}, no es necesario controlar
! los casos en que alg?n despliegue no se lleve a cabo en alg?n nodo, ya que
  entre ellos mismos se encargar?n de mantener la coherencia del despliegue.
  
  El fichero \texttt{/plfs/slices/\textless{}slice\textgreater/status} contiene,
  para cada nodo del slice, si se ha podido contactar con ?l y, en caso
! afirmativo, en qu? estado de despliegue se encuentra el nodo.
  
  Esta informaci?n se deriva a trav?s del contenido del acceso a los ficheros
***************
*** 573,579 ****
  		grupo, el nodo detecta que hay una versi?n m?s nueva de
  		despliegue.
! 	
  	\item[Modificaci?n de ficheros] : \\
! 		Cuando se produce una modificaci?n incontrolada (que no viene 
  		de uno de los \dpld registrados) de los ficheros de un nodo al
  		que se le ha realizado un despliegue, y estos son de tipo
--- 579,585 ----
  		grupo, el nodo detecta que hay una versi?n m?s nueva de
  		despliegue.
! 
  	\item[Modificaci?n de ficheros] : \\
! 		Cuando se produce una modificaci?n incontrolada (que no viene
  		de uno de los \dpld registrados) de los ficheros de un nodo al
  		que se le ha realizado un despliegue, y estos son de tipo
***************
*** 615,619 ****
  
  \begin{enumerate}
! 	\item En caso de no tener los datos de ssh, \dplc se los pedir? a alg?n
  		nodo de la lista para poder abrir las conexiones.
  
--- 621,625 ----
  
  \begin{enumerate}
! 	\item En caso de no tener los datos de SSH, \dplc se los pedir? a alg?n
  		nodo de la lista para poder abrir las conexiones.
  
***************
*** 650,654 ****
  r?pida que Internet (ya sea por configuraci?n manual o por detecci?n a trav?s
  de las interf?cies configuradas), para as? delegar un posible proceso de
! \textit{redeployment} de todo el grupo a un solo nodo, que luego haria el mismo
  \textit{redeployment} al resto de nodos del grupo (hasta pudiendo as?
  aprovecharse de las capacidades de multicast del nivel de enlace).
--- 656,660 ----
  r?pida que Internet (ya sea por configuraci?n manual o por detecci?n a trav?s
  de las interf?cies configuradas), para as? delegar un posible proceso de
! \textit{redeployment} de todo el grupo a un solo nodo, que luego har?a el mismo
  \textit{redeployment} al resto de nodos del grupo (hasta pudiendo as?
  aprovecharse de las capacidades de multicast del nivel de enlace).
***************
*** 660,665 ****
  
  Puesto que la autentificaci?n de todos los clientes es obligatoria en
! PlanetLab, cada vez que un cliente (\dpld en uestro caso) intente una operaci?n
! con un nodo (\dplc) con el que no est? autentificado, deber? primero relizar
  el proceso de autentificaci?n.
  
--- 666,671 ----
  
  Puesto que la autentificaci?n de todos los clientes es obligatoria en
! PlanetLab, cada vez que un cliente (\dpld en nuestro caso) intente una operaci?n
! con un nodo (\dplc) con el que no est? autentificado, deber? primero realizar
  el proceso de autentificaci?n.
  
***************
*** 676,680 ****
  
  Para controlar que los datos recibidos en los \dplc proceden de un \dpld
! previamente autentificado, utilizaremos una firma el?ctronica con
  $K_{priv_{d}}$ (puesto que los \dplc conocen $K_{pub_{d}}$).
  
--- 682,686 ----
  
  Para controlar que los datos recibidos en los \dplc proceden de un \dpld
! previamente autentificado, utilizaremos una firma electr?nica con
  $K_{priv_{d}}$ (puesto que los \dplc conocen $K_{pub_{d}}$).
  
***************
*** 682,686 ****
  
  \begin{enumerate}
! 	\item Se consulta con \pldb y se escoge un nodo cualquiera 
  		del slice con el que nos queremos comunicar.
  		\label{step:auth_get_node}
--- 688,692 ----
  
  \begin{enumerate}
! 	\item Se consulta con \pldb y se escoge un nodo cualquiera
  		del slice con el que nos queremos comunicar.
  		\label{step:auth_get_node}
***************
*** 688,692 ****
  	\item Se pide la clave p?blica del nodo seleccionado, $K_{pub_{c}}$, y
  		se le da la p?blica del \dpld, $K_{pub_{d}}$.\\
! 		Si el nodo no responde, se vuleve al paso
  		\ref{step:auth_get_node}).
  
--- 694,698 ----
  	\item Se pide la clave p?blica del nodo seleccionado, $K_{pub_{c}}$, y
  		se le da la p?blica del \dpld, $K_{pub_{d}}$.\\
! 		Si el nodo no responde, se vuelve al paso
  		\ref{step:auth_get_node}).
  
***************
*** 696,700 ****
  		(listado de ficheros, etc.).\\
  		Si la operaci?n falla, se aborta el proceso de registro.
! 		Si el nodo no responde, se vuleve al paso
  		\ref{step:auth_get_node}).
  
--- 702,706 ----
  		(listado de ficheros, etc.).\\
  		Si la operaci?n falla, se aborta el proceso de registro.
! 		Si el nodo no responde, se vuelve al paso
  		\ref{step:auth_get_node}).
  
***************
*** 711,715 ****
  		registro, de modo que \dpld no est? registrado en ese nodo.
  
! 	\item \dplc cierra la conexi?n ssh por falta de operaciones durante
  		cierto tiempo de \textit{timeout}, de modo que no se puede
  		hacer llegar las operaciones enviadas por \dpld.
--- 717,721 ----
  		registro, de modo que \dpld no est? registrado en ese nodo.
  
! 	\item \dplc cierra la conexi?n SSH por falta de operaciones durante
  		cierto tiempo de \textit{timeout}, de modo que no se puede
  		hacer llegar las operaciones enviadas por \dpld.
***************
*** 729,743 ****
  conexiones y si posteriormente se quisiera realizar una operaci?n en
  alguno de esos slices se deber? reiniciar de cero el proceso de registro
! (momento en el que se intentar? revalidar los fiheros que tubiera cacheados el
  \dpld).
  
! Ahora bien, \dplc no pierde los datos de conexi?n (clave y password ssh),
! puesto que como se explica en el apartado \ref{sect:redeployment}, ?stos son
  necesarios para mantener la coherencia del slice.
  
! En el caso en que se hayan cambiado la clave y/o el password ssh y que la
! conexi?n ssh est? cerrada, el \dpld recibir?a algunas operaciones de aviso (ver
  apartado \ref{sect:dpld}) procedentes de los \dplc destinatarios con la conexi?n
! ssh cerrada (que ser?n una minor?a debido a la pol?tica de revalidaci?n de
  registros comentada anteriormente).
  
--- 735,749 ----
  conexiones y si posteriormente se quisiera realizar una operaci?n en
  alguno de esos slices se deber? reiniciar de cero el proceso de registro
! (momento en el que se intentar? revalidar los ficheros que tuviera cacheados el
  \dpld).
  
! Ahora bien, \dplc no pierde los datos de conexi?n (clave y password SSH),
! puesto que como se explica en el apartado \ref{sect:redeployment}, estos son
  necesarios para mantener la coherencia del slice.
  
! En el caso en que se hayan cambiado la clave y/o el password SSH y que la
! conexi?n SSH est? cerrada, el \dpld recibir?a algunas operaciones de aviso (ver
  apartado \ref{sect:dpld}) procedentes de los \dplc destinatarios con la conexi?n
! SSH cerrada (que ser?n una minor?a debido a la pol?tica de revalidaci?n de
  registros comentada anteriormente).
  

Index: 03.tex
===================================================================
RCS file: /cvsroot/plfs/doc/03.tex,v
retrieving revision 1.11
retrieving revision 1.12
diff -C2 -d -r1.11 -r1.12
*** 03.tex	17 Jun 2005 18:57:21 -0000	1.11
--- 03.tex	19 Jun 2005 13:03:39 -0000	1.12
***************
*** 1,6 ****
  \chapter{Dise?o de los componentes}
  
- %TODO: se?alar qu? m?dulos encriptan/desencriptan datos
- 
  \textbf{\textit{En este cap?tulo definimos las principales funcionalidades que
  deben implementar cada uno de los componentes.}}
--- 1,4 ----
***************
*** 10,23 ****
  \section{\kplfs}
  
! Este m?dulo implementa la interfaz del VFS, y las operaciones que permite las
! hemos descrito en el cap?tulo anterior en el apartado \ref{sect:plfsoperations}.
  
! Las operaciones que ofrece, adem?s de las del VFS, son:
  
  \begin{description}
! 	\item[\texttt{invalidate (slice\_name, type, path)}] :\\
  		Invalida el fichero determinado por \texttt{path} del slice
! 		\texttt{slice\_name} del tipo \texttt{type} (\textit{shared} o
! 		\textit{unshared}).
  \end{description}
  
--- 8,26 ----
  \section{\kplfs}
  
! Este m?dulo implementa la interfaz del VFS (\textit{Virtual File System}, ver
! \cite{libfs, lk, lk24i, lkapi, lkmpg, ovfs, tlk}) de Linux, y las operaciones
! que permite las hemos descrito en el cap?tulo anterior en el apartado
! \ref{sect:plfsoperations}, emulando el comportamiento de sistemas de ficheros
! como Intermezzo \cite{Intermezzo}, CODA o NFS, que tienen un controlador en el
! n?cleo y otro en espacio de usuario.
  
! Las operaciones que ofrece, adem?s de las del VFS (informar a \kplfs del
! resultado de las operaciones del VFS que se comunican a \plfs), son:
  
  \begin{description}
! 	\item[\texttt{invalidate (slice\_name, node\_name, type, path)}] :\\
  		Invalida el fichero determinado por \texttt{path} del slice
! 		\texttt{slice\_name} del nodo \texttt{node\_name} del tipo
! 		\texttt{type} (\textit{shared} o \textit{unshared}).
  \end{description}
  
***************
*** 40,49 ****
  
  \begin{description}
! 	\item[\texttt{execute (component, operation, \ldots)}] :\\
  		Llama a la funci?n \texttt{operation} del componente
  		\texttt{component} con los par?metros extra que se indiquen (en caso
! 		de indicar alguno).
  		\\
  		Los posibles componentes actualmente son \dpld o \pldb.
  \end{description}
  
--- 43,60 ----
  
  \begin{description}
! 	\item[\texttt{execute (component, id, operation, \ldots)}] :\\
  		Llama a la funci?n \texttt{operation} del componente
  		\texttt{component} con los par?metros extra que se indiquen (en caso
! 		de indicar alguno), siendo \texttt{id} el identificador de la
! 		operaci?n para su posible retorno (en Linux sirve perfectamente
! 		el PID del proceso que inicia la petici?n, ya que en todo
! 		momento habr? una sola petici?n por cada thread).
  		\\
  		Los posibles componentes actualmente son \dpld o \pldb.
+ 
+ 	\item[\texttt{invalidate (slice\_name, node\_name, type, path)}] :\\
+ 		Invalida el fichero determinado por \texttt{path} del slice
+ 		\texttt{slice\_name} del nodo \texttt{node\_name} del tipo
+ 		\texttt{type} (\textit{shared} o \textit{unshared}).
  \end{description}
  
***************
*** 55,60 ****
  
  \begin{description}
! 	\item[\texttt{deployToSlice (slice\_name, path\_from, path\_to)}]
! 		:\\
  		Realiza la comunicaci?n con los componentes \dplc para desplegar
  		un fichero \textit{shared} (\texttt{path\_from}) al slice
--- 66,70 ----
  
  \begin{description}
! 	\item[\texttt{deployToSlice (slice\_name, path\_from, path\_to)}] :\\
  		Realiza la comunicaci?n con los componentes \dplc para desplegar
  		un fichero \textit{shared} (\texttt{path\_from}) al slice
***************
*** 121,130 ****
  
  	\item[\texttt{registerNeeded (slice\_name, node\_name)}] :\\
! 		Petici?n de necesidad de registro.
! 	
  	\item[\texttt{addNodeToSlice (slice\_name, node\_name)}] :\\
  		Informa de la adici?n de un nodo a un slice.
  
! 	\item[\texttt{removeNodeToSlice (slice\_name, node\_name)}] :\\
  		Informa de la eliminaci?n de un nodo a un slice.
  
--- 131,141 ----
  
  	\item[\texttt{registerNeeded (slice\_name, node\_name)}] :\\
! 		Petici?n de necesidad de registro. Si \texttt{node\_name} es
! 		nulo, se hace un registro a todo el slice \texttt{slice\_name}.
! 
  	\item[\texttt{addNodeToSlice (slice\_name, node\_name)}] :\\
  		Informa de la adici?n de un nodo a un slice.
  
! 	\item[\texttt{removeNodeFromSlice (slice\_name, node\_name)}] :\\
  		Informa de la eliminaci?n de un nodo a un slice.
  
***************
*** 136,140 ****
  En todas las anteriores operaciones, excepto las cuatro ?ltimas, se puede dar el
  caso de que el/los \dplc destinatarios de las operaciones no hayan pasado por
! un proceso de registro desde el \dpld or?gen, por lo que se puede recibir un
  aviso de necesidad de registro, al cual reaccionar? con un registro
  (\textit{unicast} o \textit{multicast} seg?n el n?mero de destinatarios de la
--- 147,151 ----
  En todas las anteriores operaciones, excepto las cuatro ?ltimas, se puede dar el
  caso de que el/los \dplc destinatarios de las operaciones no hayan pasado por
! un proceso de registro desde el \dpld origen, por lo que se puede recibir un
  aviso de necesidad de registro, al cual reaccionar? con un registro
  (\textit{unicast} o \textit{multicast} seg?n el n?mero de destinatarios de la
***************
*** 169,175 ****
  		Permite obtener el numero de nodos que hay en un slice
  
- 	\item[\texttt{getNodeSlices (node\_name)}] :\\
- 		Permite obtener la lista de los slices de un nodo
- 
  	\item[\texttt{getSliceNode (slice\_name, node\_name)}] :\\
  		Permite obtener informaci?n de un nodo de un slice
--- 180,183 ----
***************
*** 182,185 ****
--- 190,196 ----
  		(aprovech?ndose, si se da el caso, de un sistema DNS con
  		soporte para ``localidad'').
+ 
+ 	\item[\texttt{getNodeSlices (node\_name)}] :\\
+ 		Permite obtener la lista de los slices de un nodo
  \end{description}
  
***************
*** 222,229 ****
  		implementaci?n concreta del software multicast que usemos.
  
! 	\item[\texttt{setOptions (group\_id, options[])}] :\\
  		Permite obtener las posibles opciones que permita la
  		implementaci?n concreta del software multicast que usemos.
! 	
  	\item[\texttt{sendData (group\_id, data)}] :\\
  		Permite enviar datos a un grupo.
--- 233,240 ----
  		implementaci?n concreta del software multicast que usemos.
  
! 	\item[\texttt{getOptions (group\_id, options[])}] :\\
  		Permite obtener las posibles opciones que permita la
  		implementaci?n concreta del software multicast que usemos.
! 
  	\item[\texttt{sendData (group\_id, data)}] :\\
  		Permite enviar datos a un grupo.
***************
*** 257,260 ****
--- 268,273 ----
  \section{\umcr}
  
+ \nocite{RON}
+ 
  Estos nodos se encargan de hacer la transmisi?n multicast, con tal de hacer
  llegar los datos a los \umcc del grupo de destino.
***************
*** 268,275 ****
  
  En cuanto a las implementaciones, hemos encontrado varias \cite{Araneola, ESM,
! LMDD, LSAM}, pero consideramos que Araneola \cite{Araneola} se ajusta m?s a
! nuestras necesidades que las otras implementaciones por el hecho de haber sido
! dise?ada para entornos din?micos y con la fiabilidad en mente, adem?s de ser un
! sistema de comunicaci?n de M a N.
  
  
--- 281,288 ----
  
  En cuanto a las implementaciones, hemos encontrado varias \cite{Araneola, ESM,
! LMDD, LSAM, nemo, nemo-res-p2p-mcast, stealth}, pero consideramos que Araneola
! \cite{Araneola} se ajusta m?s a nuestras necesidades que las otras
! implementaciones por el hecho de haber sido dise?ada para entornos din?micos y
! con la fiabilidad en mente, adem?s de ser un sistema de comunicaci?n de M a N.
  
  
***************
*** 282,290 ****
  consideraremos administrado correctamente (por lo que no habr? configuraciones
  ni modificaciones maliciosas) y al recibir los datos de \umcc acceder? a s?
! mismo por ssh con tal de poner los ficheros al slice destino y seguidamente
  ejecutar el shell script asociado, en caso de estar presente.
  
! Para realizar esta conexi?n por ssh, el nodo del slice \dplc necesita los datos
! de clave y password ssh para que \dplc pueda hacer una conexi?n a la propia
  m?quina y acceder a la m?quina virtual asociada al slice de destino.
  
--- 295,303 ----
  consideraremos administrado correctamente (por lo que no habr? configuraciones
  ni modificaciones maliciosas) y al recibir los datos de \umcc acceder? a s?
! mismo por SSH con tal de poner los ficheros al slice destino y seguidamente
  ejecutar el shell script asociado, en caso de estar presente.
  
! Para realizar esta conexi?n por SSH, el nodo del slice \dplc necesita los datos
! de clave y password SSH para que \dplc pueda hacer una conexi?n a la propia
  m?quina y acceder a la m?quina virtual asociada al slice de destino.
  
***************
*** 318,322 ****
  		Hace una petici?n de un objeto del sistema de ficheros
  		del slice \texttt{slice\_name}.
! 	
  	\item[\texttt{deleteFile (slice\_name, path, new\_version)}] :\\
  		Hace la petici?n de eliminaci?n de un objeto del sistema de
--- 331,335 ----
  		Hace una petici?n de un objeto del sistema de ficheros
  		del slice \texttt{slice\_name}.
! 
  	\item[\texttt{deleteFile (slice\_name, path, new\_version)}] :\\
  		Hace la petici?n de eliminaci?n de un objeto del sistema de
***************
*** 330,336 ****
  		\texttt{slice\_name}.
  
! 	\item[\texttt{getKey (slice\_name, $K_{pub_{d}}$)}] :\\
! 		Informa de la clave p?blica remota y devuelve la propia clave
! 		p?blica.
  
  	\item[\texttt{getSSH (slice\_name)}] :\\
--- 343,349 ----
  		\texttt{slice\_name}.
  
! 	\item[\texttt{getKey ($K_{pub}$)}] :\\
! 		Informa de la propia clave p?blica y devuelve la clave
! 		p?blica remota.
  
  	\item[\texttt{getSSH (slice\_name)}] :\\
***************
*** 346,362 ****
  
  	\item[\texttt{needDeploy (slice\_name, node\_name, path)}] :\\
! 		Informa de la necesidad de \texttt{node\_name} de recibir un
  		deploy de \texttt{path} del slice \texttt{slice\_name}.
  
! 	\item[\texttt{registerWithResponse (slice\_name, node\_name, $K_{SSH}$,
! 		$P_{SSH}$, $K_{pub}$)}] :\\
! 		Permite a \texttt{node\_name} registrarse en \dplc como
! 		integrante del slice y devuelve un resultado para informar si
! 		se ha podido realizar la conexi?n SSH.
! 
! 	\item[\texttt{register (slice\_name, node\_name, $K_{SSH}$, $P_{SSH}$,
  		$K_{pub}$)}] :\\
  		Permite a \texttt{node\_name} registrarse en \dplc como
! 		integrante del slice.
  
  	\item[\texttt{joined (slice\_name)}] :\\
--- 359,375 ----
  
  	\item[\texttt{needDeploy (slice\_name, node\_name, path)}] :\\
! 		Informa de la necesidad a \texttt{node\_name} de recibir un
  		deploy de \texttt{path} del slice \texttt{slice\_name}.
  
! 	\item[\texttt{registerWithResponse (slice\_name, $K_{SSH}$, $P_{SSH}$,
  		$K_{pub}$)}] :\\
+ 		Permite registrarse en un \dplc como integrante del slice emisor
+ 		y devuelve un resultado para informar si se ha podido realizar
+ 		la conexi?n SSH.
+ 
+ 	\item[\texttt{register (slice\_name, $K_{SSH}$, $P_{SSH}$, $K_{pub}$)}]
+ 		:\\
  		Permite a \texttt{node\_name} registrarse en \dplc como
! 		integrante del slice emisor para realizar la conexi?n SSH.
  
  	\item[\texttt{joined (slice\_name)}] :\\
***************
*** 393,397 ****
  
  Para ello debe haber una instancia corriendo en cada m?quina virtual que
! corresponda a un slice que funciona a traves de \plfs, siendo arrancado por el
  correspondiente \dplc en el momento en que hay alg?n \dpld registrado.
  
--- 406,410 ----
  
  Para ello debe haber una instancia corriendo en cada m?quina virtual que
! corresponda a un slice que funciona a trav?s de \plfs, siendo arrancado por el
  correspondiente \dplc en el momento en que hay alg?n \dpld registrado.
  

Index: 04.tex
===================================================================
RCS file: /cvsroot/plfs/doc/04.tex,v
retrieving revision 1.7
retrieving revision 1.8
diff -C2 -d -r1.7 -r1.8
*** 04.tex	17 Jun 2005 18:57:21 -0000	1.7
--- 04.tex	19 Jun 2005 13:03:39 -0000	1.8
***************
*** 1,10 ****
  \chapter{Comunicaci?n entre los componentes}
  
- %TODO: se?alar qu? comunicaciones van encriptadas/desencriptadas
- 
  \textbf{\textit{En este cap?tulo comentamos diferentes implementaciones de la
  comunicaci?n entre componentes del sistema. Como veremos, nos centraremos en
  los tipos de mensajes en cada tipo de comunicaci?n y la seguridad que debemos
  implementar en cada una, para asegurar autenticidad y confidencialidad.}}
  
  Las comunicaciones entre componentes de un mismo nodo funcionan a modo de
--- 1,9 ----
  \chapter{Comunicaci?n entre los componentes}
  
  \textbf{\textit{En este cap?tulo comentamos diferentes implementaciones de la
  comunicaci?n entre componentes del sistema. Como veremos, nos centraremos en
  los tipos de mensajes en cada tipo de comunicaci?n y la seguridad que debemos
  implementar en cada una, para asegurar autenticidad y confidencialidad.}}
+ \\
  
  Las comunicaciones entre componentes de un mismo nodo funcionan a modo de
***************
*** 37,49 ****
  \begin{description}
  	\item [Dispositivo:]
! 		Aprovecha las propias caracter?sticas de un dispositivo de sistema,
! 		ya que permite hacer esperas no activas (es decir bloqueantes), a
! 		trav?s de \textit{poll}, \textit{read}, \textit{select} de un
! 		dispositivo que implementa \kplfs.
  
  	\item [Compartici?n de memoria:]
! 		El problema de esta soluci?n es que requiere una espera activa por
! 		parte de \uplfs, que debe ir comprobando la zona de memoria compartida
! 		para ver si hay alg?n nuevo evento procedente de \kplfs.
  \end{description}
  
--- 36,49 ----
  \begin{description}
  	\item [Dispositivo:]
! 		Aprovecha las propias caracter?sticas de un dispositivo de
! 		sistema, ya que permite hacer esperas no activas (es decir
! 		bloqueantes), a trav?s de \textit{poll}, \textit{read},
! 		\textit{select} de un dispositivo que implementa \kplfs.
  
  	\item [Compartici?n de memoria:]
! 		El problema de esta soluci?n es que requiere una espera activa
! 		por parte de \uplfs, que debe ir comprobando la zona de memoria
! 		compartida para ver si hay alg?n nuevo evento procedente de
! 		\kplfs.
  \end{description}
  
***************
*** 55,59 ****
  deber? informar dichos eventos al m?dulo a nivel usuario \uplfs (el cual deber?
  realizar las operaciones pertinentes, comunic?ndose con los diferentes
! componentes disponibles).
  
  En ?ste caso, aunque los componentes est?n en el mismo nodo, deben enviarse
--- 55,60 ----
  deber? informar dichos eventos al m?dulo a nivel usuario \uplfs (el cual deber?
  realizar las operaciones pertinentes, comunic?ndose con los diferentes
! componentes disponibles y respondiendo a cada operaci?n con su identificador
! \texttt{id}).
  
  En ?ste caso, aunque los componentes est?n en el mismo nodo, deben enviarse
***************
*** 61,70 ****
  formato que se ha comentado al inicio del cap?tulo, pero sin el par?metro de
  \texttt{Remitente}.
  
  
  
! \section{\uplfs/\dpld $\rightarrow$ \pldb}
  
  Llamada interna de funciones.
  
  
--- 62,101 ----
  formato que se ha comentado al inicio del cap?tulo, pero sin el par?metro de
  \texttt{Remitente}.
+ \\
  
+ \kplfs $\rightarrow$ \uplfs:
  
+ \begin{itemize}
+ 	\item \texttt{execute (component, id, operation, \ldots)}
+ \end{itemize}
  
! \kplfs $\leftarrow$ \uplfs:
! 
! \begin{itemize}
! 	\item Operaciones para el retorno del resultado de las peticiones del
! 		VFS a \uplfs (ver \cite{lkapi}).
! 	\item \texttt{invalidate (slice\_name, node\_name, type, path)}
! \end{itemize}
! 
! 
! 
! \section{\uplfs/\dpld/\dplc $\rightarrow$ \pldb}
  
  Llamada interna de funciones.
+ \\
+ 
+ \uplfs/\dpld/\dplc $\rightarrow$ \pldb:
+ 
+ \begin{itemize}
+ 	\item \texttt{getSlices ()}
+ 	\item \texttt{getNodes ()}
+ 	\item \texttt{getSliceNodes (slice\_name)}
+ 	\item \texttt{getSliceNodesAny (slice\_name, num)}
+ 	\item \texttt{getSliceNodesNumber (slice\_name)}
+ 	\item \texttt{getSliceNode (slice\_name, node\_name)}
+ 	\item \texttt{getSliceNodeAny (slice\_name)}
+ 	\item \texttt{getSliceNodeNearest (slice\_name)}
+ 	\item \texttt{getNodeSlices (node\_name)}
+ \end{itemize}
  
  
***************
*** 73,103 ****
  
  Llamada interna de funciones.
  
  
  
! \section{\dpld $\rightarrow$ \dplc}
! 
! Todas las comunicaciones van cifradas con $K_{pub_{c}}$ (para asegurar que s?lo
! los \dplc puedan leer los contenidos) y firmadas con $K_{priv_{d}}$ (para poder
! as? comprovar su autenticidad).
  
! Luego, en el destino, se comprueba la firma, se descifran los datos y se
! procede a realizar la operaci?n.
  
  
  
! \section{\dplc $\rightarrow$ \dpld}
  \label{sect:warnings}
  
! Todas las comunicaciones van cifradas con $K_{pub_{d}}$ (para asegurar que s?lo
! los \dplc puedan leer los contenidos) y firmadas con $K_{priv_{c}}$ (para poder
! as? comprovar su autenticidad).
  
  Luego, en el destino, se comprueba la firma, se descifran los datos y se
! procede a realizar la operaci?n.
  
  
  
! \section{\dpld/\dplc $\rightarrow$ \umcc}
  
  Cuando tanto \dpld como \dplc quieren comunicarse con un grupo (\dpld se
--- 104,191 ----
  
  Llamada interna de funciones.
+ \\
  
+ \uplfs $\rightarrow$ \dpld:
  
+ \begin{itemize}
+ 	\item \texttt{deployToSlice (slice\_name, path\_from, path\_to)}
+ 	\item \texttt{deployToNode (slice\_name, node\_name, path\_from,
+ 		path\_to)}
+ 	\item \texttt{getInfoShared (slice\_name, path)}
+ 	\item \texttt{getInfoUnshared (slice\_name, node\_name, path)}
+ 	\item \texttt{getFileShared (slice\_name, path)}
+ 	\item \texttt{getFileUnshared (slice\_name, node\_name, path)}
+ 	\item \texttt{deleteFileShared (slice\_name, path)}
+ 	\item \texttt{deleteFileUnshared (slice\_name, node\_name, path)}
+ 	\item \texttt{getSliceNodeVersion (slice\_name, node\_name)}
+ 	\item \texttt{registerNeeded (slice\_name, node\_name)}
+ \end{itemize}
  
! \uplfs $\leftarrow$ \dpld:
  
! \begin{itemize}
! 	\item \texttt{invalidate (slice\_name, node\_name, type, path)}
! \end{itemize}
  
  
  
! \section{\dpld $\longleftrightarrow$ \dplc}
  \label{sect:warnings}
  
! Teniendo en cuanta que cada componente tiene su propio par de llaves
! (un par $K_{pub_{d}}$ y $K_{priv_{d}}$ para cada \dpld y un mismo par
! $K_{pub_{c}}$ y $K_{priv_{c}}$ para todos los \dplc), todas las comunicaciones
! van cifradas con la clave p?blica del emisor (para asegurar que s?lo
! los receptores puedan leer los contenidos) y firmadas con la clave
! privada del emisor (para poder as? comprobar su autenticidad), excepto
! \texttt{getKey}, que s?lo va firmada.
  
  Luego, en el destino, se comprueba la firma, se descifran los datos y se
! procede a realizar la operaci?n pertinente.
! \\
! 
! \dpld $\rightarrow$ \dplc:
! 
! \begin{itemize}
! 	\item \texttt{deployToSlice (slice\_name, path\_to, file, type,
! 		new\_version)}
! 	\item \texttt{getInfo (slice\_name, path)}
! 	\item \texttt{getFile (slice\_name, path)}
! 	\item \texttt{deleteFile (slice\_name, path, new\_version)}
! 	\item \texttt{getVersion (slice\_name)}
! 	\item \texttt{getKey ($K_{pub_{d}}$)}
! 	\item \texttt{registerWithResponse (slice\_name, $K_{SSH}$, $P_{SSH}$,
! 		$K_{pub}$)}
! 	\item \texttt{register (slice\_name, $K_{SSH}$, $P_{SSH}$, $K_{pub}$)}
! \end{itemize}
  
+ \dpld $\leftarrow$ \dplc:
  
+ \begin{itemize}
+ 	\item \texttt{registerNeeded (slice\_name, node\_name)}
+ 	\item \texttt{invalidate (slice\_name, node\_name, type, path)}
+ \end{itemize}
  
! 
! 
! \section{\dplc $\rightarrow$ \dplc}
! 
! Como la red de \dplc intenta llegar sola a la coherencia de versiones, necesita
! ofrecer m?todos para ponerse en contacto e intercambiar datos, cifrando y
! firmando con $K_{priv_{c}}$.
! 
! \begin{itemize}
! 	\item \texttt{deployToSlice (slice\_name, path\_to, file, type,
! 		new\_version)}
! 	\item \texttt{needDeploy (slice\_name, node\_name, path)}
! 	\item \texttt{ping (slice\_name, node\_name, version)}
! 	\item \texttt{getSSH (slice\_name)}
! \end{itemize}
! 
! 
! 
! \section{\dpld/\dplc $\longleftrightarrow$ \umcc}
! 
! Llamada interna de funciones.
  
  Cuando tanto \dpld como \dplc quieren comunicarse con un grupo (\dpld se
***************
*** 105,108 ****
  \dplc se comunica con el grupo de emisores para mandarles avisos), lo hacen a
  trav?s de la red multicast, mediante el componente de entrada a ella que es
! \umcc, de forma que mandan los datos como si fueran directamente una de las dos
! comunicaciones anteriores y se transmiten transparentemente por la red \umc.
--- 193,252 ----
  \dplc se comunica con el grupo de emisores para mandarles avisos), lo hacen a
  trav?s de la red multicast, mediante el componente de entrada a ella que es
! \umcc, de forma que mandan los datos como si fueran directamente como la
! comunicaci?n anterior y se transmiten transparentemente por la red \umc.
! \\
! 
! \dpld/\dplc $\rightarrow$ \umcc:
! 
! \begin{itemize}
! 	\item \texttt{group\_id getGroup (group\_name)}
! 	\item \texttt{setOptions (group\_id, options[])}
! 	\item \texttt{getOptions (group\_id, options[])}
! 	\item \texttt{sendData (group\_id, data)}
! 	\item \texttt{recieveData (group\_id, data)}
! 	\item \texttt{joinSender (group\_id, node\_name)}
! 	\item \texttt{deleteSender (group\_id, node\_name)}
! 	\item \texttt{joinReceiver (group\_id, node\_name)}
! 	\item \texttt{deleteReceiver (group\_id, node\_name)}
! \end{itemize}
! 
! Con tal de saber \dpld cu?l es el estado de los slices, como la red multicast
! subyacente puede informar de la adici?n y eliminaci?n de receptores, es el
! propio \umcc quien se encarga de avisar a \dpld (en lugar de \dplc, evitando as?
! ocupar la red innecesariamente).
! \\
! 
! \dpld $\leftarrow$ \umcc:
! 
! \begin{itemize}
! 	\item \texttt{addNodeToSlice (slice\_name, node\_name)}
! 	\item \texttt{removeNodeFromSlice (slice\_name, node\_name)}
! \end{itemize}
! 
! \dplc $\leftarrow$ \umcc son:
! 
! \begin{itemize}
! 	\item \texttt{joined (slice\_name)}
! 	\item \texttt{deleted (slice\_name)}
! \end{itemize}
! 
! 
! 
! \section{\dplc $\longleftrightarrow$ \famon}
! 
! Llamada interna de funciones.
! \\
! 
! \dplc $\rightarrow$ \famon:
! 
! \begin{itemize}
! 	\item \texttt{watch (path)}
! 	\item \texttt{unwatch (path)}
! 	\item \texttt{stop ()}
! \end{itemize}
! 
! \dplc $\leftarrow$ \famon:
! 
! \begin{itemize}
! 	\item \texttt{invalidate (slice\_name, path)}
! \end{itemize}



From nobody at sheep.berlios.de  Sun Jun 19 17:01:07 2005
From: nobody at sheep.berlios.de (guruburux)
Date: Sun, 19 Jun 2005 17:01:07 +0200
Subject: [Plfs-svn] doc 02.tex,1.12,1.13 03.tex,1.12,1.13
Message-ID: <200506191501.j5JF17I13726@bat.berlios.de>

Update of /cvsroot/plfs/doc
In directory sheep:/tmp/cvs-serv18346/doc

Modified Files:
	02.tex 03.tex 
Log Message:
Versio Final Revisada.
Correccio d'algunas referencies invalides i text fora de marges.


Index: 02.tex
===================================================================
RCS file: /cvsroot/plfs/doc/02.tex,v
retrieving revision 1.12
retrieving revision 1.13
diff -C2 -d -r1.12 -r1.13
*** 02.tex	19 Jun 2005 13:03:39 -0000	1.12
--- 02.tex	19 Jun 2005 15:01:05 -0000	1.13
***************
*** 122,131 ****
  		Este fichero contiene la clave p?blica del \dpld para que
  		funcione todo el sistema de seguridad (ver apartado
! 		\ref{sect:segurity}).
  
  	\item[\texttt{/plfs/privatekey}] :\\
  		Este fichero contiene la clave privada del \dpld para que
  		funcione todo el sistema de seguridad (ver apartado
! 		\ref{sect:segurity}).
  
  	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/backup}] :\\
--- 122,131 ----
  		Este fichero contiene la clave p?blica del \dpld para que
  		funcione todo el sistema de seguridad (ver apartado
! 		\ref{sect:security}).
  
  	\item[\texttt{/plfs/privatekey}] :\\
  		Este fichero contiene la clave privada del \dpld para que
  		funcione todo el sistema de seguridad (ver apartado
! 		\ref{sect:security}).
  
  	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/backup}] :\\
***************
*** 245,249 ****
  	\item [\textit{edici?n}] :\\
  		La edici?n de un fichero provocar? el redespliegue del mismo
! 		una vez sea cerrado (ver apartado \label{sect:redeployment}).
  
  	\item [\textit{lectura}] :\\
--- 245,249 ----
  	\item [\textit{edici?n}] :\\
  		La edici?n de un fichero provocar? el redespliegue del mismo
! 		una vez sea cerrado (ver apartado \ref{sect:redeployment}).
  
  	\item [\textit{lectura}] :\\
***************
*** 469,473 ****
  
  Esta informaci?n se deriva a trav?s del contenido del acceso a los ficheros
! \texttt{/plfs/slices/\textless{}slice\textgreater/nodes/\textless{}
  node\textgreater/version}, dejando comprobar en qu? estado se encuentra el
  despliegue al comparar la versi?n de los nodos con la actual.
--- 469,473 ----
  
  Esta informaci?n se deriva a trav?s del contenido del acceso a los ficheros
! \texttt{/plfs/slices/ \textless{}slice\textgreater/nodes/\textless{}
  node\textgreater/version}, dejando comprobar en qu? estado se encuentra el
  despliegue al comparar la versi?n de los nodos con la actual.

Index: 03.tex
===================================================================
RCS file: /cvsroot/plfs/doc/03.tex,v
retrieving revision 1.12
retrieving revision 1.13
diff -C2 -d -r1.12 -r1.13
*** 03.tex	19 Jun 2005 13:03:39 -0000	1.12
--- 03.tex	19 Jun 2005 15:01:05 -0000	1.13
***************
*** 62,65 ****
--- 62,66 ----
  
  \section{\dpld}
+ \label{sect:dpld}
  
  Las operaciones que \dpld ofrece son las siguientes:
***************
*** 164,188 ****
  	\item[\texttt{getSlices ()}] :\\
  		Permite obtener la lista de slices que se han dado de alta en
! 		PlanetLab
  
  	\item[\texttt{getNodes ()}] :\\
  		Permite obtener la lista de nodos que se han dado de alta en
! 		PlanetLab
  
  	\item[\texttt{getSliceNodes (slice\_name)}] :\\
! 		Permite obtener la lista de los nodos que hay en un slice
  
  	\item[\texttt{getSliceNodesAny (slice\_name, num)}] :\\
  		Permite obtener una lista de \texttt{num} nodos cualesquiera de
! 		un slice
  
  	\item[\texttt{getSliceNodesNumber (slice\_name)}] :\\
! 		Permite obtener el numero de nodos que hay en un slice
  
  	\item[\texttt{getSliceNode (slice\_name, node\_name)}] :\\
! 		Permite obtener informaci?n de un nodo de un slice
  
  	\item[\texttt{getSliceNodeAny (slice\_name)}] :\\
! 		Permite obtener un nodo cualquiera de un slice
  
  	\item[\texttt{getSliceNodeNearest (slice\_name)}] :\\
--- 165,189 ----
  	\item[\texttt{getSlices ()}] :\\
  		Permite obtener la lista de slices que se han dado de alta en
! 		PlanetLab.
  
  	\item[\texttt{getNodes ()}] :\\
  		Permite obtener la lista de nodos que se han dado de alta en
! 		PlanetLab.
  
  	\item[\texttt{getSliceNodes (slice\_name)}] :\\
! 		Permite obtener la lista de los nodos que hay en un slice.
  
  	\item[\texttt{getSliceNodesAny (slice\_name, num)}] :\\
  		Permite obtener una lista de \texttt{num} nodos cualesquiera de
! 		un slice.
  
  	\item[\texttt{getSliceNodesNumber (slice\_name)}] :\\
! 		Permite obtener el numero de nodos que hay en un slice.
  
  	\item[\texttt{getSliceNode (slice\_name, node\_name)}] :\\
! 		Permite obtener informaci?n de un nodo de un slice.
  
  	\item[\texttt{getSliceNodeAny (slice\_name)}] :\\
! 		Permite obtener un nodo cualquiera de un slice.
  
  	\item[\texttt{getSliceNodeNearest (slice\_name)}] :\\
***************
*** 192,196 ****
  
  	\item[\texttt{getNodeSlices (node\_name)}] :\\
! 		Permite obtener la lista de los slices de un nodo
  \end{description}
  
--- 193,197 ----
  
  	\item[\texttt{getNodeSlices (node\_name)}] :\\
! 		Permite obtener la lista de los slices de un nodo.
  \end{description}
  
***************
*** 278,282 ****
  Como todos los accesos a la red multicast se hacen a trav?s de \umcc, se puede
  f?cilmente utilizar una implementaci?n ya existente de una red overlay
! multicast en modo usuario
  
  En cuanto a las implementaciones, hemos encontrado varias \cite{Araneola, ESM,
--- 279,283 ----
  Como todos los accesos a la red multicast se hacen a trav?s de \umcc, se puede
  f?cilmente utilizar una implementaci?n ya existente de una red overlay
! multicast en modo usuario.
  
  En cuanto a las implementaciones, hemos encontrado varias \cite{Araneola, ESM,
***************
*** 289,292 ****
--- 290,294 ----
  
  \section{\dplc}
+ \label{sect:dplc}
  
  ?ste m?dulo, situado en cada una de las m?quinas clientes (o receptoras de las
***************
*** 413,423 ****
  \begin{description}
  	\item[\texttt{watch (path)}] :\\
! 		Activa la monitorizaci?n sobre \texttt{path}
  
  	\item[\texttt{unwatch (path)}] :\\
! 		Desactiva la monitorizaci?n sobre \texttt{path}
  
  	\item[\texttt{stop ()}] :\\
! 		Detiene \famon
  \end{description}
  
--- 415,425 ----
  \begin{description}
  	\item[\texttt{watch (path)}] :\\
! 		Activa la monitorizaci?n sobre \texttt{path}.
  
  	\item[\texttt{unwatch (path)}] :\\
! 		Desactiva la monitorizaci?n sobre \texttt{path}.
  
  	\item[\texttt{stop ()}] :\\
! 		Detiene \famon.
  \end{description}
  



